{
  "name": "X Daily Pack v5 - Semantic Dedupe",
  "nodes": [
    {
      "parameters": {},
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        120,
        300
      ],
      "id": "manual-trigger"
    },
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "cronExpression",
              "expression": "0 8 * * *"
            }
          ]
        }
      },
      "name": "Trigger 8AM",
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.1,
      "position": [
        240,
        300
      ],
      "id": "trigger"
    },
    {
      "parameters": {
        "jsCode": "// RSS Fetch Node - Dynamically fetches all configured feeds\n// This replaces multiple hardcoded RSS nodes with a single dynamic fetcher\n// Uses n8n's httpRequest helper instead of Node.js http/https modules\n\n// RSS feed configuration (embedded from config/rss-feeds.json)\n// To update: copy feeds array from config/rss-feeds.json\n// Last updated: 2026-01-20 - 18 sources including GitHub Trending and Reddit\nconst feeds = [\n  // Tier A - Official sources + GitHub Trending\n  { id: \"openai-news\", name: \"OpenAI News\", url: \"https://openai.com/news/rss.xml\", tier: \"A\" },\n  { id: \"deepmind-blog\", name: \"DeepMind Blog\", url: \"https://deepmind.google/blog/rss.xml\", tier: \"A\" },\n  { id: \"google-ai-blog\", name: \"Google AI Blog\", url: \"https://blog.google/technology/ai/rss/\", tier: \"A\" },\n  { id: \"langchain-blog\", name: \"LangChain Blog\", url: \"https://blog.langchain.dev/rss/\", tier: \"A\" },\n  { id: \"huggingface-blog\", name: \"Hugging Face Blog\", url: \"https://huggingface.co/blog/feed.xml\", tier: \"A\" },\n  { id: \"github-trending-python\", name: \"GitHub Trending - Python\", url: \"https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml\", tier: \"A\" },\n  // Tier B - Expert blogs + Reddit + Product Hunt\n  { id: \"simonwillison\", name: \"Simon Willison\", url: \"https://simonwillison.net/atom/everything/\", tier: \"B\" },\n  { id: \"latent-space\", name: \"Latent Space\", url: \"https://www.latent.space/feed\", tier: \"B\" },\n  { id: \"interconnects\", name: \"Interconnects\", url: \"https://www.interconnects.ai/feed\", tier: \"B\" },\n  { id: \"lilian-weng\", name: \"Lil'Log (Lilian Weng)\", url: \"https://lilianweng.github.io/index.xml\", tier: \"B\" },\n  { id: \"github-trending-all\", name: \"GitHub Trending - All\", url: \"https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml\", tier: \"B\" },\n  { id: \"reddit-localllama\", name: \"Reddit - LocalLLaMA\", url: \"https://www.reddit.com/r/LocalLLaMA/.rss\", tier: \"B\" },\n  { id: \"reddit-machinelearning\", name: \"Reddit - MachineLearning\", url: \"https://www.reddit.com/r/MachineLearning/.rss\", tier: \"B\" },\n  { id: \"producthunt-ai\", name: \"Product Hunt - AI\", url: \"https://www.producthunt.com/feed?category=artificial-intelligence\", tier: \"B\" },\n  // Tier C - Community\n  { id: \"reddit-chatgpt\", name: \"Reddit - ChatGPT\", url: \"https://www.reddit.com/r/ChatGPT/.rss\", tier: \"C\" },\n  { id: \"hackernews-best\", name: \"Hacker News - Best\", url: \"https://hnrss.org/best?count=20\", tier: \"C\" },\n  { id: \"hackernews-ai\", name: \"Hacker News - AI\", url: \"https://hnrss.org/newest?q=AI+OR+GPT+OR+LLM&count=15\", tier: \"C\" },\n  // Tier D - Aggregators\n  { id: \"google-news-ai\", name: \"Google News - AI\", url: \"https://news.google.com/rss/search?q=artificial+intelligence+OR+AI&hl=en-US&gl=US&ceid=US:en\", tier: \"D\" }\n];\n\nconst maxItemsPerFeed = Number.parseInt($env.RSS_MAX_ITEMS_PER_FEED || '15', 10);\nconst timeoutMs = Number.parseInt($env.RSS_FETCH_TIMEOUT_MS || '15000', 10);\n\nconst parseRssDate = (dateStr) => {\n  if (!dateStr) return null;\n  try {\n    const d = new Date(dateStr);\n    return isNaN(d.getTime()) ? null : d.toISOString();\n  } catch (e) {\n    return null;\n  }\n};\n\nconst extractText = (xml, tag) => {\n  const regex = new RegExp(`<${tag}[^>]*>([\\\\s\\\\S]*?)</${tag}>`, 'i');\n  const match = xml.match(regex);\n  if (!match) return '';\n  let text = match[1].replace(/<!\\[CDATA\\[([\\s\\S]*?)\\]\\]>/g, '$1');\n  text = text.replace(/<[^>]+>/g, '');\n  text = text.replace(/&amp;/g, '&').replace(/&lt;/g, '<').replace(/&gt;/g, '>').replace(/&quot;/g, '\"').replace(/&#39;/g, \"'\");\n  return text.trim();\n};\n\nconst extractLink = (itemXml) => {\n  const hrefMatch = itemXml.match(/<link[^>]+href=[\"']([^\"']+)[\"']/i);\n  if (hrefMatch) return hrefMatch[1];\n  return extractText(itemXml, 'link');\n};\n\nconst parseItems = (xml, feedName, tier) => {\n  const items = [];\n  const itemRegex = /<(item|entry)[\\s>]([\\s\\S]*?)<\\/\\1>/gi;\n  let match;\n\n  while ((match = itemRegex.exec(xml)) !== null && items.length < maxItemsPerFeed) {\n    const itemXml = match[2];\n    const title = extractText(itemXml, 'title');\n    const link = extractLink(itemXml);\n    const description = extractText(itemXml, 'description') || extractText(itemXml, 'summary') || extractText(itemXml, 'content');\n    const pubDate = extractText(itemXml, 'pubDate') || extractText(itemXml, 'published') || extractText(itemXml, 'updated');\n\n    if (title && link) {\n      items.push({\n        title: title.substring(0, 200),\n        url: link,\n        source: feedName,\n        sourceType: 'RSS',\n        tier: tier,\n        snippet: description.substring(0, 300),\n        publishedAt: parseRssDate(pubDate)\n      });\n    }\n  }\n\n  return items;\n};\n\nconst allItems = [];\nconst errors = [];\n\nconst fetchPromises = feeds.map(async (feed) => {\n  try {\n    const response = await this.helpers.httpRequest({\n      method: 'GET',\n      url: feed.url,\n      headers: {\n        'User-Agent': 'n8n-rss-fetcher/1.0',\n        'Accept': 'application/rss+xml, application/atom+xml, application/xml, text/xml'\n      },\n      timeout: timeoutMs,\n      returnFullResponse: false\n    });\n\n    const xml = typeof response === 'string' ? response : JSON.stringify(response);\n    const items = parseItems(xml, feed.name, feed.tier);\n    return { feed: feed.id, items, error: null };\n  } catch (error) {\n    return { feed: feed.id, items: [], error: error.message };\n  }\n});\n\nconst results = await Promise.all(fetchPromises);\n\nresults.forEach(result => {\n  if (result.error) {\n    errors.push({ feed: result.feed, error: result.error });\n  } else {\n    allItems.push(...result.items);\n  }\n});\n\nconst stats = {\n  total_feeds: feeds.length,\n  successful_feeds: results.filter(r => !r.error).length,\n  failed_feeds: errors.length,\n  total_items: allItems.length,\n  errors: errors\n};\nconsole.log('RSS Fetch Stats:', JSON.stringify(stats));\n\nif (allItems.length === 0 && errors.length === feeds.length) {\n  throw new Error(`All RSS feeds failed: ${JSON.stringify(errors)}`);\n}\n\nreturn allItems.map(item => ({ json: item }));\n"
      },
      "name": "RSS Fetch All",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        440,
        200
      ],
      "id": "rss-fetch"
    },
    {
      "parameters": {
        "jsCode": "// X Keyword Search Node - Calls Rube MCP (Streamable HTTP)\n// Searches X/Twitter using RUBE_MULTI_EXECUTE_TOOL\n\nconst rubeUrl = $env.RUBE_MCP_URL || 'https://rube.app/mcp';\nconst rubeToken = $env.RUBE_AUTH_TOKEN || $env.RUBE_API_TOKEN;\n\nif (!rubeToken) {\n  throw new Error('Missing Rube token. Set RUBE_AUTH_TOKEN (or RUBE_API_TOKEN).');\n}\n\n// Read keyword queries from config (embedded for portability)\n// Last updated: 2026-01-20 - \u504f\u91cd\u5e94\u7528\u578b/\u5b9e\u8df5\u578b\u5185\u5bb9\nconst keywordQueries = [\n  { id: 'ai-agents', query: '(AI agent OR AI agents OR autonomous agent OR agentic) -is:retweet -is:reply lang:en' },\n  { id: 'ai-workflow', query: '(AI workflow OR AI automation OR AI productivity OR \"AI tools\") -is:retweet -is:reply lang:en' },\n  { id: 'llm-prompts', query: '(LLM OR \"prompt engineering\" OR \"Claude\" OR \"GPT-4\" OR \"ChatGPT\") (tutorial OR guide OR tips) -is:retweet -is:reply lang:en' },\n  { id: 'ai-built', query: '(\"I built\" OR \"I made\" OR \"just shipped\" OR \"just launched\") (AI OR GPT OR Claude OR agent) -is:retweet -is:reply lang:en' },\n  { id: 'ai-freebies', query: '(\"free tier\" OR \"free credits\" OR \"free API\" OR \"open source\") (AI OR LLM OR GPT OR Claude) -is:retweet -is:reply lang:en' },\n  { id: 'buildinpublic', query: '(#buildinpublic OR #indiehackers) (AI OR GPT OR Claude OR LLM) -is:retweet -is:reply lang:en' },\n  { id: 'ai-tips', query: '(\"pro tip\" OR \"life hack\" OR \"game changer\") (AI OR ChatGPT OR Claude) -is:retweet -is:reply lang:en' }\n];\n\nlet mcpProtocolVersion = '2025-06-18';\nlet mcpSessionId = null;\nlet requestId = 1;\n\nconst allTweets = [];\nconst seenTweetIds = new Set();\n\nconst getHeader = (headers, name) => {\n  if (!headers) return null;\n  const key = Object.keys(headers).find(k => k.toLowerCase() === name.toLowerCase());\n  return key ? headers[key] : null;\n};\n\nconst parseSseEvents = (text) => {\n  const events = [];\n  const lines = String(text || '').split('\\n');\n  for (let i = 0; i < lines.length; i += 1) {\n    const line = lines[i].trim();\n    if (!line.startsWith('data:')) continue;\n    const payload = line.slice(5).trim();\n    if (!payload || payload === '[DONE]') continue;\n    try {\n      events.push(JSON.parse(payload));\n    } catch (err) {\n      continue;\n    }\n  }\n  return events;\n};\n\nconst parseSse = (text) => {\n  const events = parseSseEvents(text);\n  return events.length ? events[events.length - 1] : null;\n};\n\nconst parseBody = (body) => {\n  if (!body) return null;\n  if (typeof body === 'string') {\n    const trimmed = body.trim();\n    if (trimmed.startsWith('{')) {\n      try {\n        return JSON.parse(trimmed);\n      } catch (err) {\n        return null;\n      }\n    }\n    return parseSse(trimmed);\n  }\n  return body;\n};\n\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n\nconst isTransientRubeError = (message) => {\n  if (!message) return false;\n  return /tools failed|rate limit|temporar|timeout|429|503/i.test(message);\n};\n\nconst normalizeRubeBody = (body) => {\n  if (!body || typeof body !== 'object') return null;\n  return body.result || body;\n};\n\nconst unwrapData = (obj) => {\n  let current = obj;\n  for (let i = 0; i < 2; i += 1) {\n    if (current && typeof current === 'object' && current.data) {\n      current = current.data;\n    }\n  }\n  return current;\n};\n\nconst extractRubePayloads = (body) => {\n  const root = normalizeRubeBody(body);\n  const candidates = [];\n  if (!root) return candidates;\n  if (root.data) candidates.push(root.data);\n  if (Array.isArray(root.content)) {\n    root.content.forEach((entry) => {\n      if (entry?.json) candidates.push(entry.json);\n      if (entry?.data) candidates.push(entry.data);\n      if (entry?.text) {\n        try {\n          candidates.push(JSON.parse(entry.text));\n        } catch (err) {\n          candidates.push(entry.text);\n        }\n      }\n    });\n  }\n  candidates.push(root);\n  return candidates;\n};\n\nconst extractRubeError = (body) => {\n  if (!body) return null;\n  if (typeof body === 'string') {\n    return /tools failed|error|failed/i.test(body) ? body : null;\n  }\n  if (body.error?.message) return body.error.message;\n  if (typeof body.error === 'string') return body.error;\n\n  const root = normalizeRubeBody(body) || body;\n  if (root?.error?.message) return root.error.message;\n  if (typeof root?.error === 'string') return root.error;\n\n  const candidates = extractRubePayloads(body);\n  for (const candidate of candidates) {\n    if (!candidate) continue;\n    if (typeof candidate === 'string') {\n      if (/tools failed|error|failed/i.test(candidate)) return candidate;\n      continue;\n    }\n    if (candidate.error?.message) return candidate.error.message;\n    if (typeof candidate.error === 'string') return candidate.error;\n    if (candidate.message && /error|failed/i.test(String(candidate.message))) return candidate.message;\n  }\n  return null;\n};\n\nconst extractRubeDetails = (body) => {\n  const candidates = extractRubePayloads(body);\n  for (const candidate of candidates) {\n    if (!candidate || typeof candidate !== 'object') continue;\n    const details = candidate.details || candidate.detail;\n    if (details?.requestId) return { requestId: details.requestId };\n    if (candidate.requestId) return { requestId: candidate.requestId };\n    if (candidate.log_id) return { logId: candidate.log_id };\n  }\n  return {};\n};\n\nconst assertRubeSuccess = (body, label) => {\n  const err = extractRubeError(body);\n  if (err) {\n    const details = extractRubeDetails(body);\n    const suffixParts = [];\n    if (details.requestId) suffixParts.push(`requestId=${details.requestId}`);\n    if (details.logId) suffixParts.push(`logId=${details.logId}`);\n    const suffix = suffixParts.length ? ` (${suffixParts.join(', ')})` : '';\n    throw new Error(`${label} failed: ${err}${suffix}`);\n  }\n};\n\nconst extractSearchResult = (body) => {\n  for (const candidate of extractRubePayloads(body)) {\n    const data = unwrapData(candidate);\n    if (data?.results || data?.session_id || data?.session) return data;\n  }\n  return null;\n};\n\nconst extractConnectionResult = (body) => {\n  for (const candidate of extractRubePayloads(body)) {\n    const data = unwrapData(candidate);\n    if (data?.connections || data?.active_connection !== undefined) return data;\n    if (data?.results || data?.toolkit_connection_statuses) return data;\n  }\n  return null;\n};\n\nconst resolveSessionId = (searchData) => {\n  const data = unwrapData(searchData);\n  if (!data) return null;\n  if (data.session?.id) return data.session.id;\n  if (data.session_id) return data.session_id;\n  const first = Array.isArray(data.results) ? data.results[0] : null;\n  if (first?.session_id) return first.session_id;\n  if (first?.session?.id) return first.session.id;\n  return null;\n};\n\nconst resolveToolContext = (searchData) => {\n  const data = unwrapData(searchData);\n  const results = Array.isArray(data?.results) ? data.results : [];\n  const primary = results[0] || {};\n  const mainTools = Array.isArray(primary?.main_tools)\n    ? primary.main_tools\n    : (Array.isArray(primary?.tools) ? primary.tools : []);\n  let toolSlug = primary?.primary_tool_slugs?.[0] || mainTools[0]?.tool_slug || mainTools[0]?.name || null;\n  if (!toolSlug) {\n    const schemaKeys = Object.keys(data?.tool_schemas || {});\n    if (schemaKeys.length > 0) {\n      toolSlug = schemaKeys.includes('TWITTER_RECENT_SEARCH') ? 'TWITTER_RECENT_SEARCH' : schemaKeys[0];\n    }\n  }\n  const toolkits = [];\n  if (Array.isArray(primary?.toolkits)) toolkits.push(...primary.toolkits);\n  if (Array.isArray(data?.toolkits)) toolkits.push(...data.toolkits);\n  if (Array.isArray(data?.toolkit_connection_statuses)) {\n    data.toolkit_connection_statuses.forEach((entry) => {\n      if (entry?.toolkit) toolkits.push(entry.toolkit);\n    });\n  }\n  if (mainTools[0]?.toolkit_name) toolkits.push(mainTools[0].toolkit_name);\n  if (mainTools[0]?.toolkit) toolkits.push(mainTools[0].toolkit);\n  let activeConnection = primary?.active_connection;\n  if (activeConnection === undefined && Array.isArray(data?.toolkit_connection_statuses)) {\n    if (data.toolkit_connection_statuses.length > 0) {\n      activeConnection = data.toolkit_connection_statuses.every((entry) => entry?.has_active_connection !== false);\n    }\n  }\n  return {\n    toolSlug,\n    toolkits: [...new Set(toolkits.filter(Boolean))],\n    activeConnection\n  };\n};\n\nconst findTweetsEnvelope = (obj) => {\n  if (!obj || typeof obj !== 'object') return null;\n  if (Array.isArray(obj.data)) return obj;\n  if (obj.data && Array.isArray(obj.data.data)) return obj.data;\n  return null;\n};\n\nconst findMultiExecuteEnvelope = (obj) => {\n  const results = obj?.data?.data?.results;\n  if (!Array.isArray(results)) return null;\n  for (const result of results) {\n    const envelope = findTweetsEnvelope(result?.response?.data);\n    if (envelope) return envelope;\n  }\n  return null;\n};\n\nconst extractTwitterPayload = (response) => {\n  const root = response?.result || response;\n  const candidates = [];\n  if (root?.data) candidates.push(root.data);\n  if (Array.isArray(root?.content)) {\n    root.content.forEach((entry) => {\n      if (entry?.json) candidates.push(entry.json);\n      if (entry?.data) candidates.push(entry.data);\n      if (entry?.text) {\n        try {\n          candidates.push(JSON.parse(entry.text));\n        } catch (err) {\n          // ignore parse errors\n        }\n      }\n    });\n  }\n  candidates.push(root);\n  for (const candidate of candidates) {\n    const envelope = findTweetsEnvelope(candidate);\n    if (envelope) return envelope;\n    const multiEnvelope = findMultiExecuteEnvelope(candidate);\n    if (multiEnvelope) return multiEnvelope;\n  }\n  return null;\n};\n\nconst mcpPost = async (payload, includeProtocolHeader = true) => {\n  const headers = {\n    Authorization: `Bearer ${rubeToken}`,\n    'Content-Type': 'application/json',\n    Accept: 'application/json, text/event-stream'\n  };\n  if (includeProtocolHeader && mcpProtocolVersion) headers['MCP-Protocol-Version'] = mcpProtocolVersion;\n  if (includeProtocolHeader && mcpSessionId) headers['Mcp-Session-Id'] = mcpSessionId;\n\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: rubeUrl,\n    headers,\n    body: payload,\n    returnFullResponse: true,\n    responseFormat: 'string'\n  });\n\n  const rawBody = response?.body ?? response;\n  const parsedBody = parseBody(rawBody);\n  return {\n    body: parsedBody || rawBody,\n    headers: response?.headers\n  };\n};\n\nconst initializeMcp = async () => {\n  const initPayload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'initialize',\n    params: {\n      protocolVersion: mcpProtocolVersion,\n      capabilities: {},\n      clientInfo: { name: 'n8n', version: '1.0.0' }\n    }\n  };\n\n  const initResponse = await mcpPost(initPayload, false);\n  const initResult = initResponse.body?.result;\n  if (initResult?.protocolVersion) {\n    mcpProtocolVersion = initResult.protocolVersion;\n  }\n\n  const sessionHeader = getHeader(initResponse.headers, 'mcp-session-id');\n  if (sessionHeader) {\n    mcpSessionId = Array.isArray(sessionHeader) ? sessionHeader[0] : sessionHeader;\n  }\n\n  await mcpPost({ jsonrpc: '2.0', method: 'notifications/initialized' }, true);\n};\n\nconst searchTools = async (query) => {\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_SEARCH_TOOLS',\n      arguments: {\n        queries: [\n          {\n            use_case: 'search recent tweets on twitter',\n            known_fields: `query: ${query}`\n          }\n        ],\n        session: { generate_id: true }\n      }\n    }\n  };\n\n  const response = await mcpPost(payload, true);\n  assertRubeSuccess(response.body, 'Rube search tools');\n  const searchData = extractSearchResult(response.body);\n  if (!searchData) throw new Error('Rube search tools returned empty payload');\n  return searchData;\n};\n\nconst ensureActiveConnections = async (toolkits, sessionId) => {\n  if (!Array.isArray(toolkits) || toolkits.length === 0) return;\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_MANAGE_CONNECTIONS',\n      arguments: {\n        toolkits,\n        session_id: sessionId\n      }\n    }\n  };\n\n  const response = await mcpPost(payload, true);\n  assertRubeSuccess(response.body, 'Rube manage connections');\n  const connData = extractConnectionResult(response.body);\n  if (!connData) return;\n\n  const toolkitStatuses = connData.toolkit_connection_statuses;\n  if (Array.isArray(toolkitStatuses) && toolkitStatuses.length > 0) {\n    const inactive = toolkitStatuses.find((entry) => entry?.has_active_connection === false);\n    if (inactive) {\n      const detail = inactive.status_message || inactive.description || 'inactive';\n      throw new Error(`Rube connection not ACTIVE: ${detail}`);\n    }\n  }\n\n  const results = connData.results;\n  if (results && typeof results === 'object' && !Array.isArray(results)) {\n    const entries = Object.values(results);\n    const inactive = entries.find((entry) => {\n      if (!entry || typeof entry !== 'object') return false;\n      if (entry.has_active_connection === false) return true;\n      const status = entry.connection_status || entry.status;\n      return status ? String(status).toUpperCase() !== 'ACTIVE' : false;\n    });\n    if (inactive) {\n      const status = inactive.connection_status || inactive.status || 'UNKNOWN';\n      const detail = inactive.instruction || inactive.status_message || inactive.description;\n      const suffix = detail ? ` - ${detail}` : '';\n      throw new Error(`Rube connection not ACTIVE: ${status}${suffix}`);\n    }\n  }\n\n  const connections = connData.connections || [];\n  if (Array.isArray(connections) && connections.length > 0) {\n    const inactive = connections.find((conn) => {\n      const status = conn.connection_status || conn.status;\n      return status && status !== 'ACTIVE';\n    });\n    if (inactive) {\n      const status = inactive.connection_status || inactive.status || 'UNKNOWN';\n      const redirect = inactive.redirect_url ? ` (open: ${inactive.redirect_url})` : '';\n      throw new Error(`Rube connection not ACTIVE: ${status}${redirect}`);\n    }\n  } else if (connData.active_connection === false) {\n    throw new Error('Rube connection not ACTIVE');\n  }\n};\n\nconst executeWithRetry = async (payload, label, maxAttempts = 3) => {\n  let lastError;\n  for (let attempt = 1; attempt <= maxAttempts; attempt += 1) {\n    try {\n      const response = await mcpPost(payload, true);\n      assertRubeSuccess(response.body, label);\n      return response;\n    } catch (error) {\n      lastError = error;\n      const message = error?.message || String(error);\n      if (attempt < maxAttempts && isTransientRubeError(message)) {\n        await sleep(600 * attempt);\n        continue;\n      }\n      throw error;\n    }\n  }\n  throw lastError;\n};\n\ntry {\n  await initializeMcp();\n\n  const seedQuery = keywordQueries[0]?.query || 'AI agent -is:retweet lang:en';\n  const searchData = await searchTools(seedQuery);\n  const sessionId = resolveSessionId(searchData);\n  if (!sessionId) throw new Error('Rube search tools missing session_id');\n\n  const toolContext = resolveToolContext(searchData);\n  if (!toolContext.toolSlug) throw new Error('Rube search tools missing tool slug');\n\n  const resolvedToolkits = toolContext.toolkits.length\n    ? toolContext.toolkits\n    : (toolContext.toolSlug?.startsWith('TWITTER_') ? ['twitter'] : []);\n  await ensureActiveConnections(resolvedToolkits, sessionId);\n\n  for (let idx = 0; idx < keywordQueries.length; idx += 1) {\n    const keywordQuery = keywordQueries[idx];\n    const payload = {\n      jsonrpc: '2.0',\n      id: requestId++,\n      method: 'tools/call',\n      params: {\n        name: 'RUBE_MULTI_EXECUTE_TOOL',\n        arguments: {\n          tools: [\n            {\n              tool_slug: toolContext.toolSlug,\n              arguments: {\n                query: keywordQuery.query,\n                max_results: 20,\n                tweet_fields: ['created_at', 'public_metrics', 'author_id'],\n                expansions: ['author_id'],\n                user_fields: ['username', 'name']\n              }\n            }\n          ],\n          sync_response_to_workbench: false,\n          memory: {},\n          session_id: sessionId,\n          current_step: 'FETCH_TWEETS',\n          current_step_metric: `${idx + 1}/${keywordQueries.length}`\n        }\n      }\n    };\n\n    const response = await executeWithRetry(payload, `Rube X keyword search (${keywordQuery.id})`);\n    const twitterPayload = extractTwitterPayload(response.body);\n    if (!twitterPayload) {\n      console.log(`Rube X keyword search returned empty payload: ${keywordQuery.id}`);\n    }\n    const tweets = twitterPayload?.data || [];\n    const users = twitterPayload?.includes?.users || [];\n\n    const userMap = {};\n    users.forEach((user) => {\n      userMap[user.id] = user;\n    });\n\n    tweets.forEach((tweet) => {\n      if (seenTweetIds.has(tweet.id)) return;\n      seenTweetIds.add(tweet.id);\n      const author = userMap[tweet.author_id] || {};\n      const username = author.username || 'unknown';\n      allTweets.push({\n        title: tweet.text.substring(0, 100) + (tweet.text.length > 100 ? '...' : ''),\n        url: `https://twitter.com/${username}/status/${tweet.id}`,\n        source: `X - ${keywordQuery.id}`,\n        snippet: tweet.text,\n        publishedAt: tweet.created_at,\n        author: username,\n        metrics: tweet.public_metrics || {}\n      });\n    });\n\n    if (idx < keywordQueries.length - 1) {\n      await sleep(400);\n    }\n  }\n\n  return allTweets.map(tweet => ({ json: tweet }));\n} catch (error) {\n  throw new Error(`X keyword search failed: ${error.message}`);\n}\n"
      },
      "name": "X Keyword Search",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        440,
        350
      ],
      "id": "x-keyword"
    },
    {
      "parameters": {
        "jsCode": "// X Account Timeline Node - Calls Rube MCP (Streamable HTTP)\n// Fetches recent tweets from configured accounts using RUBE_MULTI_EXECUTE_TOOL\n\nconst rubeUrl = $env.RUBE_MCP_URL || 'https://rube.app/mcp';\nconst rubeToken = $env.RUBE_AUTH_TOKEN || $env.RUBE_API_TOKEN;\n\nif (!rubeToken) {\n  throw new Error('Missing Rube token. Set RUBE_AUTH_TOKEN (or RUBE_API_TOKEN).');\n}\n\n// Combined account query\nconst accountQuery = 'from:AnthropicAI OR from:OpenAI OR from:LangChainAI OR from:hwchase17 OR from:karpathy OR from:sama OR from:ylecun OR from:goodside OR from:simonw OR from:swyx -is:retweet -giveaway -airdrop';\n\nlet mcpProtocolVersion = '2025-06-18';\nlet mcpSessionId = null;\nlet requestId = 1;\n\nconst allTweets = [];\nconst seenTweetIds = new Set();\n\nconst getHeader = (headers, name) => {\n  if (!headers) return null;\n  const key = Object.keys(headers).find(k => k.toLowerCase() === name.toLowerCase());\n  return key ? headers[key] : null;\n};\n\nconst parseSseEvents = (text) => {\n  const events = [];\n  const lines = String(text || '').split('\\n');\n  for (let i = 0; i < lines.length; i += 1) {\n    const line = lines[i].trim();\n    if (!line.startsWith('data:')) continue;\n    const payload = line.slice(5).trim();\n    if (!payload || payload === '[DONE]') continue;\n    try {\n      events.push(JSON.parse(payload));\n    } catch (err) {\n      continue;\n    }\n  }\n  return events;\n};\n\nconst parseSse = (text) => {\n  const events = parseSseEvents(text);\n  return events.length ? events[events.length - 1] : null;\n};\n\nconst parseBody = (body) => {\n  if (!body) return null;\n  if (typeof body === 'string') {\n    const trimmed = body.trim();\n    if (trimmed.startsWith('{')) {\n      try {\n        return JSON.parse(trimmed);\n      } catch (err) {\n        return null;\n      }\n    }\n    return parseSse(trimmed);\n  }\n  return body;\n};\n\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n\nconst isTransientRubeError = (message) => {\n  if (!message) return false;\n  return /tools failed|rate limit|temporar|timeout|429|503/i.test(message);\n};\n\nconst normalizeRubeBody = (body) => {\n  if (!body || typeof body !== 'object') return null;\n  return body.result || body;\n};\n\nconst unwrapData = (obj) => {\n  let current = obj;\n  for (let i = 0; i < 2; i += 1) {\n    if (current && typeof current === 'object' && current.data) {\n      current = current.data;\n    }\n  }\n  return current;\n};\n\nconst extractRubePayloads = (body) => {\n  const root = normalizeRubeBody(body);\n  const candidates = [];\n  if (!root) return candidates;\n  if (root.data) candidates.push(root.data);\n  if (Array.isArray(root.content)) {\n    root.content.forEach((entry) => {\n      if (entry?.json) candidates.push(entry.json);\n      if (entry?.data) candidates.push(entry.data);\n      if (entry?.text) {\n        try {\n          candidates.push(JSON.parse(entry.text));\n        } catch (err) {\n          candidates.push(entry.text);\n        }\n      }\n    });\n  }\n  candidates.push(root);\n  return candidates;\n};\n\nconst extractRubeError = (body) => {\n  if (!body) return null;\n  if (typeof body === 'string') {\n    return /tools failed|error|failed/i.test(body) ? body : null;\n  }\n  if (body.error?.message) return body.error.message;\n  if (typeof body.error === 'string') return body.error;\n\n  const root = normalizeRubeBody(body) || body;\n  if (root?.error?.message) return root.error.message;\n  if (typeof root?.error === 'string') return root.error;\n\n  const candidates = extractRubePayloads(body);\n  for (const candidate of candidates) {\n    if (!candidate) continue;\n    if (typeof candidate === 'string') {\n      if (/tools failed|error|failed/i.test(candidate)) return candidate;\n      continue;\n    }\n    if (candidate.error?.message) return candidate.error.message;\n    if (typeof candidate.error === 'string') return candidate.error;\n    if (candidate.message && /error|failed/i.test(String(candidate.message))) return candidate.message;\n  }\n  return null;\n};\n\nconst extractRubeDetails = (body) => {\n  const candidates = extractRubePayloads(body);\n  for (const candidate of candidates) {\n    if (!candidate || typeof candidate !== 'object') continue;\n    const details = candidate.details || candidate.detail;\n    if (details?.requestId) return { requestId: details.requestId };\n    if (candidate.requestId) return { requestId: candidate.requestId };\n    if (candidate.log_id) return { logId: candidate.log_id };\n  }\n  return {};\n};\n\nconst assertRubeSuccess = (body, label) => {\n  const err = extractRubeError(body);\n  if (err) {\n    const details = extractRubeDetails(body);\n    const suffixParts = [];\n    if (details.requestId) suffixParts.push(`requestId=${details.requestId}`);\n    if (details.logId) suffixParts.push(`logId=${details.logId}`);\n    const suffix = suffixParts.length ? ` (${suffixParts.join(', ')})` : '';\n    throw new Error(`${label} failed: ${err}${suffix}`);\n  }\n};\n\nconst extractSearchResult = (body) => {\n  for (const candidate of extractRubePayloads(body)) {\n    const data = unwrapData(candidate);\n    if (data?.results || data?.session_id || data?.session) return data;\n  }\n  return null;\n};\n\nconst extractConnectionResult = (body) => {\n  for (const candidate of extractRubePayloads(body)) {\n    const data = unwrapData(candidate);\n    if (data?.connections || data?.active_connection !== undefined) return data;\n    if (data?.results || data?.toolkit_connection_statuses) return data;\n  }\n  return null;\n};\n\nconst resolveSessionId = (searchData) => {\n  const data = unwrapData(searchData);\n  if (!data) return null;\n  if (data.session?.id) return data.session.id;\n  if (data.session_id) return data.session_id;\n  const first = Array.isArray(data.results) ? data.results[0] : null;\n  if (first?.session_id) return first.session_id;\n  if (first?.session?.id) return first.session.id;\n  return null;\n};\n\nconst resolveToolContext = (searchData) => {\n  const data = unwrapData(searchData);\n  const results = Array.isArray(data?.results) ? data.results : [];\n  const primary = results[0] || {};\n  const mainTools = Array.isArray(primary?.main_tools)\n    ? primary.main_tools\n    : (Array.isArray(primary?.tools) ? primary.tools : []);\n  let toolSlug = primary?.primary_tool_slugs?.[0] || mainTools[0]?.tool_slug || mainTools[0]?.name || null;\n  if (!toolSlug) {\n    const schemaKeys = Object.keys(data?.tool_schemas || {});\n    if (schemaKeys.length > 0) {\n      toolSlug = schemaKeys.includes('TWITTER_RECENT_SEARCH') ? 'TWITTER_RECENT_SEARCH' : schemaKeys[0];\n    }\n  }\n  const toolkits = [];\n  if (Array.isArray(primary?.toolkits)) toolkits.push(...primary.toolkits);\n  if (Array.isArray(data?.toolkits)) toolkits.push(...data.toolkits);\n  if (Array.isArray(data?.toolkit_connection_statuses)) {\n    data.toolkit_connection_statuses.forEach((entry) => {\n      if (entry?.toolkit) toolkits.push(entry.toolkit);\n    });\n  }\n  if (mainTools[0]?.toolkit_name) toolkits.push(mainTools[0].toolkit_name);\n  if (mainTools[0]?.toolkit) toolkits.push(mainTools[0].toolkit);\n  let activeConnection = primary?.active_connection;\n  if (activeConnection === undefined && Array.isArray(data?.toolkit_connection_statuses)) {\n    if (data.toolkit_connection_statuses.length > 0) {\n      activeConnection = data.toolkit_connection_statuses.every((entry) => entry?.has_active_connection !== false);\n    }\n  }\n  return {\n    toolSlug,\n    toolkits: [...new Set(toolkits.filter(Boolean))],\n    activeConnection\n  };\n};\n\nconst findTweetsEnvelope = (obj) => {\n  if (!obj || typeof obj !== 'object') return null;\n  if (Array.isArray(obj.data)) return obj;\n  if (obj.data && Array.isArray(obj.data.data)) return obj.data;\n  return null;\n};\n\nconst findMultiExecuteEnvelope = (obj) => {\n  const results = obj?.data?.data?.results;\n  if (!Array.isArray(results)) return null;\n  for (const result of results) {\n    const envelope = findTweetsEnvelope(result?.response?.data);\n    if (envelope) return envelope;\n  }\n  return null;\n};\n\nconst extractTwitterPayload = (response) => {\n  const root = response?.result || response;\n  const candidates = [];\n  if (root?.data) candidates.push(root.data);\n  if (Array.isArray(root?.content)) {\n    root.content.forEach((entry) => {\n      if (entry?.json) candidates.push(entry.json);\n      if (entry?.data) candidates.push(entry.data);\n      if (entry?.text) {\n        try {\n          candidates.push(JSON.parse(entry.text));\n        } catch (err) {\n          // ignore parse errors\n        }\n      }\n    });\n  }\n  candidates.push(root);\n  for (const candidate of candidates) {\n    const envelope = findTweetsEnvelope(candidate);\n    if (envelope) return envelope;\n    const multiEnvelope = findMultiExecuteEnvelope(candidate);\n    if (multiEnvelope) return multiEnvelope;\n  }\n  return null;\n};\n\nconst mcpPost = async (payload, includeProtocolHeader = true) => {\n  const headers = {\n    Authorization: `Bearer ${rubeToken}`,\n    'Content-Type': 'application/json',\n    Accept: 'application/json, text/event-stream'\n  };\n  if (includeProtocolHeader && mcpProtocolVersion) headers['MCP-Protocol-Version'] = mcpProtocolVersion;\n  if (includeProtocolHeader && mcpSessionId) headers['Mcp-Session-Id'] = mcpSessionId;\n\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: rubeUrl,\n    headers,\n    body: payload,\n    returnFullResponse: true,\n    responseFormat: 'string'\n  });\n\n  const rawBody = response?.body ?? response;\n  const parsedBody = parseBody(rawBody);\n  return {\n    body: parsedBody || rawBody,\n    headers: response?.headers\n  };\n};\n\nconst initializeMcp = async () => {\n  const initPayload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'initialize',\n    params: {\n      protocolVersion: mcpProtocolVersion,\n      capabilities: {},\n      clientInfo: { name: 'n8n', version: '1.0.0' }\n    }\n  };\n\n  const initResponse = await mcpPost(initPayload, false);\n  const initResult = initResponse.body?.result;\n  if (initResult?.protocolVersion) {\n    mcpProtocolVersion = initResult.protocolVersion;\n  }\n\n  const sessionHeader = getHeader(initResponse.headers, 'mcp-session-id');\n  if (sessionHeader) {\n    mcpSessionId = Array.isArray(sessionHeader) ? sessionHeader[0] : sessionHeader;\n  }\n\n  await mcpPost({ jsonrpc: '2.0', method: 'notifications/initialized' }, true);\n};\n\nconst searchTools = async (query) => {\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_SEARCH_TOOLS',\n      arguments: {\n        queries: [\n          {\n            use_case: 'search recent tweets on twitter',\n            known_fields: `query: ${query}`\n          }\n        ],\n        session: { generate_id: true }\n      }\n    }\n  };\n\n  const response = await mcpPost(payload, true);\n  assertRubeSuccess(response.body, 'Rube search tools');\n  const searchData = extractSearchResult(response.body);\n  if (!searchData) throw new Error('Rube search tools returned empty payload');\n  return searchData;\n};\n\nconst ensureActiveConnections = async (toolkits, sessionId) => {\n  if (!Array.isArray(toolkits) || toolkits.length === 0) return;\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_MANAGE_CONNECTIONS',\n      arguments: {\n        toolkits,\n        session_id: sessionId\n      }\n    }\n  };\n\n  const response = await mcpPost(payload, true);\n  assertRubeSuccess(response.body, 'Rube manage connections');\n  const connData = extractConnectionResult(response.body);\n  if (!connData) return;\n\n  const toolkitStatuses = connData.toolkit_connection_statuses;\n  if (Array.isArray(toolkitStatuses) && toolkitStatuses.length > 0) {\n    const inactive = toolkitStatuses.find((entry) => entry?.has_active_connection === false);\n    if (inactive) {\n      const detail = inactive.status_message || inactive.description || 'inactive';\n      throw new Error(`Rube connection not ACTIVE: ${detail}`);\n    }\n  }\n\n  const results = connData.results;\n  if (results && typeof results === 'object' && !Array.isArray(results)) {\n    const entries = Object.values(results);\n    const inactive = entries.find((entry) => {\n      if (!entry || typeof entry !== 'object') return false;\n      if (entry.has_active_connection === false) return true;\n      const status = entry.connection_status || entry.status;\n      return status ? String(status).toUpperCase() !== 'ACTIVE' : false;\n    });\n    if (inactive) {\n      const status = inactive.connection_status || inactive.status || 'UNKNOWN';\n      const detail = inactive.instruction || inactive.status_message || inactive.description;\n      const suffix = detail ? ` - ${detail}` : '';\n      throw new Error(`Rube connection not ACTIVE: ${status}${suffix}`);\n    }\n  }\n\n  const connections = connData.connections || [];\n  if (Array.isArray(connections) && connections.length > 0) {\n    const inactive = connections.find((conn) => {\n      const status = conn.connection_status || conn.status;\n      return status && status !== 'ACTIVE';\n    });\n    if (inactive) {\n      const status = inactive.connection_status || inactive.status || 'UNKNOWN';\n      const redirect = inactive.redirect_url ? ` (open: ${inactive.redirect_url})` : '';\n      throw new Error(`Rube connection not ACTIVE: ${status}${redirect}`);\n    }\n  } else if (connData.active_connection === false) {\n    throw new Error('Rube connection not ACTIVE');\n  }\n};\n\nconst executeWithRetry = async (payload, label, maxAttempts = 3) => {\n  let lastError;\n  for (let attempt = 1; attempt <= maxAttempts; attempt += 1) {\n    try {\n      const response = await mcpPost(payload, true);\n      assertRubeSuccess(response.body, label);\n      return response;\n    } catch (error) {\n      lastError = error;\n      const message = error?.message || String(error);\n      if (attempt < maxAttempts && isTransientRubeError(message)) {\n        await sleep(600 * attempt);\n        continue;\n      }\n      throw error;\n    }\n  }\n  throw lastError;\n};\n\ntry {\n  await initializeMcp();\n\n  const searchData = await searchTools(accountQuery);\n  const sessionId = resolveSessionId(searchData);\n  if (!sessionId) throw new Error('Rube search tools missing session_id');\n\n  const toolContext = resolveToolContext(searchData);\n  if (!toolContext.toolSlug) throw new Error('Rube search tools missing tool slug');\n\n  const resolvedToolkits = toolContext.toolkits.length\n    ? toolContext.toolkits\n    : (toolContext.toolSlug?.startsWith('TWITTER_') ? ['twitter'] : []);\n  await ensureActiveConnections(resolvedToolkits, sessionId);\n\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_MULTI_EXECUTE_TOOL',\n      arguments: {\n        tools: [\n          {\n            tool_slug: toolContext.toolSlug,\n            arguments: {\n              query: accountQuery,\n              max_results: 30,\n              tweet_fields: ['created_at', 'public_metrics', 'author_id'],\n              expansions: ['author_id'],\n              user_fields: ['username', 'name']\n            }\n          }\n        ],\n        sync_response_to_workbench: false,\n        memory: {},\n        session_id: sessionId,\n        current_step: 'FETCH_ACCOUNT_TWEETS',\n        current_step_metric: '1/1'\n      }\n    }\n  };\n\n  const response = await executeWithRetry(payload, 'Rube X account search');\n  const twitterPayload = extractTwitterPayload(response.body);\n  if (!twitterPayload) {\n    console.log('Rube X account search returned empty payload');\n  }\n  const tweets = twitterPayload?.data || [];\n  const users = twitterPayload?.includes?.users || [];\n\n  const userMap = {};\n  users.forEach((user) => {\n    userMap[user.id] = user;\n  });\n\n  tweets.forEach((tweet) => {\n    if (seenTweetIds.has(tweet.id)) return;\n    seenTweetIds.add(tweet.id);\n    const author = userMap[tweet.author_id] || {};\n    const username = author.username || 'unknown';\n    allTweets.push({\n      title: tweet.text.substring(0, 100) + (tweet.text.length > 100 ? '...' : ''),\n      url: `https://twitter.com/${username}/status/${tweet.id}`,\n      source: `X - @${username}`,\n      snippet: tweet.text,\n      publishedAt: tweet.created_at,\n      author: username,\n      metrics: tweet.public_metrics || {}\n    });\n  });\n\n  return allTweets.map(tweet => ({ json: tweet }));\n} catch (error) {\n  throw new Error(`X account search failed: ${error.message}`);\n}\n"
      },
      "name": "X Account Search",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        440,
        450
      ],
      "id": "x-account"
    },
    {
      "parameters": {
        "mode": "append"
      },
      "name": "Merge X",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [
        700,
        400
      ],
      "id": "merge-x"
    },
    {
      "parameters": {
        "mode": "append"
      },
      "name": "Merge All",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [
        940,
        300
      ],
      "id": "merge"
    },
    {
      "parameters": {
        "jsCode": "// Normalize all data\nconst items = $input.all();\nconst normalized = [];\n\nfor (const item of items) {\n  const data = item.json;\n  const source = data.source || 'RSS';\n  const sourceType = source.startsWith('X -') ? 'X' : 'RSS';\n  normalized.push({\n    title: data.title || data.text || '',\n    url: data.link || data.url || '',\n    source,\n    sourceType,\n    snippet: (data.description || data.summary || data.text || data.snippet || '').substring(0, 300),\n    publishedAt: data.pubDate || data.isoDate || data.created_at || data.publishedAt || new Date().toISOString(),\n    author: data.author || data.username || '',\n    metrics: data.metrics || data.public_metrics || {}\n  });\n}\n\nreturn normalized.map(item => ({ json: item }));"
      },
      "name": "Normalize",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1140,
        300
      ],
      "id": "normalize"
    },
    {
      "parameters": {
        "jsCode": "// Cross-Day Dedupe Node\n// Attempts to use workflow staticData; if\u4e0d\u53ef\u7528\u5219\u9000\u5316\u4e3a\u201c\u5f53\u6b21\u8fd0\u884c\u201d\u53bb\u91cd\uff08\u65e0\u8de8\u65e5\u6301\u4e45\u5316\uff09\n\nconst items = $input.all();\nconst EXPIRY_DAYS = Number.parseInt($env.DEDUPE_EXPIRY_DAYS || '7', 10);\nconst MAX_URLS = Number.parseInt($env.DEDUPE_MAX_URLS || '2000', 10);\n\n// Helper: load persistent store (prefer staticData)\nlet storage = { seenUrls: {} };\nlet storageMode = 'staticData';\n\ntry {\n  const staticData = this.getWorkflowStaticData\n    ? this.getWorkflowStaticData('global')\n    : this.helpers?.getWorkflowStaticData?.call(this, 'global');\n  if (!staticData) {\n    storageMode = 'volatile';\n  } else {\n    if (!staticData.seenUrls) staticData.seenUrls = {};\n    storage = staticData;\n  }\n} catch (err) {\n  storageMode = 'volatile';\n  storage = { seenUrls: {} };\n}\n\nconst now = Date.now();\nconst expiryMs = EXPIRY_DAYS * 24 * 60 * 60 * 1000;\n\n// Clean up expired entries\nconst urlKeys = Object.keys(storage.seenUrls);\nlet expiredCount = 0;\nfor (const url of urlKeys) {\n  const timestamp = storage.seenUrls[url];\n  if (now - timestamp > expiryMs) {\n    delete storage.seenUrls[url];\n    expiredCount++;\n  }\n}\n\n// Dedupe current batch\nconst unique = [];\nconst duplicateUrls = [];\nconst newUrls = [];\n\nfor (const item of items) {\n  const url = item.json.url;\n  if (!url) continue;\n\n  if (storage.seenUrls[url]) {\n    duplicateUrls.push(url);\n    continue;\n  }\n\n  storage.seenUrls[url] = now;\n  newUrls.push(url);\n  unique.push(item);\n}\n\n// Enforce max URL limit (remove oldest if over limit)\nconst allUrls = Object.entries(storage.seenUrls);\nif (allUrls.length > MAX_URLS) {\n  allUrls.sort((a, b) => a[1] - b[1]); // oldest first\n  const toRemove = allUrls.slice(0, allUrls.length - MAX_URLS);\n  for (const [url] of toRemove) {\n    delete storage.seenUrls[url];\n  }\n}\n\nconst stats = {\n  input_count: items.length,\n  unique_count: unique.length,\n  duplicate_count: duplicateUrls.length,\n  expired_cleaned: expiredCount,\n  total_stored_urls: Object.keys(storage.seenUrls).length,\n  expiry_days: EXPIRY_DAYS,\n  storage_mode: storageMode\n};\nconsole.log('Cross-Day Dedupe Stats:', JSON.stringify(stats));\n\nreturn unique;\n"
      },
      "name": "Cross-Day Dedupe",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1340,
        300
      ],
      "id": "dedupe"
    },
    {
      "parameters": {
        "jsCode": "// Semantic Dedupe Node\n// \u4f7f\u7528 OpenAI Embedding \u8fdb\u884c\u8bed\u4e49\u7ea7\u53bb\u91cd\uff0c\u8bc6\u522b\"\u4e0d\u540cURL\u4f46\u8bdd\u9898\u76f8\u540c\"\u7684\u5185\u5bb9\n//\n// \u5de5\u4f5c\u6d41\u7a0b\uff1a\n// 1. \u4e3a\u6bcf\u6761\u5185\u5bb9\u751f\u6210 Embedding \u5411\u91cf\n// 2. \u4e0e\u5386\u53f2 Embedding \u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\n// 3. \u76f8\u4f3c\u5ea6 > \u9608\u503c \u2192 \u5224\u5b9a\u4e3a\u8bed\u4e49\u91cd\u590d \u2192 \u8fc7\u6ee4\n// 4. \u901a\u8fc7\u7684\u5185\u5bb9\u66f4\u65b0\u5230\u5386\u53f2\u5b58\u50a8\n\nconst items = $input.all();\nconst apiKey = $env.OPENAI_API_KEY;\n\n// \u914d\u7f6e\u53c2\u6570\nconst SIMILARITY_THRESHOLD = Number.parseFloat($env.SEMANTIC_DEDUPE_THRESHOLD || '0.85');\nconst EXPIRY_DAYS = Number.parseInt($env.SEMANTIC_DEDUPE_EXPIRY_DAYS || '7', 10);\nconst MAX_EMBEDDINGS = Number.parseInt($env.SEMANTIC_DEDUPE_MAX_EMBEDDINGS || '500', 10);\nconst EMBEDDING_MODEL = $env.SEMANTIC_DEDUPE_MODEL || 'text-embedding-3-small';\nconst BATCH_SIZE = Number.parseInt($env.SEMANTIC_DEDUPE_BATCH_SIZE || '20', 10);\nconst DEBUG = $env.SEMANTIC_DEDUPE_DEBUG === 'true';\n\nif (!apiKey) {\n  console.log('Warning: Missing OPENAI_API_KEY, skipping semantic dedupe');\n  return items;\n}\n\nif (items.length === 0) {\n  return [];\n}\n\n// ============== \u5de5\u5177\u51fd\u6570 ==============\n\n// \u4f59\u5f26\u76f8\u4f3c\u5ea6\u8ba1\u7b97\nconst cosineSimilarity = (a, b) => {\n  if (!a || !b || a.length !== b.length) return 0;\n  let dot = 0, normA = 0, normB = 0;\n  for (let i = 0; i < a.length; i++) {\n    dot += a[i] * b[i];\n    normA += a[i] * a[i];\n    normB += b[i] * b[i];\n  }\n  if (normA === 0 || normB === 0) return 0;\n  return dot / (Math.sqrt(normA) * Math.sqrt(normB));\n};\n\n// \u751f\u6210\u5185\u5bb9\u7684\u6587\u672c\u8868\u793a\uff08\u7528\u4e8eEmbedding\uff09\nconst getContentText = (item) => {\n  const data = item.json || item;\n  const title = String(data.title || data.text || '').trim();\n  const snippet = String(data.snippet || data.description || data.summary || '').trim();\n  // \u7ec4\u5408 title + snippet\uff0c\u9650\u5236\u957f\u5ea6\uff08Embedding\u6a21\u578b\u6709token\u9650\u5236\uff09\n  const combined = `${title}\\n${snippet}`.slice(0, 500);\n  return combined;\n};\n\n// \u751f\u6210\u5185\u5bb9\u7684\u552f\u4e00\u6807\u8bc6\uff08\u7528\u4e8e\u5b58\u50a8\uff09\nconst getContentId = (item) => {\n  const data = item.json || item;\n  const url = data.url || '';\n  // \u4f7f\u7528URL\u7684hash\u4f5c\u4e3aID\n  let hash = 0;\n  for (let i = 0; i < url.length; i++) {\n    const char = url.charCodeAt(i);\n    hash = ((hash << 5) - hash) + char;\n    hash = hash & hash;\n  }\n  return `emb_${Math.abs(hash).toString(36)}`;\n};\n\n// \u6279\u91cf\u8c03\u7528 OpenAI Embedding API\nconst getEmbeddings = async (texts) => {\n  if (texts.length === 0) return [];\n\n  try {\n    const response = await this.helpers.httpRequest({\n      method: 'POST',\n      url: 'https://api.openai.com/v1/embeddings',\n      headers: {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n      },\n      body: {\n        model: EMBEDDING_MODEL,\n        input: texts\n      }\n    });\n\n    if (response?.data && Array.isArray(response.data)) {\n      // \u6309 index \u6392\u5e8f\u786e\u4fdd\u987a\u5e8f\u6b63\u786e\n      const sorted = response.data.sort((a, b) => a.index - b.index);\n      return sorted.map(d => d.embedding);\n    }\n    return [];\n  } catch (error) {\n    console.log('Embedding API error:', error.message || error);\n    return [];\n  }\n};\n\n// \u5206\u6279\u5904\u7406\nconst chunk = (arr, size) => {\n  const chunks = [];\n  for (let i = 0; i < arr.length; i += size) {\n    chunks.push(arr.slice(i, i + size));\n  }\n  return chunks;\n};\n\n// ============== \u5b58\u50a8\u7ba1\u7406 ==============\n\nlet storage = { embeddings: [] };\nlet storageMode = 'staticData';\n\ntry {\n  const staticData = this.getWorkflowStaticData\n    ? this.getWorkflowStaticData('global')\n    : this.helpers?.getWorkflowStaticData?.call(this, 'global');\n  if (!staticData) {\n    storageMode = 'volatile';\n  } else {\n    if (!staticData.semanticEmbeddings) staticData.semanticEmbeddings = [];\n    storage = { embeddings: staticData.semanticEmbeddings };\n    // \u4fdd\u6301\u5f15\u7528\u4ee5\u4fbf\u540e\u7eed\u66f4\u65b0\n    storage._staticData = staticData;\n  }\n} catch (err) {\n  storageMode = 'volatile';\n  storage = { embeddings: [] };\n}\n\nconst now = Date.now();\nconst expiryMs = EXPIRY_DAYS * 24 * 60 * 60 * 1000;\n\n// \u6e05\u7406\u8fc7\u671f\u7684 Embedding\nconst originalCount = storage.embeddings.length;\nstorage.embeddings = storage.embeddings.filter(e => (now - e.timestamp) < expiryMs);\nconst expiredCount = originalCount - storage.embeddings.length;\n\n// ============== \u4e3b\u5904\u7406\u903b\u8f91 ==============\n\n// \u51c6\u5907\u5185\u5bb9\u6587\u672c\nconst itemsWithText = items.map((item, index) => ({\n  item,\n  index,\n  text: getContentText(item),\n  id: getContentId(item)\n})).filter(entry => entry.text.length > 10); // \u8fc7\u6ee4\u7a7a\u5185\u5bb9\n\nif (itemsWithText.length === 0) {\n  console.log('Semantic Dedupe: No valid content to process');\n  return items;\n}\n\n// \u6279\u91cf\u83b7\u53d6 Embedding\nconst allTexts = itemsWithText.map(e => e.text);\nconst allEmbeddings = [];\n\nfor (const batch of chunk(allTexts, BATCH_SIZE)) {\n  const batchEmbeddings = await getEmbeddings(batch);\n  allEmbeddings.push(...batchEmbeddings);\n  // \u6dfb\u52a0\u5c0f\u5ef6\u8fdf\u907f\u514dAPI\u9650\u6d41\n  if (batch.length === BATCH_SIZE) {\n    await new Promise(resolve => setTimeout(resolve, 100));\n  }\n}\n\n// \u5982\u679c Embedding \u83b7\u53d6\u5931\u8d25\uff0c\u8fd4\u56de\u539f\u59cb\u5185\u5bb9\nif (allEmbeddings.length !== itemsWithText.length) {\n  console.log(`Semantic Dedupe: Embedding count mismatch (${allEmbeddings.length} vs ${itemsWithText.length}), skipping`);\n  return items;\n}\n\n// \u4e3a\u6bcf\u4e2a\u6761\u76ee\u6dfb\u52a0 Embedding\nitemsWithText.forEach((entry, i) => {\n  entry.embedding = allEmbeddings[i];\n});\n\n// \u4e0e\u5386\u53f2 Embedding \u6bd4\u8f83\uff0c\u627e\u51fa\u91cd\u590d\nconst unique = [];\nconst duplicates = [];\nconst newEmbeddings = [];\n\nfor (const entry of itemsWithText) {\n  if (!entry.embedding || entry.embedding.length === 0) {\n    unique.push(entry.item);\n    continue;\n  }\n\n  let isDuplicate = false;\n  let maxSimilarity = 0;\n  let mostSimilarTitle = '';\n\n  // \u4e0e\u5386\u53f2 Embedding \u6bd4\u8f83\n  for (const historical of storage.embeddings) {\n    const similarity = cosineSimilarity(entry.embedding, historical.embedding);\n    if (similarity > maxSimilarity) {\n      maxSimilarity = similarity;\n      mostSimilarTitle = historical.title || '';\n    }\n    if (similarity >= SIMILARITY_THRESHOLD) {\n      isDuplicate = true;\n      break;\n    }\n  }\n\n  // \u4e0e\u672c\u6279\u6b21\u5df2\u901a\u8fc7\u7684\u5185\u5bb9\u6bd4\u8f83\uff08\u9632\u6b62\u6279\u5185\u91cd\u590d\uff09\n  if (!isDuplicate) {\n    for (const newEmb of newEmbeddings) {\n      const similarity = cosineSimilarity(entry.embedding, newEmb.embedding);\n      if (similarity > maxSimilarity) {\n        maxSimilarity = similarity;\n        mostSimilarTitle = newEmb.title || '';\n      }\n      if (similarity >= SIMILARITY_THRESHOLD) {\n        isDuplicate = true;\n        break;\n      }\n    }\n  }\n\n  if (isDuplicate) {\n    duplicates.push({\n      title: entry.item.json?.title || entry.text.slice(0, 50),\n      similarity: maxSimilarity,\n      similarTo: mostSimilarTitle\n    });\n  } else {\n    unique.push(entry.item);\n    // \u6dfb\u52a0\u5230\u65b0 Embedding \u5217\u8868\n    newEmbeddings.push({\n      id: entry.id,\n      embedding: entry.embedding,\n      title: String(entry.item.json?.title || entry.text.slice(0, 80)),\n      timestamp: now\n    });\n  }\n}\n\n// \u66f4\u65b0\u5b58\u50a8\nstorage.embeddings.push(...newEmbeddings);\n\n// \u5982\u679c\u8d85\u8fc7\u6700\u5927\u5b58\u50a8\u91cf\uff0c\u5220\u9664\u6700\u65e7\u7684\nif (storage.embeddings.length > MAX_EMBEDDINGS) {\n  storage.embeddings.sort((a, b) => b.timestamp - a.timestamp);\n  storage.embeddings = storage.embeddings.slice(0, MAX_EMBEDDINGS);\n}\n\n// \u540c\u6b65\u5230 staticData\nif (storage._staticData) {\n  storage._staticData.semanticEmbeddings = storage.embeddings;\n}\n\n// ============== \u8f93\u51fa\u7edf\u8ba1 ==============\n\nconst stats = {\n  input_count: items.length,\n  processed_count: itemsWithText.length,\n  unique_count: unique.length,\n  duplicate_count: duplicates.length,\n  similarity_threshold: SIMILARITY_THRESHOLD,\n  expired_cleaned: expiredCount,\n  total_stored_embeddings: storage.embeddings.length,\n  storage_mode: storageMode,\n  embedding_model: EMBEDDING_MODEL\n};\n\nconsole.log('Semantic Dedupe Stats:', JSON.stringify(stats));\n\nif (DEBUG && duplicates.length > 0) {\n  console.log('Semantic Duplicates Found:');\n  duplicates.forEach(d => {\n    console.log(`  - \"${d.title.slice(0, 40)}...\" (similarity: ${d.similarity.toFixed(3)}) similar to \"${d.similarTo.slice(0, 40)}...\"`);\n  });\n}\n\n// \u8fd4\u56de\u53bb\u91cd\u540e\u7684\u5185\u5bb9\nreturn unique;\n"
      },
      "name": "Semantic Dedupe",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1540,
        300
      ],
      "id": "semantic-dedupe"
    },
    {
      "parameters": {
        "jsCode": "// LLM Ranking Node Code for n8n\n// Batch-ranks content using OpenAI API to avoid timeouts\n\nconst items = $input.all();\nconst apiKey = $env.OPENAI_API_KEY;\nconst model = $env.OPENAI_MODEL || 'gpt-4o-mini';\nconst maxItems = Number.parseInt($env.LLM_RANK_MAX_ITEMS || '40', 10);\nconst batchSize = Number.parseInt($env.LLM_RANK_BATCH_SIZE || '8', 10);\nconst xRatioRaw = Number.parseFloat($env.LLM_RANK_X_RATIO || '0.7');\nconst xRatio = Number.isFinite(xRatioRaw) ? Math.min(1, Math.max(0, xRatioRaw)) : 0.7;\nconst xAppliedCap = Number.parseInt($env.LLM_RANK_X_PER_SOURCE_CAP_APPLIED || '4', 10);\nconst xResearchCap = Number.parseInt($env.LLM_RANK_X_PER_SOURCE_CAP_RESEARCH || '1', 10);\nconst xDefaultCap = Number.parseInt($env.LLM_RANK_X_PER_SOURCE_CAP_DEFAULT || '2', 10);\n\n// Low-score filtering threshold (default: 18/30 = 60%)\nconst minScoreThreshold = Number.parseInt($env.LLM_RANK_MIN_SCORE || '18', 10);\n// Per-source cap to prevent one source dominating the output\nconst perSourceCap = Number.parseInt($env.LLM_RANK_PER_SOURCE_CAP || '3', 10);\n\nif (!apiKey) {\n  throw new Error('Missing OPENAI_API_KEY for LLM Rank.');\n}\n\nconst normalizeText = (value, maxLen) => {\n  const text = String(value || '').replace(/\\s+/g, ' ').trim();\n  if (!maxLen) return text;\n  return text.length > maxLen ? text.slice(0, maxLen) : text;\n};\n\nconst metricScore = (metrics = {}) => {\n  const like = Number(metrics.like_count || 0);\n  const rt = Number(metrics.retweet_count || 0);\n  const quote = Number(metrics.quote_count || 0);\n  const reply = Number(metrics.reply_count || 0);\n  const impression = Number(metrics.impression_count || 0);\n  return like + rt * 2 + quote * 1.5 + reply * 0.5 + impression * 0.05;\n};\n\nconst parseTime = (value) => {\n  if (!value) return 0;\n  const ts = Date.parse(value);\n  return Number.isFinite(ts) ? ts : 0;\n};\n\nconst prepped = items.map((item, index) => {\n  const data = item.json || {};\n  const metrics = data.metrics || data.public_metrics || {};\n  const source = data.source || '';\n  const sourceType = data.sourceType || (source.startsWith('X -') ? 'X' : 'RSS');\n  return {\n    index,\n    data,\n    metricScore: metricScore(metrics),\n    sourceType,\n    publishedAtScore: parseTime(data.publishedAt || data.created_at || data.pubDate || data.isoDate)\n  };\n});\n\nconst maxItemsSafe = Math.max(1, Math.min(maxItems, prepped.length));\n\nconst xItems = prepped.filter((entry) => entry.sourceType === 'X');\nconst rssItems = prepped.filter((entry) => entry.sourceType !== 'X');\n\nxItems.sort((a, b) => b.metricScore - a.metricScore);\nrssItems.sort((a, b) => b.publishedAtScore - a.publishedAtScore);\n\nlet xQuota = Math.min(xItems.length, Math.round(maxItemsSafe * xRatio));\nlet rssQuota = Math.min(rssItems.length, maxItemsSafe - xQuota);\nif (rssQuota < maxItemsSafe - xQuota) {\n  xQuota = Math.min(xItems.length, maxItemsSafe - rssQuota);\n}\n\n// High-value applied/practical sources - give them higher quota\nconst appliedSources = new Set([\n  'X - ai-agents',\n  'X - ai-workflow',\n  'X - llm-prompts',\n  'X - ai-built',        // NEW: developers shipping things\n  'X - ai-freebies',     // NEW: free resources\n  'X - buildinpublic',   // NEW: indie hackers\n  'X - ai-tips',         // NEW: practical tips\n  'X - @simonw',\n  'X - @swyx',\n  'X - @hwchase17',\n  'X - @goodside'\n]);\n// Lower priority research/theory sources\nconst researchSources = new Set([\n  'X - @ylecun',\n  'X - @sama'  // often macro commentary\n]);\n\nconst xBuckets = new Map();\nxItems.forEach((entry) => {\n  const key = entry.data.source || 'X';\n  if (!xBuckets.has(key)) xBuckets.set(key, []);\n  xBuckets.get(key).push(entry);\n});\n\nconst xSelected = [];\nconst pickedIndexes = new Set();\n\nfor (const [source, bucket] of xBuckets.entries()) {\n  bucket.sort((a, b) => b.metricScore - a.metricScore);\n  let cap = xDefaultCap;\n  if (appliedSources.has(source)) cap = xAppliedCap;\n  if (researchSources.has(source)) cap = xResearchCap;\n  for (const entry of bucket.slice(0, cap)) {\n    if (xSelected.length >= xQuota) break;\n    if (pickedIndexes.has(entry.index)) continue;\n    pickedIndexes.add(entry.index);\n    xSelected.push(entry);\n  }\n  if (xSelected.length >= xQuota) break;\n}\n\nif (xSelected.length < xQuota) {\n  for (const entry of xItems) {\n    if (xSelected.length >= xQuota) break;\n    if (pickedIndexes.has(entry.index)) continue;\n    pickedIndexes.add(entry.index);\n    xSelected.push(entry);\n  }\n}\n\nconst selected = xSelected.concat(rssItems.slice(0, rssQuota));\n\nconst chunk = (arr, size) => {\n  const chunks = [];\n  for (let i = 0; i < arr.length; i += size) {\n    chunks.push(arr.slice(i, i + size));\n  }\n  return chunks;\n};\n\nconst parseJson = (content) => {\n  if (!content) return null;\n  const trimmed = String(content).trim();\n  try {\n    return JSON.parse(trimmed);\n  } catch (error) {\n    const objStart = trimmed.indexOf('{');\n    const objEnd = trimmed.lastIndexOf('}');\n    if (objStart >= 0 && objEnd > objStart) {\n      try {\n        return JSON.parse(trimmed.slice(objStart, objEnd + 1));\n      } catch (err) {\n        // ignore and try array\n      }\n    }\n    const arrStart = trimmed.indexOf('[');\n    const arrEnd = trimmed.lastIndexOf(']');\n    if (arrStart >= 0 && arrEnd > arrStart) {\n      try {\n        return JSON.parse(trimmed.slice(arrStart, arrEnd + 1));\n      } catch (err) {\n        return null;\n      }\n    }\n  }\n  return null;\n};\n\nconst rankedItems = [];\nlet failedBatches = 0;\n\nfor (const group of chunk(selected, Math.max(1, batchSize))) {\n  const payloadItems = group.map((entry) => ({\n    id: entry.index,\n    title: normalizeText(entry.data.title || entry.data.text || '', 120),\n    snippet: normalizeText(entry.data.snippet || entry.data.description || entry.data.summary || '', 280),\n    source: normalizeText(entry.data.source || '', 80),\n    url: normalizeText(entry.data.url || '', 160),\n    source_type: entry.sourceType || 'RSS'\n  }));\n\n  const prompt = `\u4f60\u662f\u4e00\u4e2aAI\u5de5\u5177\u53d1\u70e7\u53cb\uff0c\u5e2e\u6211\u7b5b\u9009\u4eca\u5929\u6700\u503c\u5f97\u5206\u4eab\u7684\u5185\u5bb9\u3002\n\n\u3010\u6211\u60f3\u8981\u7684\u5185\u5bb9\u3011\uff08\u7ed9\u9ad8\u5206 8-10\uff09\n\u2705 GitHub\u70ed\u95e8\u9879\u76ee/\u65b0\u5de5\u5177 - \"\u8fd9\u4e2a\u5e93/\u5de5\u5177\u53ef\u4ee5\u76f4\u63a5\u7528\"\n\u2705 \u5b9e\u8df5\u6559\u7a0b/\u6848\u4f8b - \"\u6709\u4eba\u7528X\u505a\u4e86Y\uff0c\u6548\u679c\u4e0d\u9519\"\n\u2705 \u8585\u7f8a\u6bdb/\u514d\u8d39\u8d44\u6e90 - \"XX\u5f00\u653e\u514d\u8d39\u989d\u5ea6\u4e86\"\n\u2705 \u4ea7\u54c1\u53d1\u5e03/\u66f4\u65b0 - \"XX\u53d1\u5e03\u4e86\u65b0\u529f\u80fd\"\n\u2705 \u6280\u5de7/prompt\u5206\u4eab - \"\u8fd9\u4e2aprompt\u5f88\u597d\u7528\"\n\n\u3010\u6211\u4e0d\u592a\u60f3\u8981\u7684\u5185\u5bb9\u3011\uff08\u7ed9\u4f4e\u5206 0-4\uff09\n\u274c \u7eaf\u5b66\u672f\u8bba\u6587/\u7814\u7a76 - \"\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\"\uff08\u9664\u975e\u6709\u5f00\u6e90\u4ee3\u7801\uff09\n\u274c \u5b8f\u89c2\u89c2\u70b9/\u9884\u6d4b - \"AI\u5c06\u4f1a...\"\n\u274c \u65b0\u95fb\u62a5\u9053/\u878d\u8d44 - \"XX\u516c\u53f8\u83b7\u5f97\u878d\u8d44\"\n\u274c \u80a1\u7968/\u6295\u8d44\u76f8\u5173\n\n\u3010\u8bc4\u5206\u7ef4\u5ea6\u3011\uff08\u6bcf\u98790-10\u5206\uff09\n1. practical: \u5b9e\u7528\u6027 - \u80fd\u5426\u7acb\u5373\u7528\u8d77\u6765\uff1f\u6709\u4ee3\u7801/\u5de5\u5177/\u6559\u7a0b\u5417\uff1f\n2. freshness: \u65f6\u6548\u6027 - \u662f\u65b0\u53d1\u5e03/\u65b0\u53d1\u73b0\u5417\uff1f\n3. shareworthy: \u5206\u4eab\u4ef7\u503c - \u6211\u7684\u5173\u6ce8\u8005\u4f1a\u89c9\u5f97\u6709\u7528\u5417\uff1f\n\n\u8f93\u5165\u6570\u636e\uff1a\n${JSON.stringify(payloadItems, null, 2)}\n\n\u8fd4\u56deJSON\uff08\u4e0d\u8981markdown\uff09\uff1a\n{\n  \"items\": [\n    {\"id\": 0, \"practical\": 8, \"freshness\": 7, \"shareworthy\": 9, \"total\": 24, \"why\": \"\u4e00\u53e5\u8bdd\u8bf4\u660e\u4eae\u70b9\", \"type\": \"tool/tutorial/news/research\"}\n  ]\n}`;\n\n  let batchScores = [];\n  try {\n    const response = await this.helpers.httpRequest({\n      method: 'POST',\n      url: 'https://api.openai.com/v1/chat/completions',\n      headers: {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n      },\n      body: {\n        model,\n        messages: [\n          { role: 'system', content: '\u4f60\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684\u5185\u5bb9\u7b56\u5c55\u4e13\u5bb6\u3002' },\n          { role: 'user', content: prompt }\n        ],\n        temperature: 0.2,\n        response_format: { type: 'json_object' }\n      }\n    });\n\n    const parsed = parseJson(response?.choices?.[0]?.message?.content);\n    if (parsed?.items && Array.isArray(parsed.items)) {\n      batchScores = parsed.items;\n    } else if (Array.isArray(parsed)) {\n      batchScores = parsed;\n    }\n  } catch (error) {\n    failedBatches += 1;\n  }\n\n  const scoreMap = new Map();\n  batchScores.forEach((score) => {\n    if (!score || typeof score.id === 'undefined') return;\n    scoreMap.set(Number(score.id), score);\n  });\n\n  group.forEach((entry) => {\n    const score = scoreMap.get(entry.index);\n    if (score) {\n      // Support both old and new field names for backwards compatibility\n      const practical = Number(score.practical || score.relevance || 0);\n      const freshness = Number(score.freshness || score.credibility || 0);\n      const shareworthy = Number(score.shareworthy || score.novelty || 0);\n      const total = Number.isFinite(score.total)\n        ? Number(score.total)\n        : practical + freshness + shareworthy;\n      rankedItems.push({\n        json: {\n          ...entry.data,\n          score: {\n            practical,\n            freshness,\n            shareworthy,\n            total,\n            why: score.why || '',\n            type: score.type || 'unknown'\n          }\n        }\n      });\n    } else {\n      const fallbackTotal = Math.min(30, Math.round(Math.log1p(entry.metricScore) * 6));\n      rankedItems.push({\n        json: {\n          ...entry.data,\n          score: {\n            practical: 0,\n            freshness: 0,\n            shareworthy: 0,\n            total: fallbackTotal,\n            why: 'LLM\u672a\u8fd4\u56de\u7ed3\u679c\uff0c\u4f7f\u7528\u4e92\u52a8\u6307\u6807\u4f30\u7b97',\n            type: 'unknown'\n          }\n        }\n      });\n    }\n  });\n}\n\nif (failedBatches >= Math.ceil(selected.length / Math.max(1, batchSize))) {\n  throw new Error('LLM Rank failed for all batches. Check OpenAI API key/billing/connectivity.');\n}\n\nrankedItems.sort((a, b) => (b.json.score?.total || 0) - (a.json.score?.total || 0));\n\n// Filter out low-score items (below threshold)\nconst filteredByScore = rankedItems.filter((item) => {\n  const total = item.json.score?.total || 0;\n  return total >= minScoreThreshold;\n});\n\n// Apply per-source cap to prevent one source dominating\nconst sourceCounts = new Map();\nconst filteredBySourceCap = filteredByScore.filter((item) => {\n  const source = item.json.source || 'unknown';\n  const count = sourceCounts.get(source) || 0;\n  if (count >= perSourceCap) return false;\n  sourceCounts.set(source, count + 1);\n  return true;\n});\n\n// Log filtering stats for debugging\nconst statsLog = {\n  input_count: items.length,\n  selected_for_ranking: selected.length,\n  after_llm_ranking: rankedItems.length,\n  after_score_filter: filteredByScore.length,\n  after_source_cap: filteredBySourceCap.length,\n  min_score_threshold: minScoreThreshold,\n  per_source_cap: perSourceCap,\n  filtered_low_score: rankedItems.length - filteredByScore.length,\n  filtered_source_cap: filteredByScore.length - filteredBySourceCap.length\n};\nconsole.log('LLM Rank Stats:', JSON.stringify(statsLog));\n\nreturn filteredBySourceCap;\n"
      },
      "name": "LLM Rank",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1740,
        300
      ],
      "id": "llm-rank"
    },
    {
      "parameters": {
        "jsCode": "// Tweet Generation Node Code for n8n\n// Humanized tweet generation - sounds like a real AI enthusiast, not a news bot\n\nconst items = $input.all();\nconst apiKey = $env.OPENAI_API_KEY;\nconst model = $env.OPENAI_MODEL || 'gpt-4o-mini';\nconst allowTwitterLinks = String($env.TWEET_ALLOW_TWITTER_LINKS || '').toLowerCase() === 'true';\nconst blocklistRaw = $env.TWEET_TONE_BLOCKLIST || 'stupid,idiot,dumb,trash,\u5783\u573e,\u50bb,\u8822,\u611a\u8822,\u8111\u6b8b,\u4ec7\u6068';\nconst blocklist = blocklistRaw.split(',').map(w => w.trim().toLowerCase()).filter(Boolean);\n\n// Persona configuration for humanized tweets\nconst PERSONA = {\n  system: `\u4f60\u662f\u4e00\u4e2a\u6bcf\u5929\u6ce1\u5728X/GitHub/HN\u4e0a\u7684AI\u5de5\u5177\u53d1\u70e7\u53cb\u3002\n\n\u4f60\u7684\u8eab\u4efd\uff1a\n- \u72ec\u7acb\u5f00\u53d1\u8005\uff0c\u65e5\u5e38\u7528AI\u5de5\u5177\u63d0\u6548\n- \u559c\u6b22\u53d1\u73b0\u65b0\u5de5\u5177\u3001\u6d4b\u8bd5\u65b0\u529f\u80fd\n- \u5076\u5c14\u5410\u69fd\u3001\u5076\u5c14\u60ca\u559c\u3001\u5076\u5c14\u7ed9\u5efa\u8bae\n\n\u4f60\u7684\u8bf4\u8bdd\u98ce\u683c\uff1a\n- \u50cf\u8ddf\u670b\u53cb\u804a\u5929\uff0c\u4e0d\u662f\u5199\u65b0\u95fb\u7a3f\n- \u4f1a\u7528\"\u53d1\u73b0\u4e2a\u597d\u4e1c\u897f\"\u3001\"\u8bd5\u4e86\u4e0b\"\u3001\"\u8fd9\u4e2a\u601d\u8def\u6709\u610f\u601d\"\n- \u5076\u5c14\u7528\u53e3\u8bed\u8bcd\uff1a\"\u7edd\u4e86\"\u3001\"\u771f\u9999\"\u3001\"\u6709\u70b9\u4e1c\u897f\"\n- \u4f1a\u95ee\u8bfb\u8005\u95ee\u9898\uff0c\u9080\u8bf7\u4e92\u52a8\n- \u5206\u4eab\u65f6\u8bf4\"\u6211\"\u800c\u4e0d\u662f\"\u672c\u6587\"\n\n\u7edd\u5bf9\u4e0d\u8981\uff1a\n- \u50cf\u65b0\u95fb\u64ad\u62a5\u5458\u4e00\u6837\u6b63\u5f0f\n- \u7528\"\u503c\u5f97\u5173\u6ce8\"\u3001\"\u5f15\u53d1\u70ed\u8bae\"\u8fd9\u79cd\u5b98\u65b9\u8154\n- \u5806\u780c\u5f62\u5bb9\u8bcd\n- \u5199\u6210\u4ea7\u54c1\u5e7f\u544a`,\n\n  styles: [\n    { id: 'discovery', name: '\u53d1\u73b0\u5206\u4eab', desc: '\u5206\u4eab\u4f60\u53d1\u73b0\u7684\u597d\u4e1c\u897f' },\n    { id: 'insight', name: '\u4e2a\u4eba\u6d1e\u5bdf', desc: '\u4f60\u7684\u770b\u6cd5\u548c\u601d\u8003' },\n    { id: 'practical', name: '\u5b9e\u7528\u63a8\u8350', desc: '\u5177\u4f53\u600e\u4e48\u7528\u3001\u9002\u5408\u8c01' }\n  ]\n};\n\n// Collect pipeline statistics for observability\nconst pipelineStats = {\n  total_candidates: items.length,\n  by_source_type: {},\n  by_tier: {},\n  score_distribution: { high: 0, medium: 0, low: 0 },\n  avg_score: 0\n};\n\nlet scoreSum = 0;\nitems.forEach((item) => {\n  const data = item.json || {};\n  const sourceType = data.sourceType || 'RSS';\n  const tier = data.tier || 'unknown';\n  const score = data.score?.total || 0;\n\n  pipelineStats.by_source_type[sourceType] = (pipelineStats.by_source_type[sourceType] || 0) + 1;\n  pipelineStats.by_tier[tier] = (pipelineStats.by_tier[tier] || 0) + 1;\n\n  if (score >= 24) pipelineStats.score_distribution.high++;\n  else if (score >= 18) pipelineStats.score_distribution.medium++;\n  else pipelineStats.score_distribution.low++;\n\n  scoreSum += score;\n});\npipelineStats.avg_score = items.length > 0 ? Math.round(scoreSum / items.length * 10) / 10 : 0;\n\nconst isTwitterUrl = (url) => {\n  const text = String(url || '').toLowerCase();\n  return text.includes('twitter.com/') || text.includes('x.com/') || text.includes('t.co/');\n};\n\nconst eligible = items.filter((item) => {\n  const data = item.json || {};\n  const source = data.source || '';\n  const sourceType = data.sourceType || (source.startsWith('X -') ? 'X' : 'RSS');\n  const url = data.url || data.link || '';\n  if (!url) return false;\n  if (!allowTwitterLinks && isTwitterUrl(url)) return false;\n  if (sourceType === 'X') return false;\n  return true;\n});\n\nconst fallbackEligible = items.filter((item) => {\n  const data = item.json || {};\n  const url = data.url || data.link || '';\n  if (!url) return false;\n  if (!allowTwitterLinks && isTwitterUrl(url)) return false;\n  return true;\n});\n\n// Take top 10 items (prefer non-X sources)\nconst top10 = (eligible.length ? eligible : fallbackEligible).slice(0, 10);\n\nif (!top10.length) {\n  throw new Error('No eligible non-Twitter sources available for tweet generation.');\n}\n\n// Build content list - simplified, focus on what's interesting\nconst contentList = top10.map((item, idx) => {\n  const data = item.json;\n  const sourceTag = data.source?.includes('GitHub') ? '\ud83d\udd27 \u5de5\u5177' :\n                    data.source?.includes('Reddit') ? '\ud83d\udcac \u8ba8\u8bba' :\n                    data.tier === 'A' ? '\ud83d\udce2 \u5b98\u65b9' : '\ud83d\udcf0 \u8d44\u8baf';\n  return `${idx + 1}. [${sourceTag}] ${data.title}\n   ${data.url}\n   \u4eae\u70b9: ${data.score?.why || data.snippet?.substring(0, 100) || ''}`;\n}).join('\\n\\n');\n\nconst prompt = `\u4eca\u65e5AI\u5708\u8fd9\u4e9b\u5185\u5bb9\u6bd4\u8f83\u6709\u610f\u601d\uff1a\n\n${contentList}\n\n---\n\n\u4ece\u4e2d\u9009\u4f60\u6700\u60f3\u5206\u4eab\u76841-2\u6761\uff0c\u7528\u4f60\u7684\u98ce\u683c\u51993\u4e2a\u4e0d\u540c\u89d2\u5ea6\u7684\u63a8\u6587\u3002\n\n\u3010\u98ce\u683c\u8981\u6c42\u3011\n- discovery\uff08\u53d1\u73b0\u578b\uff09: \"\u53d1\u73b0\u4e2a\u597d\u4e1c\u897f...\" / \"\u4eca\u5929\u8bd5\u4e86\u4e0b...\" / \"\u8fd9\u4e2a\u9879\u76ee\u6709\u70b9\u610f\u601d...\"\n- insight\uff08\u6d1e\u5bdf\u578b\uff09: \u4f60\u5bf9\u8fd9\u4e8b\u7684\u770b\u6cd5\uff0c\u53ef\u4ee5\u6709\u6001\u5ea6\uff0c\u4f46\u4e0d\u6760\n- practical\uff08\u5b9e\u7528\u578b\uff09: \u9002\u5408\u8c01\u7528\u3001\u600e\u4e48\u7528\u3001\u6709\u5565\u5751\n\n\u3010\u786c\u6027\u89c4\u5219 - \u5fc5\u987b\u4e25\u683c\u9075\u5b88\u3011\n1. \u6bcf\u6761\u63a8\u6587**\u5fc5\u987b\u4ee5URL\u7ed3\u5c3e**\uff08\u4ece\u4e0a\u9762\u7d20\u6750\u4e2d\u590d\u5236\uff0c\u4e0d\u8981\u81ea\u5df1\u7f16\uff09\n2. \u63a8\u6587\u6587\u5b57 + URL \u603b\u957f\u5ea6 \u2264270\u5b57\u7b26\n3. \u7528\u4e2d\u6587\u5199\uff0c\u53ef\u4ee5\u5939\u82f1\u6587\u672f\u8bed\n4. \u53ef\u4ee5\u95ee\u8bfb\u8005\u95ee\u9898\u589e\u52a0\u4e92\u52a8\n\n\u3010\u63a8\u6587\u683c\u5f0f\u8303\u4f8b\u3011\n\u2705 \"\u53d1\u73b0\u4e2a\u795e\u5668\uff1ayt-dlp \u89c6\u9891\u4e0b\u8f7d\u5de5\u5177\uff0c\u652f\u6301\u6d77\u91cf\u7f51\u7ad9\u3002\u8bd5\u4e86\u4e0b\u901f\u5ea6\u5f88\u5feb\uff0c\u6709\u4eba\u7528\u8fc7\u6ca1\uff1fhttps://github.com/yt-dlp/yt-dlp\"\n\u2705 \"\u8fd9\u4e2a\u601d\u8def\u633a\u91ce\u7684\uff0c\u7528AI\u751f\u6210\u4e13\u4e1a\u5934\u50cf\u3002\u770b\u4e86\u4e0b\u6548\u679c\u786e\u5b9e\u4e0d\u9519\uff0c\u7701\u53bb\u627e\u6444\u5f71\u5e08\u7684\u94b1\u4e86 https://example.com\"\n\u274c \"\u53d1\u73b0\u4e2a\u597d\u4e1c\u897f\uff0cyt-dlp\u8fd9\u4e2a\u89c6\u9891\u4e0b\u8f7d\u5de5\u5177\u771f\u662f\u529f\u80fd\u4e30\u5bcc\" \u2190 **\u7f3aURL\uff0c\u4e0d\u5408\u683c**\n\n\u8fd4\u56deJSON\uff08\u4e0d\u8981markdown\uff09\uff1a\n{\n  \"discovery\": {\n    \"text\": \"\u63a8\u6587\u5185\u5bb9\u3010\u5fc5\u987b\u5305\u542bURL\u3011\",\n    \"source_idx\": 1,\n    \"vibe\": \"\u60ca\u559c/\u597d\u5947/\u63a8\u8350\"\n  },\n  \"insight\": {\n    \"text\": \"\u63a8\u6587\u5185\u5bb9\u3010\u5fc5\u987b\u5305\u542bURL\u3011\",\n    \"source_idx\": 2,\n    \"vibe\": \"\u601d\u8003/\u5410\u69fd/\u8ba4\u540c\"\n  },\n  \"practical\": {\n    \"text\": \"\u63a8\u6587\u5185\u5bb9\u3010\u5fc5\u987b\u5305\u542bURL\u3011\",\n    \"source_idx\": 1,\n    \"vibe\": \"\u5b9e\u7528/\u907f\u5751/\u6280\u5de7\"\n  }\n}`;\n\ntry {\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: 'https://api.openai.com/v1/chat/completions',\n    headers: {\n      'Authorization': `Bearer ${apiKey}`,\n      'Content-Type': 'application/json'\n    },\n    body: {\n      model: model,\n      messages: [\n        { role: 'system', content: PERSONA.system },\n        { role: 'user', content: prompt }\n      ],\n      temperature: 0.85, // Higher for more natural variation\n      response_format: { type: 'json_object' }\n    }\n  });\n\n  const rawTweets = JSON.parse(response.choices[0].message.content);\n\n  // Helper: Check if text contains a URL\n  const hasUrl = (text) => {\n    const urlRegex = /https?:\\/\\/[^\\s]+/;\n    return urlRegex.test(text || '');\n  };\n\n  // Helper: Ensure tweet has URL, add from source if missing\n  const ensureUrl = (tweetText, sourceIdx) => {\n    if (hasUrl(tweetText)) {\n      return tweetText;\n    }\n    // Missing URL - extract from source\n    const source = top10[sourceIdx - 1] || top10[0];\n    const url = source?.json?.url || '';\n    if (!url) {\n      return tweetText; // No URL available\n    }\n    // Add URL at the end with a space\n    return `${tweetText} ${url}`;\n  };\n\n  // Map new format to legacy format for compatibility with Slack output\n  // IMPORTANT: Ensure all tweets have URLs\n  const tweets = {\n    hot_take: {\n      text: ensureUrl(rawTweets.discovery?.text || '', rawTweets.discovery?.source_idx || 1),\n      rationale: `\u98ce\u683c: ${rawTweets.discovery?.vibe || 'discovery'}`,\n      risk: '\u786e\u4fdd\u94fe\u63a5\u6709\u6548',\n      source_idx: rawTweets.discovery?.source_idx\n    },\n    framework: {\n      text: ensureUrl(rawTweets.insight?.text || '', rawTweets.insight?.source_idx || 2),\n      rationale: `\u98ce\u683c: ${rawTweets.insight?.vibe || 'insight'}`,\n      risk: '\u89c2\u70b9\u8868\u8fbe\u9002\u5ea6',\n      source_idx: rawTweets.insight?.source_idx\n    },\n    case: {\n      text: ensureUrl(rawTweets.practical?.text || '', rawTweets.practical?.source_idx || 1),\n      rationale: `\u98ce\u683c: ${rawTweets.practical?.vibe || 'practical'}`,\n      risk: '\u5b9e\u7528\u5efa\u8bae\u9700\u51c6\u786e',\n      source_idx: rawTweets.practical?.source_idx\n    }\n  };\n\n  const hasBlocked = (value) => {\n    const text = String(value || '').toLowerCase();\n    return blocklist.some(word => word && text.includes(word));\n  };\n\n  const buildFallback = (label, source) => {\n    const title = source?.title || source?.snippet || '\u4eca\u65e5AI\u52a8\u6001';\n    const shortTitle = title.length > 60 ? title.substring(0, 60) + '...' : title;\n    const url = source?.url || '';\n    let text = '';\n    if (label === 'hot_take') {\n      text = `\u53d1\u73b0\u4e2a\u6709\u610f\u601d\u7684\uff1a${shortTitle} ${url}`;\n    } else if (label === 'framework') {\n      text = `\u8fd9\u4e2a\u601d\u8def\u53ef\u4ee5\u53c2\u8003\u4e0b\uff1a${shortTitle} ${url}`;\n    } else {\n      text = `\u5206\u4eab\u4e2a\u5b9e\u7528\u7684\uff1a${shortTitle}\uff0c\u611f\u5174\u8da3\u53ef\u4ee5\u770b\u770b ${url}`;\n    }\n    return {\n      text,\n      rationale: '\u4f7f\u7528\u7b80\u5316\u7684\u4eba\u6027\u5316\u6a21\u677f',\n      risk: '\u5185\u5bb9\u8f83\u7b80\u5355',\n      tone_guarded: true\n    };\n  };\n\n  const applyToneGuard = (tweetObj, label, fallbackSource) => {\n    if (!tweetObj || !tweetObj.text) return buildFallback(label, fallbackSource);\n    const blocked = [tweetObj.text, tweetObj.rationale, tweetObj.risk].some(hasBlocked);\n    if (blocked) return buildFallback(label, fallbackSource);\n    return tweetObj;\n  };\n\n  const fallbackSources = [top10[0]?.json, top10[1]?.json, top10[2]?.json];\n\n  // Validate and truncate tweets to ensure 280 character limit\n  const MAX_LENGTH = 280;\n  const validateAndTruncate = (tweetObj) => {\n    if (!tweetObj || !tweetObj.text) return tweetObj;\n\n    const text = tweetObj.text;\n    const length = text.length;\n\n    if (length <= MAX_LENGTH) {\n      return { ...tweetObj, length, truncated: false };\n    }\n\n    // Extract URLs to preserve them\n    const urlRegex = /(https?:\\/\\/[^\\s]+)/g;\n    const urls = text.match(urlRegex) || [];\n\n    // Calculate available space for text (280 - URLs - ellipsis - spaces)\n    const urlsLength = urls.reduce((sum, url) => sum + url.length, 0);\n    const availableSpace = MAX_LENGTH - urlsLength - 3; // 3 for \"...\"\n\n    if (availableSpace < 50) {\n      // If not enough space, just hard truncate\n      return {\n        ...tweetObj,\n        text: text.substring(0, MAX_LENGTH - 3) + '...',\n        length: MAX_LENGTH,\n        truncated: true,\n        original_length: length\n      };\n    }\n\n    // Smart truncate: remove text but keep URLs\n    let textWithoutUrls = text;\n    urls.forEach(url => {\n      textWithoutUrls = textWithoutUrls.replace(url, '');\n    });\n\n    const truncatedText = textWithoutUrls.substring(0, availableSpace).trim();\n    const finalText = truncatedText + '... ' + urls.join(' ');\n\n    return {\n      ...tweetObj,\n      text: finalText,\n      length: finalText.length,\n      truncated: true,\n      original_length: length\n    };\n  };\n\n  // Validate all three tweet types\n  const validatedTweets = {\n    hot_take: validateAndTruncate(applyToneGuard(tweets.hot_take, 'hot_take', fallbackSources[0])),\n    framework: validateAndTruncate(applyToneGuard(tweets.framework, 'framework', fallbackSources[1])),\n    case: validateAndTruncate(applyToneGuard(tweets.case, 'case', fallbackSources[2]))\n  };\n\n  // Log pipeline stats for debugging\n  console.log('Pipeline Stats:', JSON.stringify(pipelineStats));\n\n  return [{\n    json: {\n      tweets: validatedTweets,\n      sources: top10.map(item => item.json),\n      generated_at: new Date().toISOString(),\n      pipeline_stats: pipelineStats\n    }\n  }];\n} catch (error) {\n  throw new Error(`Tweet generation failed: ${error.message}`);\n}\n"
      },
      "name": "Generate Tweets",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1940,
        300
      ],
      "id": "tweet-gen"
    },
    {
      "parameters": {
        "jsCode": "// Slack Block Kit Output Node Code for n8n\n// This code formats and sends the daily pack to Slack\n\nconst data = $input.first().json;\nconst slackToken = $env.SLACK_BOT_TOKEN;\nconst channelId = $env.SLACK_CHANNEL_ID;\nconst xWriteEnabled = String($env.X_WRITE_ENABLED || '').toLowerCase() === 'true';\n\nconst tweets = data.tweets;\nconst sources = data.sources;\nconst modeLine = xWriteEnabled\n  ? '\ud83d\udfe2 \u5b9e\u53d1\u6a21\u5f0f\uff1a\u5df2\u5f00\u542f X \u5199\u5165\uff08X_WRITE_ENABLED=true\uff09'\n  : '\ud83d\udd34 DRY-RUN\uff1a\u672a\u5f00\u542f X \u5199\u5165\uff08X_WRITE_ENABLED=false\uff09';\nconst modeNote = xWriteEnabled\n  ? '\uff08\u5c06\u771f\u5b9e\u53d1\u5e03\u5230 X\uff09'\n  : '\uff08\u9ed8\u8ba4\u4ec5 dry-run\uff0c\u4e0d\u4f1a\u771f\u7684\u53d1\u63a8\uff1b\u9700\u8981\u4f60\u5728\u73af\u5883\u53d8\u91cf\u5f00\u542f X \u5199\u5165\u5f00\u5173\uff09';\nconst instructions = [\n  '\u5728\u672c\u6d88\u606f\u7ebf\u7a0b\u56de\u590d\u4ee5\u4e0b\u6307\u4ee4\u4ee5\u6267\u884c\u52a8\u4f5c\uff1a',\n  '`post 1` \u53d1\u5e03 Option 1',\n  '`post 2` \u53d1\u5e03 Option 2',\n  '`post 3` \u53d1\u5e03 Option 3',\n  modeLine,\n  modeNote,\n].join('\\n');\n\n// Extract Top 3 highlights from sources\n// Priority: Tier A sources, high scores, keywords indicating major changes\nconst highlightKeywords = [\n  'release', 'launch', 'announce', 'new', 'update', 'v2', 'v3', 'v4',\n  'gpt-5', 'claude', 'gemini', 'llama', 'mistral', 'api', 'sdk',\n  '\u53d1\u5e03', '\u66f4\u65b0', '\u5347\u7ea7', '\u65b0\u7248', '\u91cd\u5927', '\u7a81\u7834'\n];\n\nconst getHighlightScore = (source) => {\n  let score = source.score?.total || 0;\n  // Boost Tier A sources significantly\n  if (source.tier === 'A') score += 15;\n  else if (source.tier === 'B') score += 5;\n  // Boost items with highlight keywords\n  const titleLower = (source.title || '').toLowerCase();\n  const snippetLower = (source.snippet || '').toLowerCase();\n  for (const kw of highlightKeywords) {\n    if (titleLower.includes(kw) || snippetLower.includes(kw)) {\n      score += 3;\n      break;\n    }\n  }\n  return score;\n};\n\nconst topHighlights = [...sources]\n  .map(s => ({ ...s, highlightScore: getHighlightScore(s) }))\n  .sort((a, b) => b.highlightScore - a.highlightScore)\n  .slice(0, 3);\n\n// Format highlight text with emoji based on tier\nconst formatHighlight = (item, idx) => {\n  const tierEmoji = item.tier === 'A' ? '\ud83d\udd34' : item.tier === 'B' ? '\ud83d\udfe0' : '\ud83d\udfe1';\n  const title = (item.title || '').substring(0, 60);\n  const source = item.source || 'Unknown';\n  return `${tierEmoji} *${idx + 1}. ${title}*\\n   _\u6765\u6e90: ${source}_`;\n};\n\nconst highlightsText = topHighlights.length > 0\n  ? topHighlights.map((h, i) => formatHighlight(h, i)).join('\\n\\n')\n  : '_\u4eca\u65e5\u65e0\u91cd\u5927\u53d8\u52a8_';\n\n// Build Slack Block Kit message\nconst blocks = [\n  {\n    \"type\": \"header\",\n    \"text\": {\n      \"type\": \"plain_text\",\n      \"text\": \"\ud83d\udce6 Today's X Daily Pack\",\n      \"emoji\": true\n    }\n  },\n  {\n    \"type\": \"context\",\n    \"elements\": [\n      {\n        \"type\": \"mrkdwn\",\n        \"text\": `Generated: ${new Date().toLocaleString('zh-CN', { timeZone: 'Asia/Shanghai' })}`\n      }\n    ]\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*\ud83d\udd25 Top 3 \u91cd\u5927\u53d8\u52a8*\\n\\n${highlightsText}`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*\u2705 \u5ba1\u6838\u4e0e\u53d1\u5e03*\\n${instructions}`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": \"*\ud83c\udfaf \u63a8\u6587\u9009\u9879\uff08\u9009\u4e00\u4e2a\u53d1\u5e03\uff09*\"\n    }\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*Option 1: Hot Take* ${tweets.hot_take.truncated ? '\u26a0\ufe0f _\u5df2\u622a\u65ad_' : ''}\\n${tweets.hot_take.text}\\n\\n_\u5b57\u7b26\u6570: ${tweets.hot_take.length || tweets.hot_take.text.length}/280_${tweets.hot_take.truncated ? ` | _\u539f\u59cb: ${tweets.hot_take.original_length}_` : ''}\\n_\u7406\u7531: ${tweets.hot_take.rationale}_\\n_\u98ce\u9669: ${tweets.hot_take.risk}_`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*Option 2: Framework* ${tweets.framework.truncated ? '\u26a0\ufe0f _\u5df2\u622a\u65ad_' : ''}\\n${tweets.framework.text}\\n\\n_\u5b57\u7b26\u6570: ${tweets.framework.length || tweets.framework.text.length}/280_${tweets.framework.truncated ? ` | _\u539f\u59cb: ${tweets.framework.original_length}_` : ''}\\n_\u7406\u7531: ${tweets.framework.rationale}_\\n_\u98ce\u9669: ${tweets.framework.risk}_`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*Option 3: Case Study* ${tweets.case.truncated ? '\u26a0\ufe0f _\u5df2\u622a\u65ad_' : ''}\\n${tweets.case.text}\\n\\n_\u5b57\u7b26\u6570: ${tweets.case.length || tweets.case.text.length}/280_${tweets.case.truncated ? ` | _\u539f\u59cb: ${tweets.case.original_length}_` : ''}\\n_\u7406\u7531: ${tweets.case.rationale}_\\n_\u98ce\u9669: ${tweets.case.risk}_`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": \"*\ud83d\udcda \u4eca\u65e5\u7d20\u6750\uff08Top 10\uff09*\"\n    }\n  }\n];\n\n// Add sources\nsources.forEach((source, idx) => {\n  blocks.push({\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `${idx + 1}. *${source.title}*\\n\u6765\u6e90: ${source.source} | \u8bc4\u5206: ${source.score?.total || 0}/30\\n<${source.url}|\u67e5\u770b\u94fe\u63a5>`\n    }\n  });\n});\n\n// Add pipeline stats section for observability\nconst stats = data.pipeline_stats;\nif (stats) {\n  const tierBreakdown = Object.entries(stats.by_tier || {})\n    .map(([tier, count]) => `${tier}: ${count}`)\n    .join(' | ');\n  const sourceBreakdown = Object.entries(stats.by_source_type || {})\n    .map(([type, count]) => `${type}: ${count}`)\n    .join(' | ');\n  const scoreDist = stats.score_distribution || {};\n\n  blocks.push(\n    { \"type\": \"divider\" },\n    {\n      \"type\": \"context\",\n      \"elements\": [\n        {\n          \"type\": \"mrkdwn\",\n          \"text\": `\ud83d\udcca *\u8fd0\u884c\u7edf\u8ba1* | \u5019\u9009: ${stats.total_candidates || 0} | \u5e73\u5747\u5206: ${stats.avg_score || 0}/30 | \u9ad8\u5206(\u226524): ${scoreDist.high || 0} | \u4e2d\u5206(18-23): ${scoreDist.medium || 0}`\n        }\n      ]\n    },\n    {\n      \"type\": \"context\",\n      \"elements\": [\n        {\n          \"type\": \"mrkdwn\",\n          \"text\": `\ud83d\udcc1 \u6765\u6e90\u5206\u5e03: ${sourceBreakdown} | \ud83d\udcc8 Tier\u5206\u5e03: ${tierBreakdown}`\n        }\n      ]\n    }\n  );\n}\n\n// Send to Slack\ntry {\n  const packMetadata = {\n    event_type: 'x_daily_pack',\n    event_payload: {\n      version: 1,\n      generated_at: data.generated_at || new Date().toISOString(),\n      tweets: {\n        hot_take: tweets?.hot_take?.text || '',\n        framework: tweets?.framework?.text || '',\n        case: tweets?.case?.text || ''\n      },\n      sources: (sources || []).slice(0, 10).map((s) => ({\n        title: s?.title || '',\n        url: s?.url || '',\n        source: s?.source || '',\n        score: s?.score?.total || 0\n      }))\n    }\n  };\n\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: 'https://slack.com/api/chat.postMessage',\n    headers: {\n      'Authorization': `Bearer ${slackToken}`,\n      'Content-Type': 'application/json'\n    },\n    body: {\n      channel: channelId,\n      blocks: blocks,\n      metadata: packMetadata,\n      text: 'Today\\'s X Daily Pack'\n    }\n  });\n\n  if (!response.ok) {\n    throw new Error(`Slack API error: ${response.error}`);\n  }\n\n  return [{\n    json: {\n      success: true,\n      message_ts: response.ts,\n      channel: response.channel\n    }\n  }];\n} catch (error) {\n  throw new Error(`Slack send failed: ${error.message}`);\n}\n"
      },
      "name": "Send to Slack",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2140,
        300
      ],
      "id": "slack-output"
    },
    {
      "parameters": {
        "jsCode": "// Telegram Output Node - Sends daily pack summary to Telegram\n// Parallel output alongside Slack - requires TELEGRAM_ENABLED=true\n\nconst telegramToken = $env.TELEGRAM_DAILY_BOT_TOKEN;\nconst chatId = $env.TELEGRAM_DAILY_CHAT_ID;\nconst enabled = String($env.TELEGRAM_ENABLED || '').toLowerCase() === 'true';\nconst strictMode = String($env.TELEGRAM_STRICT || 'true').toLowerCase() !== 'false';\n\n// Skip if not enabled or missing config\nif (!enabled) {\n  console.log('Telegram: Disabled (TELEGRAM_ENABLED != true)');\n  return [{ json: { telegram: 'disabled' } }];\n}\n\nif (!telegramToken || !chatId) {\n  throw new Error('Telegram: Missing TELEGRAM_DAILY_BOT_TOKEN or TELEGRAM_DAILY_CHAT_ID');\n}\n\nconst data = $input.first().json;\nconst tweets = data.tweets || {};\nconst sources = data.sources || [];\nconst stats = data.pipeline_stats || {};\n\n// Select top 3 highlights (prefer Tier A, high scores, tools)\nconst highlights = [...sources]\n  .sort((a, b) => {\n    // Boost GitHub/Reddit/ProductHunt sources\n    const aBoost = (a.source?.includes('GitHub') || a.source?.includes('Reddit') || a.source?.includes('Product Hunt')) ? 10 : 0;\n    const bBoost = (b.source?.includes('GitHub') || b.source?.includes('Reddit') || b.source?.includes('Product Hunt')) ? 10 : 0;\n    // Boost Tier A\n    const aTier = a.tier === 'A' ? 5 : (a.tier === 'B' ? 2 : 0);\n    const bTier = b.tier === 'A' ? 5 : (b.tier === 'B' ? 2 : 0);\n    const aScore = (a.score?.total || 0) + aBoost + aTier;\n    const bScore = (b.score?.total || 0) + bBoost + bTier;\n    return bScore - aScore;\n  })\n  .slice(0, 3);\n\n// Format source tag\nconst getSourceTag = (source) => {\n  if (source?.includes('GitHub')) return '\ud83d\udd27';\n  if (source?.includes('Reddit')) return '\ud83d\udcac';\n  if (source?.includes('Product Hunt')) return '\ud83d\ude80';\n  return '\ud83d\udcf0';\n};\n\nconst escapeHtml = (value) => String(value || '')\n  .replace(/&/g, '&amp;')\n  .replace(/</g, '&lt;')\n  .replace(/>/g, '&gt;');\n\nconst escapeAttr = (value) => escapeHtml(encodeURI(String(value || '')));\n\n// Build Telegram message (HTML format)\nconst highlightsText = highlights.map((h, i) => {\n  const tag = getSourceTag(h.source);\n  const title = escapeHtml((h.title || '').substring(0, 60));\n  const shortWhy = escapeHtml((h.score?.why || '').substring(0, 50));\n  const link = h.url ? `<a href=\"${escapeAttr(h.url)}\">\u67e5\u770b\u94fe\u63a5</a>` : '\u67e5\u770b\u94fe\u63a5';\n  return `${tag} <b>${i + 1}. ${title}</b>\\n   ${link}\\n   <i>${shortWhy}</i>`;\n}).join('\\n\\n');\n\n// Pick best tweet (discovery style)\nconst bestTweet = tweets.hot_take?.text || tweets.framework?.text || tweets.case?.text || '';\nconst bestTweetText = escapeHtml(`${bestTweet.substring(0, 250)}${bestTweet.length > 250 ? '...' : ''}`);\nconst highlightsBlock = highlightsText || '<i>\u4eca\u65e5\u65e0\u91cd\u5927\u53d8\u52a8</i>';\n\nconst message = `\ud83d\udce6 <b>AI Frontline Daily</b>\n<i>${escapeHtml(new Date().toLocaleDateString('zh-CN', { timeZone: 'Asia/Shanghai' }))}</i>\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\ud83d\udd25 <b>\u4eca\u65e5\u4eae\u70b9</b>\n\n${highlightsBlock}\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u270d\ufe0f <b>\u63a8\u8350\u63a8\u6587</b>\n\n<pre>${bestTweetText}</pre>\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\ud83d\udcca \u5019\u9009: ${stats.total_candidates || 0} | \u5e73\u5747\u5206: ${stats.avg_score || 0}/30\n\ud83d\udd17 \u5b8c\u6574\u5ba1\u9605\u8bf7\u67e5\u770b Slack`;\n\ntry {\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `https://api.telegram.org/bot${telegramToken}/sendMessage`,\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: {\n      chat_id: chatId,\n      text: message,\n      parse_mode: 'HTML',\n      disable_web_page_preview: false\n    },\n    throwHttpErrors: false\n  });\n\n  if (!response?.ok) {\n    const code = response?.error_code || response?.statusCode || 'unknown';\n    const desc = response?.description || response?.message || 'Unknown error';\n    throw new Error(`Telegram API error ${code}: ${desc}`);\n  }\n\n  console.log('Telegram: Message sent successfully');\n  return [{\n    json: {\n      telegram: 'sent',\n      message_id: response.result?.message_id,\n      chat_id: chatId\n    }\n  }];\n} catch (error) {\n  const message = `Telegram send failed: ${error.message}`;\n  console.error(message);\n  if (strictMode) {\n    throw new Error(message);\n  }\n  return [{ json: { telegram: 'failed', error: error.message } }];\n}\n"
      },
      "name": "Send to Telegram",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2140,
        450
      ],
      "id": "telegram-output"
    }
  ],
  "connections": {
    "Manual Trigger": {
      "main": [
        [
          {
            "node": "RSS Fetch All",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Keyword Search",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Account Search",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Trigger 8AM": {
      "main": [
        [
          {
            "node": "RSS Fetch All",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Keyword Search",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Account Search",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RSS Fetch All": {
      "main": [
        [
          {
            "node": "Merge All",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "X Keyword Search": {
      "main": [
        [
          {
            "node": "Merge X",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "X Account Search": {
      "main": [
        [
          {
            "node": "Merge X",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge X": {
      "main": [
        [
          {
            "node": "Merge All",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge All": {
      "main": [
        [
          {
            "node": "Normalize",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Normalize": {
      "main": [
        [
          {
            "node": "Cross-Day Dedupe",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Cross-Day Dedupe": {
      "main": [
        [
          {
            "node": "Semantic Dedupe",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Rank": {
      "main": [
        [
          {
            "node": "Generate Tweets",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Tweets": {
      "main": [
        [
          {
            "node": "Send to Slack",
            "type": "main",
            "index": 0
          },
          {
            "node": "Send to Telegram",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Semantic Dedupe": {
      "main": [
        [
          {
            "node": "LLM Rank",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1",
    "availableInMCP": false,
    "callerPolicy": "workflowsFromSameOwner",
    "timezone": "Asia/Shanghai"
  }
}