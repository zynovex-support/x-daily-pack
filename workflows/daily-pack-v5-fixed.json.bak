{
  "name": "X Daily Pack v5 - Semantic Dedupe",
  "nodes": [
    {
      "parameters": {
        "path": "x-daily-pack-trigger",
        "responseMode": "onReceived",
        "options": {}
      },
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        0,
        300
      ],
      "id": "webhook-trigger",
      "webhookId": "x-daily-pack-trigger"
    },
    {
      "parameters": {},
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        120,
        300
      ],
      "id": "manual-trigger"
    },
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "cronExpression",
              "expression": "0 0,12 * * *"
            }
          ]
        }
      },
      "name": "Trigger UTC 0h 12h",
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.1,
      "position": [
        240,
        300
      ],
      "id": "trigger"
    },
    {
      "parameters": {
        "jsCode": "// RSS Fetch Node - Dynamically fetches all configured feeds\n// This replaces multiple hardcoded RSS nodes with a single dynamic fetcher\n// Uses n8n's httpRequest helper instead of Node.js http/https modules\n\n// RSS feed configuration (embedded from config/rss-feeds.json)\n// To update: copy feeds array from config/rss-feeds.json\n// Last updated: 2026-01-24 - 33 sources (Phase 2.4 expansion)\nconst feeds = [\n  // Tier A - Official sources (12)\n  { id: \"openai-news\", name: \"OpenAI News\", url: \"https://openai.com/news/rss.xml\", tier: \"A\" },\n  { id: \"deepmind-blog\", name: \"DeepMind Blog\", url: \"https://deepmind.google/blog/rss.xml\", tier: \"A\" },\n  { id: \"google-ai-blog\", name: \"Google AI Blog\", url: \"https://blog.google/technology/ai/rss/\", tier: \"A\" },\n  { id: \"langchain-blog\", name: \"LangChain Blog\", url: \"https://blog.langchain.dev/rss/\", tier: \"A\" },\n  { id: \"huggingface-blog\", name: \"Hugging Face Blog\", url: \"https://huggingface.co/blog/feed.xml\", tier: \"A\" },\n  { id: \"microsoft-ai\", name: \"Microsoft AI Blog\", url: \"https://blogs.microsoft.com/ai/feed/\", tier: \"A\" },\n  { id: \"aws-ml\", name: \"AWS Machine Learning Blog\", url: \"https://aws.amazon.com/blogs/machine-learning/feed/\", tier: \"A\" },\n  { id: \"nvidia-developer\", name: \"Nvidia Developer Blog\", url: \"https://developer.nvidia.com/blog/feed\", tier: \"A\" },\n  { id: \"nvidia-news\", name: \"Nvidia Newsroom\", url: \"https://nvidianews.nvidia.com/rss.xml\", tier: \"A\" },\n  { id: \"meta-engineering\", name: \"Meta Engineering\", url: \"https://engineering.fb.com/feed/\", tier: \"A\" },\n  { id: \"wandb\", name: \"Weights & Biases Blog\", url: \"https://wandb.ai/fully-connected/rss.xml\", tier: \"A\" },\n  { id: \"import-ai\", name: \"Import AI (Jack Clark)\", url: \"https://importai.substack.com/feed\", tier: \"A\" },\n  { id: \"anthropic-news\", name: \"Anthropic News\", url: \"https://raw.githubusercontent.com/Olshansk/rss-feeds/main/feeds/feed_anthropic.xml\", tier: \"A\" },\n  // Tier B - Expert blogs + Media + Reddit + Research (16)\n  { id: \"simonwillison\", name: \"Simon Willison\", url: \"https://simonwillison.net/atom/everything/\", tier: \"B\" },\n  { id: \"latent-space\", name: \"Latent Space\", url: \"https://www.latent.space/feed\", tier: \"B\" },\n  { id: \"interconnects\", name: \"Interconnects\", url: \"https://www.interconnects.ai/feed\", tier: \"B\" },\n  { id: \"lilian-weng\", name: \"Lil'Log (Lilian Weng)\", url: \"https://lilianweng.github.io/index.xml\", tier: \"B\" },\n  { id: \"reddit-localllama\", name: \"Reddit - LocalLLaMA\", url: \"https://www.reddit.com/r/LocalLLaMA/.rss\", tier: \"B\" },\n  { id: \"reddit-machinelearning\", name: \"Reddit - MachineLearning\", url: \"https://www.reddit.com/r/MachineLearning/.rss\", tier: \"B\" },\n  { id: \"producthunt-ai\", name: \"Product Hunt - AI\", url: \"https://www.producthunt.com/feed?category=artificial-intelligence\", tier: \"B\" },\n  { id: \"techcrunch-ai\", name: \"TechCrunch AI\", url: \"https://techcrunch.com/category/artificial-intelligence/feed/\", tier: \"B\" },\n  { id: \"venturebeat-ai\", name: \"VentureBeat AI\", url: \"https://venturebeat.com/category/ai/feed/\", tier: \"B\" },\n  { id: \"mit-tech-review\", name: \"MIT Technology Review\", url: \"https://www.technologyreview.com/feed/\", tier: \"B\" },\n  { id: \"theverge-ai\", name: \"The Verge AI\", url: \"https://www.theverge.com/rss/ai-artificial-intelligence/index.xml\", tier: \"B\" },\n  { id: \"wired-ai\", name: \"Wired AI\", url: \"https://www.wired.com/feed/tag/ai/latest/rss\", tier: \"B\" },\n  { id: \"infoq-ai\", name: \"InfoQ AI/ML\", url: \"https://feed.infoq.com/ai-ml-data-eng/\", tier: \"B\" },\n  { id: \"arxiv-ai\", name: \"ArXiv AI\", url: \"https://rss.arxiv.org/rss/cs.AI\", tier: \"B\" },\n  { id: \"36kr\", name: \"36Kr\", url: \"https://36kr.com/feed\", tier: \"B\" },\n  // Tier C - Community + Tools\n  { id: \"github-trending-python\", name: \"GitHub Trending - Python\", url: \"https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml\", tier: \"C\" },\n  { id: \"github-trending-all\", name: \"GitHub Trending - All\", url: \"https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml\", tier: \"C\" },\n  { id: \"reddit-chatgpt\", name: \"Reddit - ChatGPT\", url: \"https://www.reddit.com/r/ChatGPT/.rss\", tier: \"C\" },\n  { id: \"hackernews-best\", name: \"Hacker News - Best\", url: \"https://hnrss.org/best?count=20\", tier: \"C\" },\n  { id: \"hackernews-ai\", name: \"Hacker News - AI\", url: \"https://hnrss.org/newest?q=AI+OR+GPT+OR+LLM&count=15\", tier: \"C\" },\n  // Tier D - Aggregators\n  { id: \"google-news-ai\", name: \"Google News - AI\", url: \"https://news.google.com/rss/search?q=artificial+intelligence+OR+AI&hl=en-US&gl=US&ceid=US:en\", tier: \"D\" }\n];\n\nconst maxItemsPerFeed = Number.parseInt($env.RSS_MAX_ITEMS_PER_FEED || '15', 10);\nconst timeoutMs = Number.parseInt($env.RSS_FETCH_TIMEOUT_MS || '15000', 10);\nconst maxRetries = Number.parseInt($env.RSS_RETRY_MAX_ATTEMPTS || '3', 10);\nconst retryDelayMs = Number.parseInt($env.RSS_RETRY_INITIAL_DELAY_MS || '500', 10);\n\n// Exponential backoff retry function\nconst retryWithBackoff = async (fn, maxAttempts = maxRetries, initialDelayMs = retryDelayMs) => {\n  let lastError;\n  for (let attempt = 1; attempt <= maxAttempts; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      lastError = error;\n      const isRetryable = /timeout|ETIMEDOUT|ECONNRESET|ECONNREFUSED|429|503|502/i.test(error.message);\n      if (!isRetryable || attempt === maxAttempts) throw error;\n      const delayMs = initialDelayMs * Math.pow(2, attempt - 1);\n      console.log(`[RSS Retry] Attempt ${attempt}/${maxAttempts} failed, retrying in ${delayMs}ms`);\n      await new Promise(r => setTimeout(r, delayMs));\n    }\n  }\n  throw lastError;\n};\n\nconst parseRssDate = (dateStr) => {\n  if (!dateStr) return null;\n  try {\n    const d = new Date(dateStr);\n    return isNaN(d.getTime()) ? null : d.toISOString();\n  } catch (e) {\n    return null;\n  }\n};\n\nconst extractText = (xml, tag) => {\n  const regex = new RegExp(`<${tag}[^>]*>([\\\\s\\\\S]*?)</${tag}>`, 'i');\n  const match = xml.match(regex);\n  if (!match) return '';\n  let text = match[1].replace(/<!\\[CDATA\\[([\\s\\S]*?)\\]\\]>/g, '$1');\n  text = text.replace(/<[^>]+>/g, '');\n  text = text.replace(/&amp;/g, '&').replace(/&lt;/g, '<').replace(/&gt;/g, '>').replace(/&quot;/g, '\"').replace(/&#39;/g, \"'\");\n  return text.trim();\n};\n\nconst extractLink = (itemXml) => {\n  const hrefMatch = itemXml.match(/<link[^>]+href=[\"']([^\"']+)[\"']/i);\n  if (hrefMatch) return hrefMatch[1];\n  return extractText(itemXml, 'link');\n};\n\nconst parseItems = (xml, feedName, tier) => {\n  const items = [];\n  const itemRegex = /<(item|entry)[\\s>]([\\s\\S]*?)<\\/\\1>/gi;\n  let match;\n\n  while ((match = itemRegex.exec(xml)) !== null && items.length < maxItemsPerFeed) {\n    const itemXml = match[2];\n    const title = extractText(itemXml, 'title');\n    const link = extractLink(itemXml);\n    const description = extractText(itemXml, 'description') || extractText(itemXml, 'summary') || extractText(itemXml, 'content');\n    const pubDate = extractText(itemXml, 'pubDate') || extractText(itemXml, 'published') || extractText(itemXml, 'updated');\n\n    if (title && link) {\n      items.push({\n        title: title.substring(0, 200),\n        url: link,\n        source: feedName,\n        sourceType: 'RSS',\n        tier: tier,\n        snippet: description.substring(0, 300),\n        publishedAt: parseRssDate(pubDate)\n      });\n    }\n  }\n\n  return items;\n};\n\nconst allItems = [];\nconst errors = [];\n\nconst fetchPromises = feeds.map(async (feed) => {\n  try {\n    const response = await retryWithBackoff(async () => {\n      return await this.helpers.httpRequest({\n        method: 'GET',\n        url: feed.url,\n        headers: {\n          'User-Agent': 'n8n-rss-fetcher/1.0',\n          'Accept': 'application/rss+xml, application/atom+xml, application/xml, text/xml'\n        },\n        timeout: timeoutMs,\n        returnFullResponse: false\n      });\n    });\n\n    const xml = typeof response === 'string' ? response : JSON.stringify(response);\n    const items = parseItems(xml, feed.name, feed.tier);\n    return { feed: feed.id, items, error: null, retried: false };\n  } catch (error) {\n    return { feed: feed.id, items: [], error: error.message, retried: true };\n  }\n});\n\nconst results = await Promise.all(fetchPromises);\n\nresults.forEach(result => {\n  if (result.error) {\n    errors.push({ feed: result.feed, error: result.error });\n  } else {\n    allItems.push(...result.items);\n  }\n});\n\nconst stats = {\n  total_feeds: feeds.length,\n  successful_feeds: results.filter(r => !r.error).length,\n  failed_feeds: errors.length,\n  total_items: allItems.length,\n  errors: errors\n};\nconsole.log('RSS Fetch Stats:', JSON.stringify(stats));\n\nif (allItems.length === 0 && errors.length === feeds.length) {\n  throw new Error(`All RSS feeds failed: ${JSON.stringify(errors)}`);\n}\n\nreturn allItems.map(item => ({ json: item }));\n"
      },
      "name": "RSS Fetch All",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        440,
        200
      ],
      "id": "rss-fetch"
    },
    {
      "parameters": {
        "jsCode": "// X Keyword Search Node - Calls Rube MCP (Streamable HTTP)\n// Searches X/Twitter using RUBE_MULTI_EXECUTE_TOOL\n\nconst rubeUrl = $env.RUBE_MCP_URL || 'https://rube.app/mcp';\nconst rubeToken = $env.RUBE_AUTH_TOKEN || $env.RUBE_API_TOKEN;\n\nif (!rubeToken) {\n  throw new Error('Missing Rube token. Set RUBE_AUTH_TOKEN (or RUBE_API_TOKEN).');\n}\n\n// 默认关键词查询 (fallback)\nconst DEFAULT_KEYWORD_QUERIES = [\n  { id: 'ai-agents', query: '(AI agent OR AI agents OR autonomous agent OR agentic) -is:retweet -is:reply lang:en' },\n  { id: 'ai-workflow', query: '(AI workflow OR AI automation OR AI productivity OR \"AI tools\") -is:retweet -is:reply lang:en' },\n  { id: 'llm-prompts', query: '(LLM OR \"prompt engineering\" OR \"Claude\" OR \"GPT-4\" OR \"ChatGPT\") (tutorial OR guide OR tips) -is:retweet -is:reply lang:en' },\n  { id: 'ai-built', query: '(\"I built\" OR \"I made\" OR \"just shipped\" OR \"just launched\") (AI OR GPT OR Claude OR agent) -is:retweet -is:reply lang:en' },\n  { id: 'ai-freebies', query: '(\"free tier\" OR \"free credits\" OR \"free API\" OR \"open source\") (AI OR LLM OR GPT OR Claude) -is:retweet -is:reply lang:en' },\n  { id: 'buildinpublic', query: '(#buildinpublic OR #indiehackers) (AI OR GPT OR Claude OR LLM) -is:retweet -is:reply lang:en' },\n  { id: 'ai-tips', query: '(\"pro tip\" OR \"life hack\" OR \"game changer\") (AI OR ChatGPT OR Claude) -is:retweet -is:reply lang:en' }\n];\n\n// 从配置服务器获取查询\nconst CONFIG_URL = $env.CONFIG_SERVER_URL || 'http://localhost:3001';\nlet keywordQueries = DEFAULT_KEYWORD_QUERIES;\n\ntry {\n  const configResp = await this.helpers.httpRequest({\n    method: 'GET',\n    url: `${CONFIG_URL}/queries/x-keywords`,\n    timeout: 5000\n  });\n  const data = typeof configResp === 'string' ? JSON.parse(configResp) : configResp;\n  if (data.queries && data.queries.length > 0) {\n    keywordQueries = data.queries;\n    console.log(`[X Keywords] Loaded ${keywordQueries.length} queries from config server`);\n  }\n} catch (err) {\n  console.log(`[X Keywords] Config fetch failed (${err.message}), using defaults`);\n}\n\nlet mcpProtocolVersion = '2025-06-18';\nlet mcpSessionId = null;\nlet requestId = 1;\n\nconst allTweets = [];\nconst seenTweetIds = new Set();\n\nconst getHeader = (headers, name) => {\n  if (!headers) return null;\n  const key = Object.keys(headers).find(k => k.toLowerCase() === name.toLowerCase());\n  return key ? headers[key] : null;\n};\n\nconst parseSseEvents = (text) => {\n  const events = [];\n  const lines = String(text || '').split('\\n');\n  for (let i = 0; i < lines.length; i += 1) {\n    const line = lines[i].trim();\n    if (!line.startsWith('data:')) continue;\n    const payload = line.slice(5).trim();\n    if (!payload || payload === '[DONE]') continue;\n    try {\n      events.push(JSON.parse(payload));\n    } catch (err) {\n      continue;\n    }\n  }\n  return events;\n};\n\nconst parseSse = (text) => {\n  const events = parseSseEvents(text);\n  return events.length ? events[events.length - 1] : null;\n};\n\nconst parseBody = (body) => {\n  if (!body) return null;\n  if (typeof body === 'string') {\n    const trimmed = body.trim();\n    if (trimmed.startsWith('{')) {\n      try {\n        return JSON.parse(trimmed);\n      } catch (err) {\n        return null;\n      }\n    }\n    return parseSse(trimmed);\n  }\n  return body;\n};\n\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n\nconst isTransientRubeError = (message) => {\n  if (!message) return false;\n  return /tools failed|rate limit|temporar|timeout|429|503/i.test(message);\n};\n\nconst normalizeRubeBody = (body) => {\n  if (!body || typeof body !== 'object') return null;\n  return body.result || body;\n};\n\nconst unwrapData = (obj) => {\n  let current = obj;\n  for (let i = 0; i < 2; i += 1) {\n    if (current && typeof current === 'object' && current.data) {\n      current = current.data;\n    }\n  }\n  return current;\n};\n\nconst extractRubePayloads = (body) => {\n  const root = normalizeRubeBody(body);\n  const candidates = [];\n  if (!root) return candidates;\n  if (root.data) candidates.push(root.data);\n  if (Array.isArray(root.content)) {\n    root.content.forEach((entry) => {\n      if (entry?.json) candidates.push(entry.json);\n      if (entry?.data) candidates.push(entry.data);\n      if (entry?.text) {\n        try {\n          candidates.push(JSON.parse(entry.text));\n        } catch (err) {\n          candidates.push(entry.text);\n        }\n      }\n    });\n  }\n  candidates.push(root);\n  return candidates;\n};\n\nconst extractRubeError = (body) => {\n  if (!body) return null;\n  if (typeof body === 'string') {\n    return /tools failed|error|failed/i.test(body) ? body : null;\n  }\n  if (body.error?.message) return body.error.message;\n  if (typeof body.error === 'string') return body.error;\n\n  const root = normalizeRubeBody(body) || body;\n  if (root?.error?.message) return root.error.message;\n  if (typeof root?.error === 'string') return root.error;\n\n  const candidates = extractRubePayloads(body);\n  for (const candidate of candidates) {\n    if (!candidate) continue;\n    if (typeof candidate === 'string') {\n      if (/tools failed|error|failed/i.test(candidate)) return candidate;\n      continue;\n    }\n    if (candidate.error?.message) return candidate.error.message;\n    if (typeof candidate.error === 'string') return candidate.error;\n    if (candidate.message && /error|failed/i.test(String(candidate.message))) return candidate.message;\n  }\n  return null;\n};\n\nconst extractRubeDetails = (body) => {\n  const candidates = extractRubePayloads(body);\n  for (const candidate of candidates) {\n    if (!candidate || typeof candidate !== 'object') continue;\n    const details = candidate.details || candidate.detail;\n    if (details?.requestId) return { requestId: details.requestId };\n    if (candidate.requestId) return { requestId: candidate.requestId };\n    if (candidate.log_id) return { logId: candidate.log_id };\n  }\n  return {};\n};\n\nconst assertRubeSuccess = (body, label) => {\n  const err = extractRubeError(body);\n  if (err) {\n    const details = extractRubeDetails(body);\n    const suffixParts = [];\n    if (details.requestId) suffixParts.push(`requestId=${details.requestId}`);\n    if (details.logId) suffixParts.push(`logId=${details.logId}`);\n    const suffix = suffixParts.length ? ` (${suffixParts.join(', ')})` : '';\n    throw new Error(`${label} failed: ${err}${suffix}`);\n  }\n};\n\nconst extractSearchResult = (body) => {\n  for (const candidate of extractRubePayloads(body)) {\n    const data = unwrapData(candidate);\n    if (data?.results || data?.session_id || data?.session) return data;\n  }\n  return null;\n};\n\nconst extractConnectionResult = (body) => {\n  for (const candidate of extractRubePayloads(body)) {\n    const data = unwrapData(candidate);\n    if (data?.connections || data?.active_connection !== undefined) return data;\n    if (data?.results || data?.toolkit_connection_statuses) return data;\n  }\n  return null;\n};\n\nconst resolveSessionId = (searchData) => {\n  const data = unwrapData(searchData);\n  if (!data) return null;\n  if (data.session?.id) return data.session.id;\n  if (data.session_id) return data.session_id;\n  const first = Array.isArray(data.results) ? data.results[0] : null;\n  if (first?.session_id) return first.session_id;\n  if (first?.session?.id) return first.session.id;\n  return null;\n};\n\nconst resolveToolContext = (searchData) => {\n  const data = unwrapData(searchData);\n  const results = Array.isArray(data?.results) ? data.results : [];\n  const primary = results[0] || {};\n  const mainTools = Array.isArray(primary?.main_tools)\n    ? primary.main_tools\n    : (Array.isArray(primary?.tools) ? primary.tools : []);\n  let toolSlug = primary?.primary_tool_slugs?.[0] || mainTools[0]?.tool_slug || mainTools[0]?.name || null;\n  if (!toolSlug) {\n    const schemaKeys = Object.keys(data?.tool_schemas || {});\n    if (schemaKeys.length > 0) {\n      toolSlug = schemaKeys.includes('TWITTER_RECENT_SEARCH') ? 'TWITTER_RECENT_SEARCH' : schemaKeys[0];\n    }\n  }\n  const toolkits = [];\n  if (Array.isArray(primary?.toolkits)) toolkits.push(...primary.toolkits);\n  if (Array.isArray(data?.toolkits)) toolkits.push(...data.toolkits);\n  if (Array.isArray(data?.toolkit_connection_statuses)) {\n    data.toolkit_connection_statuses.forEach((entry) => {\n      if (entry?.toolkit) toolkits.push(entry.toolkit);\n    });\n  }\n  if (mainTools[0]?.toolkit_name) toolkits.push(mainTools[0].toolkit_name);\n  if (mainTools[0]?.toolkit) toolkits.push(mainTools[0].toolkit);\n  let activeConnection = primary?.active_connection;\n  if (activeConnection === undefined && Array.isArray(data?.toolkit_connection_statuses)) {\n    if (data.toolkit_connection_statuses.length > 0) {\n      activeConnection = data.toolkit_connection_statuses.every((entry) => entry?.has_active_connection !== false);\n    }\n  }\n  return {\n    toolSlug,\n    toolkits: [...new Set(toolkits.filter(Boolean))],\n    activeConnection\n  };\n};\n\nconst findTweetsEnvelope = (obj) => {\n  if (!obj || typeof obj !== 'object') return null;\n  if (Array.isArray(obj.data)) return obj;\n  if (obj.data && Array.isArray(obj.data.data)) return obj.data;\n  return null;\n};\n\nconst findMultiExecuteEnvelope = (obj) => {\n  const results = obj?.data?.data?.results;\n  if (!Array.isArray(results)) return null;\n  for (const result of results) {\n    const envelope = findTweetsEnvelope(result?.response?.data);\n    if (envelope) return envelope;\n  }\n  return null;\n};\n\nconst extractTwitterPayload = (response) => {\n  const root = response?.result || response;\n  const candidates = [];\n  if (root?.data) candidates.push(root.data);\n  if (Array.isArray(root?.content)) {\n    root.content.forEach((entry) => {\n      if (entry?.json) candidates.push(entry.json);\n      if (entry?.data) candidates.push(entry.data);\n      if (entry?.text) {\n        try {\n          candidates.push(JSON.parse(entry.text));\n        } catch (err) {\n          // ignore parse errors\n        }\n      }\n    });\n  }\n  candidates.push(root);\n  for (const candidate of candidates) {\n    const envelope = findTweetsEnvelope(candidate);\n    if (envelope) return envelope;\n    const multiEnvelope = findMultiExecuteEnvelope(candidate);\n    if (multiEnvelope) return multiEnvelope;\n  }\n  return null;\n};\n\nconst mcpPost = async (payload, includeProtocolHeader = true) => {\n  const headers = {\n    Authorization: `Bearer ${rubeToken}`,\n    'Content-Type': 'application/json',\n    Accept: 'application/json, text/event-stream'\n  };\n  if (includeProtocolHeader && mcpProtocolVersion) headers['MCP-Protocol-Version'] = mcpProtocolVersion;\n  if (includeProtocolHeader && mcpSessionId) headers['Mcp-Session-Id'] = mcpSessionId;\n\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: rubeUrl,\n    headers,\n    body: payload,\n    returnFullResponse: true,\n    responseFormat: 'string'\n  });\n\n  const rawBody = response?.body ?? response;\n  const parsedBody = parseBody(rawBody);\n  return {\n    body: parsedBody || rawBody,\n    headers: response?.headers\n  };\n};\n\nconst initializeMcp = async () => {\n  const initPayload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'initialize',\n    params: {\n      protocolVersion: mcpProtocolVersion,\n      capabilities: {},\n      clientInfo: { name: 'n8n', version: '1.0.0' }\n    }\n  };\n\n  const initResponse = await mcpPost(initPayload, false);\n  const initResult = initResponse.body?.result;\n  if (initResult?.protocolVersion) {\n    mcpProtocolVersion = initResult.protocolVersion;\n  }\n\n  const sessionHeader = getHeader(initResponse.headers, 'mcp-session-id');\n  if (sessionHeader) {\n    mcpSessionId = Array.isArray(sessionHeader) ? sessionHeader[0] : sessionHeader;\n  }\n\n  await mcpPost({ jsonrpc: '2.0', method: 'notifications/initialized' }, true);\n};\n\nconst searchTools = async (query) => {\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_SEARCH_TOOLS',\n      arguments: {\n        queries: [\n          {\n            use_case: 'search recent tweets on twitter',\n            known_fields: `query: ${query}`\n          }\n        ],\n        session: { generate_id: true }\n      }\n    }\n  };\n\n  const response = await mcpPost(payload, true);\n  assertRubeSuccess(response.body, 'Rube search tools');\n  const searchData = extractSearchResult(response.body);\n  if (!searchData) throw new Error('Rube search tools returned empty payload');\n  return searchData;\n};\n\nconst ensureActiveConnections = async (toolkits, sessionId) => {\n  if (!Array.isArray(toolkits) || toolkits.length === 0) return;\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_MANAGE_CONNECTIONS',\n      arguments: {\n        toolkits,\n        session_id: sessionId\n      }\n    }\n  };\n\n  const response = await mcpPost(payload, true);\n  assertRubeSuccess(response.body, 'Rube manage connections');\n  const connData = extractConnectionResult(response.body);\n  if (!connData) return;\n\n  const toolkitStatuses = connData.toolkit_connection_statuses;\n  if (Array.isArray(toolkitStatuses) && toolkitStatuses.length > 0) {\n    const inactive = toolkitStatuses.find((entry) => entry?.has_active_connection === false);\n    if (inactive) {\n      const detail = inactive.status_message || inactive.description || 'inactive';\n      throw new Error(`Rube connection not ACTIVE: ${detail}`);\n    }\n  }\n\n  const results = connData.results;\n  if (results && typeof results === 'object' && !Array.isArray(results)) {\n    const entries = Object.values(results);\n    const inactive = entries.find((entry) => {\n      if (!entry || typeof entry !== 'object') return false;\n      if (entry.has_active_connection === false) return true;\n      const status = entry.connection_status || entry.status;\n      return status ? String(status).toUpperCase() !== 'ACTIVE' : false;\n    });\n    if (inactive) {\n      const status = inactive.connection_status || inactive.status || 'UNKNOWN';\n      const detail = inactive.instruction || inactive.status_message || inactive.description;\n      const suffix = detail ? ` - ${detail}` : '';\n      throw new Error(`Rube connection not ACTIVE: ${status}${suffix}`);\n    }\n  }\n\n  const connections = connData.connections || [];\n  if (Array.isArray(connections) && connections.length > 0) {\n    const inactive = connections.find((conn) => {\n      const status = conn.connection_status || conn.status;\n      return status && status !== 'ACTIVE';\n    });\n    if (inactive) {\n      const status = inactive.connection_status || inactive.status || 'UNKNOWN';\n      const redirect = inactive.redirect_url ? ` (open: ${inactive.redirect_url})` : '';\n      throw new Error(`Rube connection not ACTIVE: ${status}${redirect}`);\n    }\n  } else if (connData.active_connection === false) {\n    throw new Error('Rube connection not ACTIVE');\n  }\n};\n\nconst executeWithRetry = async (payload, label, maxAttempts = 3) => {\n  let lastError;\n  for (let attempt = 1; attempt <= maxAttempts; attempt += 1) {\n    try {\n      const response = await mcpPost(payload, true);\n      assertRubeSuccess(response.body, label);\n      return response;\n    } catch (error) {\n      lastError = error;\n      const message = error?.message || String(error);\n      if (attempt < maxAttempts && isTransientRubeError(message)) {\n        await sleep(600 * attempt);\n        continue;\n      }\n      throw error;\n    }\n  }\n  throw lastError;\n};\n\ntry {\n  await initializeMcp();\n\n  const seedQuery = keywordQueries[0]?.query || 'AI agent -is:retweet lang:en';\n  const searchData = await searchTools(seedQuery);\n  const sessionId = resolveSessionId(searchData);\n  if (!sessionId) throw new Error('Rube search tools missing session_id');\n\n  const toolContext = resolveToolContext(searchData);\n  if (!toolContext.toolSlug) throw new Error('Rube search tools missing tool slug');\n\n  const resolvedToolkits = toolContext.toolkits.length\n    ? toolContext.toolkits\n    : (toolContext.toolSlug?.startsWith('TWITTER_') ? ['twitter'] : []);\n  await ensureActiveConnections(resolvedToolkits, sessionId);\n\n  for (let idx = 0; idx < keywordQueries.length; idx += 1) {\n    const keywordQuery = keywordQueries[idx];\n    const payload = {\n      jsonrpc: '2.0',\n      id: requestId++,\n      method: 'tools/call',\n      params: {\n        name: 'RUBE_MULTI_EXECUTE_TOOL',\n        arguments: {\n          tools: [\n            {\n              tool_slug: toolContext.toolSlug,\n              arguments: {\n                query: keywordQuery.query,\n                max_results: 20,\n                tweet_fields: ['created_at', 'public_metrics', 'author_id'],\n                expansions: ['author_id'],\n                user_fields: ['username', 'name']\n              }\n            }\n          ],\n          sync_response_to_workbench: false,\n          memory: {},\n          session_id: sessionId,\n          current_step: 'FETCH_TWEETS',\n          current_step_metric: `${idx + 1}/${keywordQueries.length}`\n        }\n      }\n    };\n\n    const response = await executeWithRetry(payload, `Rube X keyword search (${keywordQuery.id})`);\n    const twitterPayload = extractTwitterPayload(response.body);\n    if (!twitterPayload) {\n      console.log(`Rube X keyword search returned empty payload: ${keywordQuery.id}`);\n    }\n    const tweets = twitterPayload?.data || [];\n    const users = twitterPayload?.includes?.users || [];\n\n    const userMap = {};\n    users.forEach((user) => {\n      userMap[user.id] = user;\n    });\n\n    tweets.forEach((tweet) => {\n      if (seenTweetIds.has(tweet.id)) return;\n      seenTweetIds.add(tweet.id);\n      const author = userMap[tweet.author_id] || {};\n      const username = author.username || 'unknown';\n      allTweets.push({\n        title: tweet.text.substring(0, 100) + (tweet.text.length > 100 ? '...' : ''),\n        url: `https://twitter.com/${username}/status/${tweet.id}`,\n        source: `X - ${keywordQuery.id}`,\n        snippet: tweet.text,\n        publishedAt: tweet.created_at,\n        author: username,\n        metrics: tweet.public_metrics || {},\n        sourceType: 'X',\n        tier: 'B'\n      });\n    });\n\n    if (idx < keywordQueries.length - 1) {\n      await sleep(400);\n    }\n  }\n\n  return allTweets.map(tweet => ({ json: tweet }));\n} catch (error) {\n  throw new Error(`X keyword search failed: ${error.message}`);\n}\n"
      },
      "name": "X Keyword Search",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        440,
        350
      ],
      "id": "x-keyword"
    },
    {
      "parameters": {
        "jsCode": "// X Account Timeline Node - Calls Rube MCP (Streamable HTTP)\n// Fetches recent tweets from configured accounts using RUBE_MULTI_EXECUTE_TOOL\n\nconst rubeUrl = $env.RUBE_MCP_URL || 'https://rube.app/mcp';\nconst rubeToken = $env.RUBE_AUTH_TOKEN || $env.RUBE_API_TOKEN;\n\nif (!rubeToken) {\n  throw new Error('Missing Rube token. Set RUBE_AUTH_TOKEN (or RUBE_API_TOKEN).');\n}\n\n// 默认账号查询 (fallback)\nconst DEFAULT_ACCOUNT_QUERY = 'from:AnthropicAI OR from:OpenAI OR from:LangChainAI OR from:hwchase17 OR from:karpathy OR from:sama OR from:ylecun OR from:goodside OR from:simonw OR from:swyx OR from:nvidia OR from:awscloud OR from:Microsoft OR from:GoogleAI OR from:ycombinator -is:retweet -giveaway -airdrop';\n\n// 从配置服务器获取查询\nconst CONFIG_URL = $env.CONFIG_SERVER_URL || 'http://localhost:3001';\nlet accountQuery = DEFAULT_ACCOUNT_QUERY;\n\ntry {\n  const configResp = await this.helpers.httpRequest({\n    method: 'GET',\n    url: `${CONFIG_URL}/queries/x-accounts`,\n    timeout: 5000\n  });\n  const data = typeof configResp === 'string' ? JSON.parse(configResp) : configResp;\n  if (data.query) {\n    accountQuery = data.query;\n    console.log(`[X Accounts] Loaded query from config server`);\n  }\n} catch (err) {\n  console.log(`[X Accounts] Config fetch failed (${err.message}), using defaults`);\n}\n\nlet mcpProtocolVersion = '2025-06-18';\nlet mcpSessionId = null;\nlet requestId = 1;\n\nconst allTweets = [];\nconst seenTweetIds = new Set();\n\nconst getHeader = (headers, name) => {\n  if (!headers) return null;\n  const key = Object.keys(headers).find(k => k.toLowerCase() === name.toLowerCase());\n  return key ? headers[key] : null;\n};\n\nconst parseSseEvents = (text) => {\n  const events = [];\n  const lines = String(text || '').split('\\n');\n  for (let i = 0; i < lines.length; i += 1) {\n    const line = lines[i].trim();\n    if (!line.startsWith('data:')) continue;\n    const payload = line.slice(5).trim();\n    if (!payload || payload === '[DONE]') continue;\n    try {\n      events.push(JSON.parse(payload));\n    } catch (err) {\n      continue;\n    }\n  }\n  return events;\n};\n\nconst parseSse = (text) => {\n  const events = parseSseEvents(text);\n  return events.length ? events[events.length - 1] : null;\n};\n\nconst parseBody = (body) => {\n  if (!body) return null;\n  if (typeof body === 'string') {\n    const trimmed = body.trim();\n    if (trimmed.startsWith('{')) {\n      try {\n        return JSON.parse(trimmed);\n      } catch (err) {\n        return null;\n      }\n    }\n    return parseSse(trimmed);\n  }\n  return body;\n};\n\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n\nconst isTransientRubeError = (message) => {\n  if (!message) return false;\n  return /tools failed|rate limit|temporar|timeout|429|503/i.test(message);\n};\n\nconst normalizeRubeBody = (body) => {\n  if (!body || typeof body !== 'object') return null;\n  return body.result || body;\n};\n\nconst unwrapData = (obj) => {\n  let current = obj;\n  for (let i = 0; i < 2; i += 1) {\n    if (current && typeof current === 'object' && current.data) {\n      current = current.data;\n    }\n  }\n  return current;\n};\n\nconst extractRubePayloads = (body) => {\n  const root = normalizeRubeBody(body);\n  const candidates = [];\n  if (!root) return candidates;\n  if (root.data) candidates.push(root.data);\n  if (Array.isArray(root.content)) {\n    root.content.forEach((entry) => {\n      if (entry?.json) candidates.push(entry.json);\n      if (entry?.data) candidates.push(entry.data);\n      if (entry?.text) {\n        try {\n          candidates.push(JSON.parse(entry.text));\n        } catch (err) {\n          candidates.push(entry.text);\n        }\n      }\n    });\n  }\n  candidates.push(root);\n  return candidates;\n};\n\nconst extractRubeError = (body) => {\n  if (!body) return null;\n  if (typeof body === 'string') {\n    return /tools failed|error|failed/i.test(body) ? body : null;\n  }\n  if (body.error?.message) return body.error.message;\n  if (typeof body.error === 'string') return body.error;\n\n  const root = normalizeRubeBody(body) || body;\n  if (root?.error?.message) return root.error.message;\n  if (typeof root?.error === 'string') return root.error;\n\n  const candidates = extractRubePayloads(body);\n  for (const candidate of candidates) {\n    if (!candidate) continue;\n    if (typeof candidate === 'string') {\n      if (/tools failed|error|failed/i.test(candidate)) return candidate;\n      continue;\n    }\n    if (candidate.error?.message) return candidate.error.message;\n    if (typeof candidate.error === 'string') return candidate.error;\n    if (candidate.message && /error|failed/i.test(String(candidate.message))) return candidate.message;\n  }\n  return null;\n};\n\nconst extractRubeDetails = (body) => {\n  const candidates = extractRubePayloads(body);\n  for (const candidate of candidates) {\n    if (!candidate || typeof candidate !== 'object') continue;\n    const details = candidate.details || candidate.detail;\n    if (details?.requestId) return { requestId: details.requestId };\n    if (candidate.requestId) return { requestId: candidate.requestId };\n    if (candidate.log_id) return { logId: candidate.log_id };\n  }\n  return {};\n};\n\nconst assertRubeSuccess = (body, label) => {\n  const err = extractRubeError(body);\n  if (err) {\n    const details = extractRubeDetails(body);\n    const suffixParts = [];\n    if (details.requestId) suffixParts.push(`requestId=${details.requestId}`);\n    if (details.logId) suffixParts.push(`logId=${details.logId}`);\n    const suffix = suffixParts.length ? ` (${suffixParts.join(', ')})` : '';\n    throw new Error(`${label} failed: ${err}${suffix}`);\n  }\n};\n\nconst extractSearchResult = (body) => {\n  for (const candidate of extractRubePayloads(body)) {\n    const data = unwrapData(candidate);\n    if (data?.results || data?.session_id || data?.session) return data;\n  }\n  return null;\n};\n\nconst extractConnectionResult = (body) => {\n  for (const candidate of extractRubePayloads(body)) {\n    const data = unwrapData(candidate);\n    if (data?.connections || data?.active_connection !== undefined) return data;\n    if (data?.results || data?.toolkit_connection_statuses) return data;\n  }\n  return null;\n};\n\nconst resolveSessionId = (searchData) => {\n  const data = unwrapData(searchData);\n  if (!data) return null;\n  if (data.session?.id) return data.session.id;\n  if (data.session_id) return data.session_id;\n  const first = Array.isArray(data.results) ? data.results[0] : null;\n  if (first?.session_id) return first.session_id;\n  if (first?.session?.id) return first.session.id;\n  return null;\n};\n\nconst resolveToolContext = (searchData) => {\n  const data = unwrapData(searchData);\n  const results = Array.isArray(data?.results) ? data.results : [];\n  const primary = results[0] || {};\n  const mainTools = Array.isArray(primary?.main_tools)\n    ? primary.main_tools\n    : (Array.isArray(primary?.tools) ? primary.tools : []);\n  let toolSlug = primary?.primary_tool_slugs?.[0] || mainTools[0]?.tool_slug || mainTools[0]?.name || null;\n  if (!toolSlug) {\n    const schemaKeys = Object.keys(data?.tool_schemas || {});\n    if (schemaKeys.length > 0) {\n      toolSlug = schemaKeys.includes('TWITTER_RECENT_SEARCH') ? 'TWITTER_RECENT_SEARCH' : schemaKeys[0];\n    }\n  }\n  const toolkits = [];\n  if (Array.isArray(primary?.toolkits)) toolkits.push(...primary.toolkits);\n  if (Array.isArray(data?.toolkits)) toolkits.push(...data.toolkits);\n  if (Array.isArray(data?.toolkit_connection_statuses)) {\n    data.toolkit_connection_statuses.forEach((entry) => {\n      if (entry?.toolkit) toolkits.push(entry.toolkit);\n    });\n  }\n  if (mainTools[0]?.toolkit_name) toolkits.push(mainTools[0].toolkit_name);\n  if (mainTools[0]?.toolkit) toolkits.push(mainTools[0].toolkit);\n  let activeConnection = primary?.active_connection;\n  if (activeConnection === undefined && Array.isArray(data?.toolkit_connection_statuses)) {\n    if (data.toolkit_connection_statuses.length > 0) {\n      activeConnection = data.toolkit_connection_statuses.every((entry) => entry?.has_active_connection !== false);\n    }\n  }\n  return {\n    toolSlug,\n    toolkits: [...new Set(toolkits.filter(Boolean))],\n    activeConnection\n  };\n};\n\nconst findTweetsEnvelope = (obj) => {\n  if (!obj || typeof obj !== 'object') return null;\n  if (Array.isArray(obj.data)) return obj;\n  if (obj.data && Array.isArray(obj.data.data)) return obj.data;\n  return null;\n};\n\nconst findMultiExecuteEnvelope = (obj) => {\n  const results = obj?.data?.data?.results;\n  if (!Array.isArray(results)) return null;\n  for (const result of results) {\n    const envelope = findTweetsEnvelope(result?.response?.data);\n    if (envelope) return envelope;\n  }\n  return null;\n};\n\nconst extractTwitterPayload = (response) => {\n  const root = response?.result || response;\n  const candidates = [];\n  if (root?.data) candidates.push(root.data);\n  if (Array.isArray(root?.content)) {\n    root.content.forEach((entry) => {\n      if (entry?.json) candidates.push(entry.json);\n      if (entry?.data) candidates.push(entry.data);\n      if (entry?.text) {\n        try {\n          candidates.push(JSON.parse(entry.text));\n        } catch (err) {\n          // ignore parse errors\n        }\n      }\n    });\n  }\n  candidates.push(root);\n  for (const candidate of candidates) {\n    const envelope = findTweetsEnvelope(candidate);\n    if (envelope) return envelope;\n    const multiEnvelope = findMultiExecuteEnvelope(candidate);\n    if (multiEnvelope) return multiEnvelope;\n  }\n  return null;\n};\n\nconst mcpPost = async (payload, includeProtocolHeader = true) => {\n  const headers = {\n    Authorization: `Bearer ${rubeToken}`,\n    'Content-Type': 'application/json',\n    Accept: 'application/json, text/event-stream'\n  };\n  if (includeProtocolHeader && mcpProtocolVersion) headers['MCP-Protocol-Version'] = mcpProtocolVersion;\n  if (includeProtocolHeader && mcpSessionId) headers['Mcp-Session-Id'] = mcpSessionId;\n\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: rubeUrl,\n    headers,\n    body: payload,\n    returnFullResponse: true,\n    responseFormat: 'string'\n  });\n\n  const rawBody = response?.body ?? response;\n  const parsedBody = parseBody(rawBody);\n  return {\n    body: parsedBody || rawBody,\n    headers: response?.headers\n  };\n};\n\nconst initializeMcp = async () => {\n  const initPayload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'initialize',\n    params: {\n      protocolVersion: mcpProtocolVersion,\n      capabilities: {},\n      clientInfo: { name: 'n8n', version: '1.0.0' }\n    }\n  };\n\n  const initResponse = await mcpPost(initPayload, false);\n  const initResult = initResponse.body?.result;\n  if (initResult?.protocolVersion) {\n    mcpProtocolVersion = initResult.protocolVersion;\n  }\n\n  const sessionHeader = getHeader(initResponse.headers, 'mcp-session-id');\n  if (sessionHeader) {\n    mcpSessionId = Array.isArray(sessionHeader) ? sessionHeader[0] : sessionHeader;\n  }\n\n  await mcpPost({ jsonrpc: '2.0', method: 'notifications/initialized' }, true);\n};\n\nconst searchTools = async (query) => {\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_SEARCH_TOOLS',\n      arguments: {\n        queries: [\n          {\n            use_case: 'search recent tweets on twitter',\n            known_fields: `query: ${query}`\n          }\n        ],\n        session: { generate_id: true }\n      }\n    }\n  };\n\n  const response = await mcpPost(payload, true);\n  assertRubeSuccess(response.body, 'Rube search tools');\n  const searchData = extractSearchResult(response.body);\n  if (!searchData) throw new Error('Rube search tools returned empty payload');\n  return searchData;\n};\n\nconst ensureActiveConnections = async (toolkits, sessionId) => {\n  if (!Array.isArray(toolkits) || toolkits.length === 0) return;\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_MANAGE_CONNECTIONS',\n      arguments: {\n        toolkits,\n        session_id: sessionId\n      }\n    }\n  };\n\n  const response = await mcpPost(payload, true);\n  assertRubeSuccess(response.body, 'Rube manage connections');\n  const connData = extractConnectionResult(response.body);\n  if (!connData) return;\n\n  const toolkitStatuses = connData.toolkit_connection_statuses;\n  if (Array.isArray(toolkitStatuses) && toolkitStatuses.length > 0) {\n    const inactive = toolkitStatuses.find((entry) => entry?.has_active_connection === false);\n    if (inactive) {\n      const detail = inactive.status_message || inactive.description || 'inactive';\n      throw new Error(`Rube connection not ACTIVE: ${detail}`);\n    }\n  }\n\n  const results = connData.results;\n  if (results && typeof results === 'object' && !Array.isArray(results)) {\n    const entries = Object.values(results);\n    const inactive = entries.find((entry) => {\n      if (!entry || typeof entry !== 'object') return false;\n      if (entry.has_active_connection === false) return true;\n      const status = entry.connection_status || entry.status;\n      return status ? String(status).toUpperCase() !== 'ACTIVE' : false;\n    });\n    if (inactive) {\n      const status = inactive.connection_status || inactive.status || 'UNKNOWN';\n      const detail = inactive.instruction || inactive.status_message || inactive.description;\n      const suffix = detail ? ` - ${detail}` : '';\n      throw new Error(`Rube connection not ACTIVE: ${status}${suffix}`);\n    }\n  }\n\n  const connections = connData.connections || [];\n  if (Array.isArray(connections) && connections.length > 0) {\n    const inactive = connections.find((conn) => {\n      const status = conn.connection_status || conn.status;\n      return status && status !== 'ACTIVE';\n    });\n    if (inactive) {\n      const status = inactive.connection_status || inactive.status || 'UNKNOWN';\n      const redirect = inactive.redirect_url ? ` (open: ${inactive.redirect_url})` : '';\n      throw new Error(`Rube connection not ACTIVE: ${status}${redirect}`);\n    }\n  } else if (connData.active_connection === false) {\n    throw new Error('Rube connection not ACTIVE');\n  }\n};\n\nconst executeWithRetry = async (payload, label, maxAttempts = 3) => {\n  let lastError;\n  for (let attempt = 1; attempt <= maxAttempts; attempt += 1) {\n    try {\n      const response = await mcpPost(payload, true);\n      assertRubeSuccess(response.body, label);\n      return response;\n    } catch (error) {\n      lastError = error;\n      const message = error?.message || String(error);\n      if (attempt < maxAttempts && isTransientRubeError(message)) {\n        await sleep(600 * attempt);\n        continue;\n      }\n      throw error;\n    }\n  }\n  throw lastError;\n};\n\ntry {\n  await initializeMcp();\n\n  const searchData = await searchTools(accountQuery);\n  const sessionId = resolveSessionId(searchData);\n  if (!sessionId) throw new Error('Rube search tools missing session_id');\n\n  const toolContext = resolveToolContext(searchData);\n  if (!toolContext.toolSlug) throw new Error('Rube search tools missing tool slug');\n\n  const resolvedToolkits = toolContext.toolkits.length\n    ? toolContext.toolkits\n    : (toolContext.toolSlug?.startsWith('TWITTER_') ? ['twitter'] : []);\n  await ensureActiveConnections(resolvedToolkits, sessionId);\n\n  const payload = {\n    jsonrpc: '2.0',\n    id: requestId++,\n    method: 'tools/call',\n    params: {\n      name: 'RUBE_MULTI_EXECUTE_TOOL',\n      arguments: {\n        tools: [\n          {\n            tool_slug: toolContext.toolSlug,\n            arguments: {\n              query: accountQuery,\n              max_results: 30,\n              tweet_fields: ['created_at', 'public_metrics', 'author_id'],\n              expansions: ['author_id'],\n              user_fields: ['username', 'name']\n            }\n          }\n        ],\n        sync_response_to_workbench: false,\n        memory: {},\n        session_id: sessionId,\n        current_step: 'FETCH_ACCOUNT_TWEETS',\n        current_step_metric: '1/1'\n      }\n    }\n  };\n\n  const response = await executeWithRetry(payload, 'Rube X account search');\n  const twitterPayload = extractTwitterPayload(response.body);\n  if (!twitterPayload) {\n    console.log('Rube X account search returned empty payload');\n  }\n  const tweets = twitterPayload?.data || [];\n  const users = twitterPayload?.includes?.users || [];\n\n  const userMap = {};\n  users.forEach((user) => {\n    userMap[user.id] = user;\n  });\n\n  tweets.forEach((tweet) => {\n    if (seenTweetIds.has(tweet.id)) return;\n    seenTweetIds.add(tweet.id);\n    const author = userMap[tweet.author_id] || {};\n    const username = author.username || 'unknown';\n    allTweets.push({\n      title: tweet.text.substring(0, 100) + (tweet.text.length > 100 ? '...' : ''),\n      url: `https://twitter.com/${username}/status/${tweet.id}`,\n      source: `X - @${username}`,\n      snippet: tweet.text,\n      publishedAt: tweet.created_at,\n      author: username,\n      metrics: tweet.public_metrics || {},\n      sourceType: 'X',\n      tier: 'B'\n    });\n  });\n\n  return allTweets.map(tweet => ({ json: tweet }));\n} catch (error) {\n  throw new Error(`X account search failed: ${error.message}`);\n}\n"
      },
      "name": "X Account Search",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        440,
        450
      ],
      "id": "x-account"
    },
    {
      "parameters": {
        "mode": "append"
      },
      "name": "Merge X",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [
        700,
        400
      ],
      "id": "merge-x"
    },
    {
      "parameters": {
        "mode": "append"
      },
      "name": "Merge All",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [
        940,
        300
      ],
      "id": "merge"
    },
    {
      "parameters": {
        "jsCode": "// Normalize all data\nconst items = $input.all();\nconst normalized = [];\n\nfor (const item of items) {\n  const data = item.json;\n  const source = data.source || 'RSS';\n  const sourceType = source.startsWith('X -') ? 'X' : 'RSS';\n  normalized.push({\n    title: data.title || data.text || '',\n    url: data.link || data.url || '',\n    source,\n    sourceType,\n    tier: data.tier || 'B',\n    snippet: (data.description || data.summary || data.text || data.snippet || '').substring(0, 300),\n    publishedAt: data.pubDate || data.isoDate || data.created_at || data.publishedAt || new Date().toISOString(),\n    author: data.author || data.username || '',\n    metrics: data.metrics || data.public_metrics || {}\n  });\n}\n\nreturn normalized.map(item => ({ json: item }));"
      },
      "name": "Normalize",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1140,
        300
      ],
      "id": "normalize"
    },
    {
      "parameters": {
        "jsCode": "// Cross-Day Dedupe Node\n// Attempts to use workflow staticData; if不可用则退化为“当次运行”去重（无跨日持久化）\n\nconst items = $input.all();\nconst EXPIRY_DAYS = Number.parseInt($env.DEDUPE_EXPIRY_DAYS || '7', 10);\nconst MAX_URLS = Number.parseInt($env.DEDUPE_MAX_URLS || '2000', 10);\n\n// Helper: load persistent store (prefer staticData)\nlet storage = { seenUrls: {} };\nlet storageMode = 'staticData';\n\ntry {\n  const staticData = this.getWorkflowStaticData\n    ? this.getWorkflowStaticData('global')\n    : this.helpers?.getWorkflowStaticData?.call(this, 'global');\n  if (!staticData) {\n    storageMode = 'volatile';\n  } else {\n    if (!staticData.seenUrls) staticData.seenUrls = {};\n    storage = staticData;\n  }\n} catch (err) {\n  storageMode = 'volatile';\n  storage = { seenUrls: {} };\n}\n\nconst now = Date.now();\nconst expiryMs = EXPIRY_DAYS * 24 * 60 * 60 * 1000;\n\n// Clean up expired entries\nconst urlKeys = Object.keys(storage.seenUrls);\nlet expiredCount = 0;\nfor (const url of urlKeys) {\n  const timestamp = storage.seenUrls[url];\n  if (now - timestamp > expiryMs) {\n    delete storage.seenUrls[url];\n    expiredCount++;\n  }\n}\n\n// Dedupe current batch\nconst unique = [];\nconst duplicateUrls = [];\nconst newUrls = [];\n\nfor (const item of items) {\n  const url = item.json.url;\n  if (!url) continue;\n\n  if (storage.seenUrls[url]) {\n    duplicateUrls.push(url);\n    continue;\n  }\n\n  storage.seenUrls[url] = now;\n  newUrls.push(url);\n  unique.push(item);\n}\n\n// Enforce max URL limit (remove oldest if over limit)\nconst allUrls = Object.entries(storage.seenUrls);\nif (allUrls.length > MAX_URLS) {\n  allUrls.sort((a, b) => a[1] - b[1]); // oldest first\n  const toRemove = allUrls.slice(0, allUrls.length - MAX_URLS);\n  for (const [url] of toRemove) {\n    delete storage.seenUrls[url];\n  }\n}\n\nconst stats = {\n  input_count: items.length,\n  unique_count: unique.length,\n  duplicate_count: duplicateUrls.length,\n  expired_cleaned: expiredCount,\n  total_stored_urls: Object.keys(storage.seenUrls).length,\n  expiry_days: EXPIRY_DAYS,\n  storage_mode: storageMode\n};\nconsole.log('Cross-Day Dedupe Stats:', JSON.stringify(stats));\n\nreturn unique;\n"
      },
      "name": "Cross-Day Dedupe",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1340,
        300
      ],
      "id": "dedupe"
    },
    {
      "parameters": {
        "jsCode": "// Semantic Dedupe Node\n// 使用 OpenAI Embedding 进行语义级去重，识别\"不同URL但话题相同\"的内容\n//\n// 工作流程：\n// 1. 为每条内容生成 Embedding 向量\n// 2. 与历史 Embedding 计算余弦相似度\n// 3. 相似度 > 阈值 → 判定为语义重复 → 过滤\n// 4. 通过的内容更新到历史存储\n\nconst items = $input.all();\nconst apiKey = $env.OPENAI_API_KEY;\n\n// 配置参数\nconst SIMILARITY_THRESHOLD = Number.parseFloat($env.SEMANTIC_DEDUPE_THRESHOLD || '0.85');\nconst EXPIRY_DAYS = Number.parseInt($env.SEMANTIC_DEDUPE_EXPIRY_DAYS || '7', 10);\nconst MAX_EMBEDDINGS = Number.parseInt($env.SEMANTIC_DEDUPE_MAX_EMBEDDINGS || '500', 10);\nconst EMBEDDING_MODEL = $env.SEMANTIC_DEDUPE_MODEL || 'text-embedding-3-small';\nconst BATCH_SIZE = Number.parseInt($env.SEMANTIC_DEDUPE_BATCH_SIZE || '20', 10);\nconst DEBUG = $env.SEMANTIC_DEDUPE_DEBUG === 'true';\n\nif (!apiKey) {\n  console.log('Warning: Missing OPENAI_API_KEY, skipping semantic dedupe');\n  return items;\n}\n\nif (items.length === 0) {\n  return [];\n}\n\n// ============== 工具函数 ==============\n\n// 余弦相似度计算\nconst cosineSimilarity = (a, b) => {\n  if (!a || !b || a.length !== b.length) return 0;\n  let dot = 0, normA = 0, normB = 0;\n  for (let i = 0; i < a.length; i++) {\n    dot += a[i] * b[i];\n    normA += a[i] * a[i];\n    normB += b[i] * b[i];\n  }\n  if (normA === 0 || normB === 0) return 0;\n  return dot / (Math.sqrt(normA) * Math.sqrt(normB));\n};\n\n// 生成内容的文本表示（用于Embedding）\nconst getContentText = (item) => {\n  const data = item.json || item;\n  const title = String(data.title || data.text || '').trim();\n  const snippet = String(data.snippet || data.description || data.summary || '').trim();\n  // 组合 title + snippet，限制长度（Embedding模型有token限制）\n  const combined = `${title}\\n${snippet}`.slice(0, 500);\n  return combined;\n};\n\n// 生成内容的唯一标识（用于存储）\nconst getContentId = (item) => {\n  const data = item.json || item;\n  const url = data.url || '';\n  // 使用URL的hash作为ID\n  let hash = 0;\n  for (let i = 0; i < url.length; i++) {\n    const char = url.charCodeAt(i);\n    hash = ((hash << 5) - hash) + char;\n    hash = hash & hash;\n  }\n  return `emb_${Math.abs(hash).toString(36)}`;\n};\n\n// 批量调用 OpenAI Embedding API\nconst getEmbeddings = async (texts) => {\n  if (texts.length === 0) return [];\n\n  try {\n    const response = await this.helpers.httpRequest({\n      method: 'POST',\n      url: 'https://api.openai.com/v1/embeddings',\n      headers: {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n      },\n      body: {\n        model: EMBEDDING_MODEL,\n        input: texts\n      }\n    });\n\n    if (response?.data && Array.isArray(response.data)) {\n      // 按 index 排序确保顺序正确\n      const sorted = response.data.sort((a, b) => a.index - b.index);\n      return sorted.map(d => d.embedding);\n    }\n    return [];\n  } catch (error) {\n    console.log('Embedding API error:', error.message || error);\n    return [];\n  }\n};\n\n// 分批处理\nconst chunk = (arr, size) => {\n  const chunks = [];\n  for (let i = 0; i < arr.length; i += size) {\n    chunks.push(arr.slice(i, i + size));\n  }\n  return chunks;\n};\n\n// ============== 存储管理 ==============\n\nlet storage = { embeddings: [] };\nlet storageMode = 'staticData';\n\ntry {\n  const staticData = this.getWorkflowStaticData\n    ? this.getWorkflowStaticData('global')\n    : this.helpers?.getWorkflowStaticData?.call(this, 'global');\n  if (!staticData) {\n    storageMode = 'volatile';\n  } else {\n    if (!staticData.semanticEmbeddings) staticData.semanticEmbeddings = [];\n    storage = { embeddings: staticData.semanticEmbeddings };\n    // 保持引用以便后续更新\n    storage._staticData = staticData;\n  }\n} catch (err) {\n  storageMode = 'volatile';\n  storage = { embeddings: [] };\n}\n\nconst now = Date.now();\nconst expiryMs = EXPIRY_DAYS * 24 * 60 * 60 * 1000;\n\n// 清理过期的 Embedding\nconst originalCount = storage.embeddings.length;\nstorage.embeddings = storage.embeddings.filter(e => (now - e.timestamp) < expiryMs);\nconst expiredCount = originalCount - storage.embeddings.length;\n\n// ============== 主处理逻辑 ==============\n\n// 准备内容文本\nconst itemsWithText = items.map((item, index) => ({\n  item,\n  index,\n  text: getContentText(item),\n  id: getContentId(item)\n})).filter(entry => entry.text.length > 10); // 过滤空内容\n\nif (itemsWithText.length === 0) {\n  console.log('Semantic Dedupe: No valid content to process');\n  return items;\n}\n\n// 批量获取 Embedding\nconst allTexts = itemsWithText.map(e => e.text);\nconst allEmbeddings = [];\n\nfor (const batch of chunk(allTexts, BATCH_SIZE)) {\n  const batchEmbeddings = await getEmbeddings(batch);\n  allEmbeddings.push(...batchEmbeddings);\n  // 添加小延迟避免API限流\n  if (batch.length === BATCH_SIZE) {\n    await new Promise(resolve => setTimeout(resolve, 100));\n  }\n}\n\n// 如果 Embedding 获取失败，返回原始内容\nif (allEmbeddings.length !== itemsWithText.length) {\n  console.log(`Semantic Dedupe: Embedding count mismatch (${allEmbeddings.length} vs ${itemsWithText.length}), skipping`);\n  return items;\n}\n\n// 为每个条目添加 Embedding\nitemsWithText.forEach((entry, i) => {\n  entry.embedding = allEmbeddings[i];\n});\n\n// 与历史 Embedding 比较，找出重复\nconst unique = [];\nconst duplicates = [];\nconst newEmbeddings = [];\n\nfor (const entry of itemsWithText) {\n  if (!entry.embedding || entry.embedding.length === 0) {\n    unique.push(entry.item);\n    continue;\n  }\n\n  let isDuplicate = false;\n  let maxSimilarity = 0;\n  let mostSimilarTitle = '';\n\n  // 与历史 Embedding 比较\n  for (const historical of storage.embeddings) {\n    const similarity = cosineSimilarity(entry.embedding, historical.embedding);\n    if (similarity > maxSimilarity) {\n      maxSimilarity = similarity;\n      mostSimilarTitle = historical.title || '';\n    }\n    if (similarity >= SIMILARITY_THRESHOLD) {\n      isDuplicate = true;\n      break;\n    }\n  }\n\n  // 与本批次已通过的内容比较（防止批内重复）\n  if (!isDuplicate) {\n    for (const newEmb of newEmbeddings) {\n      const similarity = cosineSimilarity(entry.embedding, newEmb.embedding);\n      if (similarity > maxSimilarity) {\n        maxSimilarity = similarity;\n        mostSimilarTitle = newEmb.title || '';\n      }\n      if (similarity >= SIMILARITY_THRESHOLD) {\n        isDuplicate = true;\n        break;\n      }\n    }\n  }\n\n  if (isDuplicate) {\n    duplicates.push({\n      title: entry.item.json?.title || entry.text.slice(0, 50),\n      similarity: maxSimilarity,\n      similarTo: mostSimilarTitle\n    });\n  } else {\n    unique.push(entry.item);\n    // 添加到新 Embedding 列表\n    newEmbeddings.push({\n      id: entry.id,\n      embedding: entry.embedding,\n      title: String(entry.item.json?.title || entry.text.slice(0, 80)),\n      timestamp: now\n    });\n  }\n}\n\n// 更新存储\nstorage.embeddings.push(...newEmbeddings);\n\n// 如果超过最大存储量，删除最旧的\nif (storage.embeddings.length > MAX_EMBEDDINGS) {\n  storage.embeddings.sort((a, b) => b.timestamp - a.timestamp);\n  storage.embeddings = storage.embeddings.slice(0, MAX_EMBEDDINGS);\n}\n\n// 同步到 staticData\nif (storage._staticData) {\n  storage._staticData.semanticEmbeddings = storage.embeddings;\n}\n\n// ============== 输出统计 ==============\n\nconst stats = {\n  input_count: items.length,\n  processed_count: itemsWithText.length,\n  unique_count: unique.length,\n  duplicate_count: duplicates.length,\n  similarity_threshold: SIMILARITY_THRESHOLD,\n  expired_cleaned: expiredCount,\n  total_stored_embeddings: storage.embeddings.length,\n  storage_mode: storageMode,\n  embedding_model: EMBEDDING_MODEL\n};\n\nconsole.log('Semantic Dedupe Stats:', JSON.stringify(stats));\n\nif (DEBUG && duplicates.length > 0) {\n  console.log('Semantic Duplicates Found:');\n  duplicates.forEach(d => {\n    console.log(`  - \"${d.title.slice(0, 40)}...\" (similarity: ${d.similarity.toFixed(3)}) similar to \"${d.similarTo.slice(0, 40)}...\"`);\n  });\n}\n\n// 返回去重后的内容\nreturn unique;\n"
      },
      "name": "Semantic Dedupe",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1540,
        300
      ],
      "id": "semantic-dedupe"
    },
    {
      "parameters": {
        "jsCode": "// LLM Ranking Node Code for n8n\n// Batch-ranks content using OpenAI API to avoid timeouts\n\nconst items = $input.all();\nconst apiKey = $env.OPENAI_API_KEY;\nconst model = $env.OPENAI_MODEL || 'gpt-4o-mini';\nconst maxItems = Number.parseInt($env.LLM_RANK_MAX_ITEMS || '40', 10);\nconst batchSize = Number.parseInt($env.LLM_RANK_BATCH_SIZE || '8', 10);\nconst xRatioRaw = Number.parseFloat($env.LLM_RANK_X_RATIO || '0.4');\nconst xRatio = Number.isFinite(xRatioRaw) ? Math.min(1, Math.max(0, xRatioRaw)) : 0.4;\nconst xAppliedCap = Number.parseInt($env.LLM_RANK_X_PER_SOURCE_CAP_APPLIED || '4', 10);\nconst xResearchCap = Number.parseInt($env.LLM_RANK_X_PER_SOURCE_CAP_RESEARCH || '1', 10);\nconst xDefaultCap = Number.parseInt($env.LLM_RANK_X_PER_SOURCE_CAP_DEFAULT || '2', 10);\n\n// Low-score filtering threshold (default: 18/30 = 60%)\nconst minScoreThreshold = Number.parseInt($env.LLM_RANK_MIN_SCORE || '18', 10);\n// Per-source cap to prevent one source dominating the output\nconst perSourceCap = Number.parseInt($env.LLM_RANK_PER_SOURCE_CAP || '3', 10);\n\nif (!apiKey) {\n  throw new Error('Missing OPENAI_API_KEY for LLM Rank.');\n}\n\nconst normalizeText = (value, maxLen) => {\n  const text = String(value || '').replace(/\\s+/g, ' ').trim();\n  if (!maxLen) return text;\n  return text.length > maxLen ? text.slice(0, maxLen) : text;\n};\n\nconst metricScore = (metrics = {}) => {\n  const like = Number(metrics.like_count || 0);\n  const rt = Number(metrics.retweet_count || 0);\n  const quote = Number(metrics.quote_count || 0);\n  const reply = Number(metrics.reply_count || 0);\n  const impression = Number(metrics.impression_count || 0);\n  return like + rt * 2 + quote * 1.5 + reply * 0.5 + impression * 0.05;\n};\n\nconst parseTime = (value) => {\n  if (!value) return 0;\n  const ts = Date.parse(value);\n  return Number.isFinite(ts) ? ts : 0;\n};\n\nconst prepped = items.map((item, index) => {\n  const data = item.json || {};\n  const metrics = data.metrics || data.public_metrics || {};\n  const source = data.source || '';\n  const sourceType = data.sourceType || (source.startsWith('X -') ? 'X' : 'RSS');\n  return {\n    index,\n    data,\n    metricScore: metricScore(metrics),\n    sourceType,\n    publishedAtScore: parseTime(data.publishedAt || data.created_at || data.pubDate || data.isoDate)\n  };\n});\n\nconst maxItemsSafe = Math.max(1, Math.min(maxItems, prepped.length));\n\nconst xItems = prepped.filter((entry) => entry.sourceType === 'X');\nconst rssItems = prepped.filter((entry) => entry.sourceType !== 'X');\n\nxItems.sort((a, b) => b.metricScore - a.metricScore);\n\n// RSS按Tier优先排序，同Tier按时间排序（确保Tier A官方源优先进入评分）\nconst tierPriority = { 'A': 4, 'B': 3, 'C': 2, 'D': 1 };\nrssItems.sort((a, b) => {\n  const tierA = tierPriority[a.data.tier] || 2;\n  const tierB = tierPriority[b.data.tier] || 2;\n  if (tierA !== tierB) return tierB - tierA;\n  return b.publishedAtScore - a.publishedAtScore;\n});\n\nlet xQuota = Math.min(xItems.length, Math.round(maxItemsSafe * xRatio));\nlet rssQuota = Math.min(rssItems.length, maxItemsSafe - xQuota);\nif (rssQuota < maxItemsSafe - xQuota) {\n  xQuota = Math.min(xItems.length, maxItemsSafe - rssQuota);\n}\n\n// High-value applied/practical sources - give them higher quota\nconst appliedSources = new Set([\n  'X - ai-agents',\n  'X - ai-workflow',\n  'X - llm-prompts',\n  'X - ai-built',        // NEW: developers shipping things\n  'X - ai-freebies',     // NEW: free resources\n  'X - buildinpublic',   // NEW: indie hackers\n  'X - ai-tips',         // NEW: practical tips\n  'X - @simonw',\n  'X - @swyx',\n  'X - @hwchase17',\n  'X - @goodside'\n]);\n// Lower priority research/theory sources\nconst researchSources = new Set([\n  'X - @ylecun',\n  'X - @sama'  // often macro commentary\n]);\n\nconst xBuckets = new Map();\nxItems.forEach((entry) => {\n  const key = entry.data.source || 'X';\n  if (!xBuckets.has(key)) xBuckets.set(key, []);\n  xBuckets.get(key).push(entry);\n});\n\nconst xSelected = [];\nconst pickedIndexes = new Set();\n\nfor (const [source, bucket] of xBuckets.entries()) {\n  bucket.sort((a, b) => b.metricScore - a.metricScore);\n  let cap = xDefaultCap;\n  if (appliedSources.has(source)) cap = xAppliedCap;\n  if (researchSources.has(source)) cap = xResearchCap;\n  for (const entry of bucket.slice(0, cap)) {\n    if (xSelected.length >= xQuota) break;\n    if (pickedIndexes.has(entry.index)) continue;\n    pickedIndexes.add(entry.index);\n    xSelected.push(entry);\n  }\n  if (xSelected.length >= xQuota) break;\n}\n\nif (xSelected.length < xQuota) {\n  for (const entry of xItems) {\n    if (xSelected.length >= xQuota) break;\n    if (pickedIndexes.has(entry.index)) continue;\n    pickedIndexes.add(entry.index);\n    xSelected.push(entry);\n  }\n}\n\nconst selected = xSelected.concat(rssItems.slice(0, rssQuota));\n\nconst chunk = (arr, size) => {\n  const chunks = [];\n  for (let i = 0; i < arr.length; i += size) {\n    chunks.push(arr.slice(i, i + size));\n  }\n  return chunks;\n};\n\nconst parseJson = (content) => {\n  if (!content) return null;\n  const trimmed = String(content).trim();\n  try {\n    return JSON.parse(trimmed);\n  } catch (error) {\n    const objStart = trimmed.indexOf('{');\n    const objEnd = trimmed.lastIndexOf('}');\n    if (objStart >= 0 && objEnd > objStart) {\n      try {\n        return JSON.parse(trimmed.slice(objStart, objEnd + 1));\n      } catch (err) {\n        // ignore and try array\n      }\n    }\n    const arrStart = trimmed.indexOf('[');\n    const arrEnd = trimmed.lastIndexOf(']');\n    if (arrStart >= 0 && arrEnd > arrStart) {\n      try {\n        return JSON.parse(trimmed.slice(arrStart, arrEnd + 1));\n      } catch (err) {\n        return null;\n      }\n    }\n  }\n  return null;\n};\n\nconst rankedItems = [];\nlet failedBatches = 0;\n\nfor (const group of chunk(selected, Math.max(1, batchSize))) {\n  const payloadItems = group.map((entry) => ({\n    id: entry.index,\n    title: normalizeText(entry.data.title || entry.data.text || '', 120),\n    snippet: normalizeText(entry.data.snippet || entry.data.description || entry.data.summary || '', 280),\n    source: normalizeText(entry.data.source || '', 80),\n    url: normalizeText(entry.data.url || '', 160),\n    source_type: entry.sourceType || 'RSS',\n    tier: entry.data.tier || 'B'\n  }));\n\n  const prompt = `你是AI行业情报分析师，帮我筛选对商业决策最有价值的内容。\n\n【评分维度】（总分30分）\n\n1. timeliness 时效性 (0-6分)\n   6分: 官方公告/产品发布（24小时内）\n   5分: 重要更新/突发新闻\n   4分: 本周热点/趋势分析\n   3分: 深度报告/案例研究\n   1-2分: 历史内容/旧闻\n\n2. impact 影响力 (0-9分)\n   9分: 行业变革级（新模型发布、重大政策、API定价变化）\n   7-8分: 重大产品更新（GPT/Claude/Gemini新功能、官方SDK更新）\n   5-6分: 官方工具发布/重要功能更新\n   3-4分: GitHub trending工具/小库/插件（除非是突破性项目）\n   1-2分: 讨论/观点/评论/普通开源项目\n\n3. actionability 可行动性 (0-7分)\n   7分: 官方API/SDK更新，可直接集成到产品\n   5-6分: 有完整教程/示例，需要适配但可落地\n   3-4分: 有参考价值，GitHub工具需评估是否成熟\n   1-2分: 纯理论/概念/早期实验项目\n\n4. relevance 相关性 (0-8分)\n   8分: 直接影响商业决策（定价、竞品、市场）\n   6-7分: 工作流/效率提升\n   4-5分: 产品架构/技术选型\n   2-3分: 一般AI新闻\n   1分: 边缘相关\n\n【源可信度加权】\n评分时请考虑信息源的可信度层级(tier字段):\n\nTier A (官方源): 评分时给予更高的impact和actionability\n  - 来自公司官方博客/SDK/协议\n  - 信息准确性最高\n  - 示例: OpenAI News, Google AI Blog, AWS ML Blog\n\nTier B (权威源): 正常评分\n  - 专家博客、权威媒体、学术源\n  - 示例: TechCrunch, MIT Tech Review, ArXiv\n\nTier C (社区源): 评分时更严格\n  - 社区讨论、GitHub trending\n  - 需要内容本身非常有价值才给高分\n  - 示例: Reddit, Hacker News, GitHub Trending\n\nTier D (聚合源): 评分时最严格\n  - 新闻聚合，可能二次转载\n  - 示例: Google News\n\n【高分内容示例】(24-30分)\n✅ \"OpenAI发布GPT-5\" - 官方公告+行业变革+高相关\n✅ \"Claude新增代码执行功能\" - 产品更新+可直接用\n✅ \"Anthropic API定价下调50%\" - 直接影响商业决策\n\n【中分内容示例】(15-23分)\n⚠️ \"GitHub trending: 新AI工具库\" - 工具发现，需评估成熟度\n⚠️ \"XX公司融资1亿\" - 行业动态，参考价值\n\n【低分内容示例】(0-14分)\n❌ \"AI将改变世界\" - 观点文章，无可行动性\n❌ \"论文：提出新算法\" - 纯研究，除非有成熟开源实现\n❌ \"GitHub: 又一个LLM wrapper\" - 普通开源项目，低影响力\n\n输入数据：\n${JSON.stringify(payloadItems, null, 2)}\n\n返回JSON（不要markdown）：\n{\n  \"items\": [\n    {\n      \"id\": 0,\n      \"timeliness\": 5,\n      \"impact\": 7,\n      \"actionability\": 6,\n      \"relevance\": 7,\n      \"total\": 25,\n      \"why\": \"一句话说明价值点\",\n      \"category\": \"announcement/insight/tool/case/research/risk\"\n    }\n  ]\n}`;\n\n  let batchScores = [];\n  try {\n    const response = await this.helpers.httpRequest({\n      method: 'POST',\n      url: 'https://api.openai.com/v1/chat/completions',\n      headers: {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n      },\n      body: {\n        model,\n        messages: [\n          { role: 'system', content: '你是一个专业的内容策展专家。' },\n          { role: 'user', content: prompt }\n        ],\n        temperature: 0.2,\n        response_format: { type: 'json_object' }\n      }\n    });\n\n    const parsed = parseJson(response?.choices?.[0]?.message?.content);\n    if (parsed?.items && Array.isArray(parsed.items)) {\n      batchScores = parsed.items;\n    } else if (Array.isArray(parsed)) {\n      batchScores = parsed;\n    }\n  } catch (error) {\n    failedBatches += 1;\n  }\n\n  const scoreMap = new Map();\n  batchScores.forEach((score) => {\n    if (!score || typeof score.id === 'undefined') return;\n    scoreMap.set(Number(score.id), score);\n  });\n\n  group.forEach((entry) => {\n    const score = scoreMap.get(entry.index);\n    if (score) {\n      // New 4-dimension scoring (Phase 2.2)\n      const timeliness = Number(score.timeliness || score.freshness || 0);\n      let impact = Number(score.impact || score.shareworthy || 0);\n      let actionability = Number(score.actionability || score.practical || 0);\n      const relevance = Number(score.relevance || 0);\n\n      // Phase 2.5: Tier-based score adjustment\n      const tier = entry.data.tier || 'B';\n      const TIER_BOOST = {\n        'A': { impact: 2, actionability: 1 },\n        'B': { impact: 0, actionability: 0 },\n        'C': { impact: -1, actionability: -1 },\n        'D': { impact: -2, actionability: -1 }\n      };\n      const boost = TIER_BOOST[tier] || TIER_BOOST['B'];\n      const adjustedImpact = Math.max(0, Math.min(9, impact + boost.impact));\n      const adjustedActionability = Math.max(0, Math.min(7, actionability + boost.actionability));\n\n      const total = timeliness + adjustedImpact + adjustedActionability + relevance;\n      rankedItems.push({\n        json: {\n          ...entry.data,\n          score: {\n            timeliness,\n            impact: adjustedImpact,\n            actionability: adjustedActionability,\n            relevance,\n            total,\n            why: score.why || '',\n            category: score.category || score.type || 'unknown',\n            tierBoost: boost.impact + boost.actionability\n          }\n        }\n      });\n    } else {\n      const fallbackTotal = Math.min(30, Math.round(Math.log1p(entry.metricScore) * 6));\n      rankedItems.push({\n        json: {\n          ...entry.data,\n          score: {\n            timeliness: 0,\n            impact: 0,\n            actionability: 0,\n            relevance: 0,\n            total: fallbackTotal,\n            why: 'LLM未返回结果，使用互动指标估算',\n            category: 'unknown'\n          }\n        }\n      });\n    }\n  });\n}\n\nif (failedBatches >= Math.ceil(selected.length / Math.max(1, batchSize))) {\n  throw new Error('LLM Rank failed for all batches. Check OpenAI API key/billing/connectivity.');\n}\n\nrankedItems.sort((a, b) => (b.json.score?.total || 0) - (a.json.score?.total || 0));\n\n// Filter out low-score items (below threshold)\nconst filteredByScore = rankedItems.filter((item) => {\n  const total = item.json.score?.total || 0;\n  return total >= minScoreThreshold;\n});\n\n// Apply per-source cap to prevent one source dominating\nconst sourceCounts = new Map();\nconst filteredBySourceCap = filteredByScore.filter((item) => {\n  const source = item.json.source || 'unknown';\n  const count = sourceCounts.get(source) || 0;\n  if (count >= perSourceCap) return false;\n  sourceCounts.set(source, count + 1);\n  return true;\n});\n\n// Log filtering stats for debugging\nconst statsLog = {\n  input_count: items.length,\n  selected_for_ranking: selected.length,\n  after_llm_ranking: rankedItems.length,\n  after_score_filter: filteredByScore.length,\n  after_source_cap: filteredBySourceCap.length,\n  min_score_threshold: minScoreThreshold,\n  per_source_cap: perSourceCap,\n  filtered_low_score: rankedItems.length - filteredByScore.length,\n  filtered_source_cap: filteredByScore.length - filteredBySourceCap.length\n};\nconsole.log('LLM Rank Stats:', JSON.stringify(statsLog));\n\nreturn filteredBySourceCap;\n"
      },
      "name": "LLM Rank",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1740,
        300
      ],
      "id": "llm-rank"
    },
    {
      "parameters": {
        "jsCode": "// Tweet Generation Node Code for n8n\n// Humanized tweet generation - sounds like a real AI enthusiast, not a news bot\n\nconst items = $input.all();\nconst apiKey = $env.OPENAI_API_KEY;\nconst model = $env.OPENAI_MODEL || 'gpt-4o-mini';\nconst allowTwitterLinks = String($env.TWEET_ALLOW_TWITTER_LINKS || '').toLowerCase() === 'true';\nconst blocklistRaw = $env.TWEET_TONE_BLOCKLIST || 'stupid,idiot,dumb,trash,垃圾,傻,蠢,愚蠢,脑残,仇恨';\nconst blocklist = blocklistRaw.split(',').map(w => w.trim().toLowerCase()).filter(Boolean);\n\n// Persona configuration for humanized tweets\nconst PERSONA = {\n  system: `你是一个每天泡在X/GitHub/HN上的AI工具发烧友。\n\n你的身份：\n- 独立开发者，日常用AI工具提效\n- 喜欢发现新工具、测试新功能\n- 偶尔吐槽、偶尔惊喜、偶尔给建议\n\n你的说话风格：\n- 像跟朋友聊天，不是写新闻稿\n- 会用\"发现个好东西\"、\"试了下\"、\"这个思路有意思\"\n- 偶尔用口语词：\"绝了\"、\"真香\"、\"有点东西\"\n- 会问读者问题，邀请互动\n- 分享时说\"我\"而不是\"本文\"\n\n绝对不要：\n- 像新闻播报员一样正式\n- 用\"值得关注\"、\"引发热议\"这种官方腔\n- 堆砌形容词\n- 写成产品广告`,\n\n  styles: [\n    { id: 'discovery', name: '发现分享', desc: '分享你发现的好东西' },\n    { id: 'insight', name: '个人洞察', desc: '你的看法和思考' },\n    { id: 'practical', name: '实用推荐', desc: '具体怎么用、适合谁' }\n  ]\n};\n\n// Collect pipeline statistics for observability\nconst pipelineStats = {\n  total_candidates: items.length,\n  by_source_type: {},\n  by_tier: {},\n  by_category: {},  // 新增：分类统计\n  score_distribution: { high: 0, medium: 0, low: 0 },\n  avg_score: 0\n};\n\nlet scoreSum = 0;\nitems.forEach((item) => {\n  const data = item.json || {};\n  const sourceType = data.sourceType || 'RSS';\n  const tier = data.tier || 'unknown';\n  const score = data.score?.total || 0;\n  const category = data.score?.category || 'unknown';\n\n  pipelineStats.by_source_type[sourceType] = (pipelineStats.by_source_type[sourceType] || 0) + 1;\n  pipelineStats.by_tier[tier] = (pipelineStats.by_tier[tier] || 0) + 1;\n  pipelineStats.by_category[category] = (pipelineStats.by_category[category] || 0) + 1;\n\n  if (score >= 24) pipelineStats.score_distribution.high++;\n  else if (score >= 18) pipelineStats.score_distribution.medium++;\n  else pipelineStats.score_distribution.low++;\n\n  scoreSum += score;\n});\npipelineStats.avg_score = items.length > 0 ? Math.round(scoreSum / items.length * 10) / 10 : 0;\n\nconst isTwitterUrl = (url) => {\n  const text = String(url || '').toLowerCase();\n  return text.includes('twitter.com/') || text.includes('x.com/') || text.includes('t.co/');\n};\n\nconst eligible = items.filter((item) => {\n  const data = item.json || {};\n  const source = data.source || '';\n  const sourceType = data.sourceType || (source.startsWith('X -') ? 'X' : 'RSS');\n  const url = data.url || data.link || '';\n  if (!url) return false;\n  if (!allowTwitterLinks && isTwitterUrl(url)) return false;\n  if (sourceType === 'X') return false;\n  return true;\n});\n\nconst fallbackEligible = items.filter((item) => {\n  const data = item.json || {};\n  const url = data.url || data.link || '';\n  if (!url) return false;\n  if (!allowTwitterLinks && isTwitterUrl(url)) return false;\n  return true;\n});\n\n// Take top 10 items (prefer non-X sources)\nconst top10 = (eligible.length ? eligible : fallbackEligible).slice(0, 10);\n\nif (!top10.length) {\n  throw new Error('No eligible non-Twitter sources available for tweet generation.');\n}\n\n// Build content list - simplified, focus on what's interesting\nconst contentList = top10.map((item, idx) => {\n  const data = item.json;\n  const sourceTag = data.source?.includes('GitHub') ? '🔧 工具' :\n                    data.source?.includes('Reddit') ? '💬 讨论' :\n                    data.tier === 'A' ? '📢 官方' : '📰 资讯';\n  return `${idx + 1}. [${sourceTag}] ${data.title}\n   ${data.url}\n   亮点: ${data.score?.why || data.snippet?.substring(0, 100) || ''}`;\n}).join('\\n\\n');\n\nconst prompt = `今日AI圈这些内容比较有意思：\n\n${contentList}\n\n---\n\n从中选你最想分享的1-2条，用你的风格写3个不同角度的推文。\n\n【风格要求】\n- discovery（发现型）: \"发现个好东西...\" / \"今天试了下...\" / \"这个项目有点意思...\"\n- insight（洞察型）: 你对这事的看法，可以有态度，但不杠\n- practical（实用型）: 适合谁用、怎么用、有啥坑\n\n【硬性规则 - 必须严格遵守】\n1. 每条推文**必须以URL结尾**（从上面素材中复制，不要自己编）\n2. 推文文字 + URL 总长度 ≤270字符\n3. 用中文写，可以夹英文术语\n4. 可以问读者问题增加互动\n\n【推文格式范例】\n✅ \"发现个神器：yt-dlp 视频下载工具，支持海量网站。试了下速度很快，有人用过没？https://github.com/yt-dlp/yt-dlp\"\n✅ \"这个思路挺野的，用AI生成专业头像。看了下效果确实不错，省去找摄影师的钱了 https://example.com\"\n❌ \"发现个好东西，yt-dlp这个视频下载工具真是功能丰富\" ← **缺URL，不合格**\n\n返回JSON（不要markdown）：\n{\n  \"discovery\": {\n    \"text\": \"推文内容【必须包含URL】\",\n    \"source_idx\": 1,\n    \"vibe\": \"惊喜/好奇/推荐\"\n  },\n  \"insight\": {\n    \"text\": \"推文内容【必须包含URL】\",\n    \"source_idx\": 2,\n    \"vibe\": \"思考/吐槽/认同\"\n  },\n  \"practical\": {\n    \"text\": \"推文内容【必须包含URL】\",\n    \"source_idx\": 1,\n    \"vibe\": \"实用/避坑/技巧\"\n  }\n}`;\n\ntry {\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: 'https://api.openai.com/v1/chat/completions',\n    headers: {\n      'Authorization': `Bearer ${apiKey}`,\n      'Content-Type': 'application/json'\n    },\n    body: {\n      model: model,\n      messages: [\n        { role: 'system', content: PERSONA.system },\n        { role: 'user', content: prompt }\n      ],\n      temperature: 0.85, // Higher for more natural variation\n      response_format: { type: 'json_object' }\n    }\n  });\n\n  const rawTweets = JSON.parse(response.choices[0].message.content);\n\n  // Helper: Check if text contains a URL\n  const hasUrl = (text) => {\n    const urlRegex = /https?:\\/\\/[^\\s]+/;\n    return urlRegex.test(text || '');\n  };\n\n  // Helper: Ensure tweet has URL, add from source if missing\n  const ensureUrl = (tweetText, sourceIdx) => {\n    if (hasUrl(tweetText)) {\n      return tweetText;\n    }\n    // Missing URL - extract from source\n    const source = top10[sourceIdx - 1] || top10[0];\n    const url = source?.json?.url || '';\n    if (!url) {\n      return tweetText; // No URL available\n    }\n    // Add URL at the end with a space\n    return `${tweetText} ${url}`;\n  };\n\n  // Map new format to legacy format for compatibility with Slack output\n  // IMPORTANT: Ensure all tweets have URLs\n  const tweets = {\n    hot_take: {\n      text: ensureUrl(rawTweets.discovery?.text || '', rawTweets.discovery?.source_idx || 1),\n      rationale: `风格: ${rawTweets.discovery?.vibe || 'discovery'}`,\n      risk: '确保链接有效',\n      source_idx: rawTweets.discovery?.source_idx\n    },\n    framework: {\n      text: ensureUrl(rawTweets.insight?.text || '', rawTweets.insight?.source_idx || 2),\n      rationale: `风格: ${rawTweets.insight?.vibe || 'insight'}`,\n      risk: '观点表达适度',\n      source_idx: rawTweets.insight?.source_idx\n    },\n    case: {\n      text: ensureUrl(rawTweets.practical?.text || '', rawTweets.practical?.source_idx || 1),\n      rationale: `风格: ${rawTweets.practical?.vibe || 'practical'}`,\n      risk: '实用建议需准确',\n      source_idx: rawTweets.practical?.source_idx\n    }\n  };\n\n  const hasBlocked = (value) => {\n    const text = String(value || '').toLowerCase();\n    return blocklist.some(word => word && text.includes(word));\n  };\n\n  const buildFallback = (label, source) => {\n    const title = source?.title || source?.snippet || '今日AI动态';\n    const shortTitle = title.length > 60 ? title.substring(0, 60) + '...' : title;\n    const url = source?.url || '';\n    let text = '';\n    if (label === 'hot_take') {\n      text = `发现个有意思的：${shortTitle} ${url}`;\n    } else if (label === 'framework') {\n      text = `这个思路可以参考下：${shortTitle} ${url}`;\n    } else {\n      text = `分享个实用的：${shortTitle}，感兴趣可以看看 ${url}`;\n    }\n    return {\n      text,\n      rationale: '使用简化的人性化模板',\n      risk: '内容较简单',\n      tone_guarded: true\n    };\n  };\n\n  const applyToneGuard = (tweetObj, label, fallbackSource) => {\n    if (!tweetObj || !tweetObj.text) return buildFallback(label, fallbackSource);\n    const blocked = [tweetObj.text, tweetObj.rationale, tweetObj.risk].some(hasBlocked);\n    if (blocked) return buildFallback(label, fallbackSource);\n    return tweetObj;\n  };\n\n  const fallbackSources = [top10[0]?.json, top10[1]?.json, top10[2]?.json];\n\n  // Validate and truncate tweets to ensure 280 character limit\n  const MAX_LENGTH = 280;\n  const validateAndTruncate = (tweetObj) => {\n    if (!tweetObj || !tweetObj.text) return tweetObj;\n\n    const text = tweetObj.text;\n    const length = text.length;\n\n    if (length <= MAX_LENGTH) {\n      return { ...tweetObj, length, truncated: false };\n    }\n\n    // Extract URLs to preserve them\n    const urlRegex = /(https?:\\/\\/[^\\s]+)/g;\n    const urls = text.match(urlRegex) || [];\n\n    // Calculate available space for text (280 - URLs - ellipsis - spaces)\n    const urlsLength = urls.reduce((sum, url) => sum + url.length, 0);\n    const availableSpace = MAX_LENGTH - urlsLength - 3; // 3 for \"...\"\n\n    if (availableSpace < 50) {\n      // If not enough space, just hard truncate\n      return {\n        ...tweetObj,\n        text: text.substring(0, MAX_LENGTH - 3) + '...',\n        length: MAX_LENGTH,\n        truncated: true,\n        original_length: length\n      };\n    }\n\n    // Smart truncate: remove text but keep URLs\n    let textWithoutUrls = text;\n    urls.forEach(url => {\n      textWithoutUrls = textWithoutUrls.replace(url, '');\n    });\n\n    const truncatedText = textWithoutUrls.substring(0, availableSpace).trim();\n    const finalText = truncatedText + '... ' + urls.join(' ');\n\n    return {\n      ...tweetObj,\n      text: finalText,\n      length: finalText.length,\n      truncated: true,\n      original_length: length\n    };\n  };\n\n  // Validate all three tweet types\n  const validatedTweets = {\n    hot_take: validateAndTruncate(applyToneGuard(tweets.hot_take, 'hot_take', fallbackSources[0])),\n    framework: validateAndTruncate(applyToneGuard(tweets.framework, 'framework', fallbackSources[1])),\n    case: validateAndTruncate(applyToneGuard(tweets.case, 'case', fallbackSources[2]))\n  };\n\n  // Log pipeline stats for debugging\n  console.log('Pipeline Stats:', JSON.stringify(pipelineStats));\n\n  return [{\n    json: {\n      tweets: validatedTweets,\n      sources: top10.map(item => item.json),\n      generated_at: new Date().toISOString(),\n      pipeline_stats: pipelineStats\n    }\n  }];\n} catch (error) {\n  throw new Error(`Tweet generation failed: ${error.message}`);\n}\n"
      },
      "name": "Generate Tweets",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1940,
        300
      ],
      "id": "tweet-gen"
    },
    {
      "parameters": {
        "jsCode": "// Slack Block Kit Output Node Code for n8n\n// This code formats and sends the daily pack to Slack\n\nconst data = $input.first().json;\nconst slackToken = $env.SLACK_BOT_TOKEN;\nconst channelId = $env.SLACK_CHANNEL_ID;\nconst xWriteEnabled = String($env.X_WRITE_ENABLED || '').toLowerCase() === 'true';\n\n// 分类配置\nconst groupByCategory = ($env.SLACK_GROUP_BY_CATEGORY || 'true') === 'true';\nconst includeCategoriesRaw = $env.SLACK_INCLUDE_CATEGORIES || '';\nconst excludeCategoriesRaw = $env.SLACK_EXCLUDE_CATEGORIES || '';\n\nconst includeCategories = includeCategoriesRaw\n  ? includeCategoriesRaw.split(',').map(c => c.trim().toLowerCase()).filter(Boolean)\n  : null;\nconst excludeCategories = excludeCategoriesRaw\n  ? excludeCategoriesRaw.split(',').map(c => c.trim().toLowerCase()).filter(Boolean)\n  : [];\n\nconst categoryOrder = ['announcement', 'tool', 'insight', 'case', 'research', 'risk', 'unknown'];\nconst categoryLabels = {\n  'announcement': '📢 公告/发布',\n  'tool': '🛠️ 工具/产品',\n  'insight': '💡 洞察/观点',\n  'case': '📊 案例/应用',\n  'research': '🔬 研究/论文',\n  'risk': '⚠️ 风险/警示',\n  'unknown': '📄 其他'\n};\n\nconst tweets = data.tweets;\nconst sources = data.sources;\nconst modeLine = xWriteEnabled\n  ? '🟢 实发模式：已开启 X 写入（X_WRITE_ENABLED=true）'\n  : '🔴 DRY-RUN：未开启 X 写入（X_WRITE_ENABLED=false）';\nconst modeNote = xWriteEnabled\n  ? '（将真实发布到 X）'\n  : '（默认仅 dry-run，不会真的发推；需要你在环境变量开启 X 写入开关）';\nconst instructions = [\n  '在本消息线程回复以下指令以执行动作：',\n  '`post 1` 发布 Option 1',\n  '`post 2` 发布 Option 2',\n  '`post 3` 发布 Option 3',\n  modeLine,\n  modeNote,\n].join('\\n');\n\n// Extract Top 3 highlights from sources\n// Priority: Tier A sources, high scores, keywords indicating major changes\nconst highlightKeywords = [\n  'release', 'launch', 'announce', 'new', 'update', 'v2', 'v3', 'v4',\n  'gpt-5', 'claude', 'gemini', 'llama', 'mistral', 'api', 'sdk',\n  '发布', '更新', '升级', '新版', '重大', '突破'\n];\n\nconst getHighlightScore = (source) => {\n  let score = source.score?.total || 0;\n  // Boost Tier A sources significantly\n  if (source.tier === 'A') score += 15;\n  else if (source.tier === 'B') score += 5;\n  // Boost items with highlight keywords\n  const titleLower = (source.title || '').toLowerCase();\n  const snippetLower = (source.snippet || '').toLowerCase();\n  for (const kw of highlightKeywords) {\n    if (titleLower.includes(kw) || snippetLower.includes(kw)) {\n      score += 3;\n      break;\n    }\n  }\n  return score;\n};\n\nconst topHighlights = [...sources]\n  .map(s => ({ ...s, highlightScore: getHighlightScore(s) }))\n  .sort((a, b) => b.highlightScore - a.highlightScore)\n  .slice(0, 3);\n\n// Format highlight text with emoji based on tier\nconst formatHighlight = (item, idx) => {\n  const tierEmoji = item.tier === 'A' ? '🔴' : item.tier === 'B' ? '🟠' : '🟡';\n  const title = (item.title || '').substring(0, 60);\n  const source = item.source || 'Unknown';\n  return `${tierEmoji} *${idx + 1}. ${title}*\\n   _来源: ${source}_`;\n};\n\nconst highlightsText = topHighlights.length > 0\n  ? topHighlights.map((h, i) => formatHighlight(h, i)).join('\\n\\n')\n  : '_今日无重大变动_';\n\n// Build Slack Block Kit message\nconst blocks = [\n  {\n    \"type\": \"header\",\n    \"text\": {\n      \"type\": \"plain_text\",\n      \"text\": \"📦 Today's X Daily Pack\",\n      \"emoji\": true\n    }\n  },\n  {\n    \"type\": \"context\",\n    \"elements\": [\n      {\n        \"type\": \"mrkdwn\",\n        \"text\": `Generated: ${new Date().toLocaleString('zh-CN', { timeZone: 'Asia/Shanghai' })}`\n      }\n    ]\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*🔥 Top 3 重大变动*\\n\\n${highlightsText}`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*✅ 审核与发布*\\n${instructions}`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": \"*🎯 推文选项（选一个发布）*\"\n    }\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*Option 1: Hot Take* ${tweets.hot_take.truncated ? '⚠️ _已截断_' : ''}\\n${tweets.hot_take.text}\\n\\n_字符数: ${tweets.hot_take.length || tweets.hot_take.text.length}/280_${tweets.hot_take.truncated ? ` | _原始: ${tweets.hot_take.original_length}_` : ''}\\n_理由: ${tweets.hot_take.rationale}_\\n_风险: ${tweets.hot_take.risk}_`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*Option 2: Framework* ${tweets.framework.truncated ? '⚠️ _已截断_' : ''}\\n${tweets.framework.text}\\n\\n_字符数: ${tweets.framework.length || tweets.framework.text.length}/280_${tweets.framework.truncated ? ` | _原始: ${tweets.framework.original_length}_` : ''}\\n_理由: ${tweets.framework.rationale}_\\n_风险: ${tweets.framework.risk}_`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": `*Option 3: Case Study* ${tweets.case.truncated ? '⚠️ _已截断_' : ''}\\n${tweets.case.text}\\n\\n_字符数: ${tweets.case.length || tweets.case.text.length}/280_${tweets.case.truncated ? ` | _原始: ${tweets.case.original_length}_` : ''}\\n_理由: ${tweets.case.rationale}_\\n_风险: ${tweets.case.risk}_`\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": \"*📚 今日素材（Top 10）*\"\n    }\n  }\n];\n\n// Add sources with category grouping and filtering\n// Step 1: Apply category filter\nlet filteredSources = sources;\nif (includeCategories && includeCategories.length > 0) {\n  filteredSources = sources.filter(s =>\n    includeCategories.includes((s.score?.category || 'unknown').toLowerCase())\n  );\n}\nif (excludeCategories.length > 0) {\n  filteredSources = filteredSources.filter(s =>\n    !excludeCategories.includes((s.score?.category || 'unknown').toLowerCase())\n  );\n}\n\n// Step 2: Display sources (grouped or flat)\nif (groupByCategory && filteredSources.length > 0) {\n  // Group by category\n  const groupedSources = {};\n  filteredSources.forEach((source) => {\n    const cat = source.score?.category || 'unknown';\n    if (!groupedSources[cat]) groupedSources[cat] = [];\n    groupedSources[cat].push(source);\n  });\n\n  // Render grouped\n  let itemIdx = 0;\n  categoryOrder.forEach((cat) => {\n    const items = groupedSources[cat];\n    if (!items || items.length === 0) return;\n\n    // Category header\n    blocks.push({\n      \"type\": \"section\",\n      \"text\": { \"type\": \"mrkdwn\", \"text\": `*${categoryLabels[cat]}* (${items.length})` }\n    });\n\n    // Items in this category\n    items.forEach((source) => {\n      itemIdx++;\n      const s = source.score || {};\n      blocks.push({\n        \"type\": \"section\",\n        \"text\": {\n          \"type\": \"mrkdwn\",\n          \"text\": `${itemIdx}. *${source.title}*\\n来源: ${source.source} | 总分: ${s.total || 0}/30\\n时效${s.timeliness || 0} 影响${s.impact || 0} 可行动${s.actionability || 0} 相关${s.relevance || 0}\\n<${source.url}|查看链接>`\n        }\n      });\n    });\n  });\n} else {\n  // Flat list (original behavior)\n  filteredSources.forEach((source, idx) => {\n    const s = source.score || {};\n    const categoryEmoji = {\n      'announcement': '📢',\n      'insight': '💡',\n      'tool': '🛠️',\n      'case': '📊',\n      'research': '🔬',\n      'risk': '⚠️'\n    }[s.category] || '📄';\n\n    blocks.push({\n      \"type\": \"section\",\n      \"text\": {\n        \"type\": \"mrkdwn\",\n        \"text\": `${idx + 1}. ${categoryEmoji} *${source.title}*\\n来源: ${source.source} | 总分: ${s.total || 0}/30\\n时效${s.timeliness || 0} 影响${s.impact || 0} 可行动${s.actionability || 0} 相关${s.relevance || 0}\\n<${source.url}|查看链接>`\n      }\n    });\n  });\n}\n\n// Add pipeline stats section for observability\nconst stats = data.pipeline_stats;\nif (stats) {\n  const tierBreakdown = Object.entries(stats.by_tier || {})\n    .map(([tier, count]) => `${tier}: ${count}`)\n    .join(' | ');\n  const sourceBreakdown = Object.entries(stats.by_source_type || {})\n    .map(([type, count]) => `${type}: ${count}`)\n    .join(' | ');\n  const categoryBreakdown = Object.entries(stats.by_category || {})\n    .map(([cat, count]) => `${categoryLabels[cat]?.split(' ')[0] || '📄'}${count}`)\n    .join(' ');\n  const scoreDist = stats.score_distribution || {};\n\n  blocks.push(\n    { \"type\": \"divider\" },\n    {\n      \"type\": \"context\",\n      \"elements\": [\n        {\n          \"type\": \"mrkdwn\",\n          \"text\": `📊 *运行统计* | 候选: ${stats.total_candidates || 0} | 平均分: ${stats.avg_score || 0}/30 | 高分(≥24): ${scoreDist.high || 0} | 中分(18-23): ${scoreDist.medium || 0}`\n        }\n      ]\n    },\n    {\n      \"type\": \"context\",\n      \"elements\": [\n        {\n          \"type\": \"mrkdwn\",\n          \"text\": `📁 来源: ${sourceBreakdown} | 📈 Tier: ${tierBreakdown}`\n        }\n      ]\n    },\n    {\n      \"type\": \"context\",\n      \"elements\": [\n        {\n          \"type\": \"mrkdwn\",\n          \"text\": `📂 分类: ${categoryBreakdown}`\n        }\n      ]\n    }\n  );\n}\n\n// Send to Slack\ntry {\n  const packMetadata = {\n    event_type: 'x_daily_pack',\n    event_payload: {\n      version: 1,\n      generated_at: data.generated_at || new Date().toISOString(),\n      tweets: {\n        hot_take: tweets?.hot_take?.text || '',\n        framework: tweets?.framework?.text || '',\n        case: tweets?.case?.text || ''\n      },\n      sources: (sources || []).slice(0, 10).map((s) => ({\n        title: s?.title || '',\n        url: s?.url || '',\n        source: s?.source || '',\n        score: s?.score?.total || 0\n      }))\n    }\n  };\n\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: 'https://slack.com/api/chat.postMessage',\n    headers: {\n      'Authorization': `Bearer ${slackToken}`,\n      'Content-Type': 'application/json'\n    },\n    body: {\n      channel: channelId,\n      blocks: blocks,\n      metadata: packMetadata,\n      text: 'Today\\'s X Daily Pack'\n    }\n  });\n\n  if (!response.ok) {\n    throw new Error(`Slack API error: ${response.error}`);\n  }\n\n  return [{\n    json: {\n      success: true,\n      message_ts: response.ts,\n      channel: response.channel\n    }\n  }];\n} catch (error) {\n  throw new Error(`Slack send failed: ${error.message}`);\n}\n"
      },
      "name": "Send to Slack",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2140,
        300
      ],
      "id": "slack-output"
    },
    {
      "parameters": {
        "jsCode": "// Telegram Output Node - Sends daily pack summary to Telegram\n// Parallel output alongside Slack - requires TELEGRAM_ENABLED=true\n\nconst telegramToken = $env.TELEGRAM_BOT_TOKEN;\nconst chatId = $env.TELEGRAM_CHAT_ID;\nconst enabled = String($env.TELEGRAM_ENABLED || '').toLowerCase() === 'true';\nconst strictMode = String($env.TELEGRAM_STRICT || 'true').toLowerCase() !== 'false';\n\n// Skip if not enabled or missing config\nif (!enabled) {\n  console.log('Telegram: Disabled (TELEGRAM_ENABLED != true)');\n  return [{ json: { telegram: 'disabled' } }];\n}\n\nif (!telegramToken || !chatId) {\n  throw new Error('Telegram: Missing TELEGRAM_BOT_TOKEN or TELEGRAM_CHAT_ID');\n}\n\nconst data = $input.first().json;\nconst tweets = data.tweets || {};\nconst sources = data.sources || [];\nconst stats = data.pipeline_stats || {};\n\n// Select top 3 highlights (prefer Tier A, high scores, tools)\nconst highlights = [...sources]\n  .sort((a, b) => {\n    // Boost GitHub/Reddit/ProductHunt sources\n    const aBoost = (a.source?.includes('GitHub') || a.source?.includes('Reddit') || a.source?.includes('Product Hunt')) ? 10 : 0;\n    const bBoost = (b.source?.includes('GitHub') || b.source?.includes('Reddit') || b.source?.includes('Product Hunt')) ? 10 : 0;\n    // Boost Tier A\n    const aTier = a.tier === 'A' ? 5 : (a.tier === 'B' ? 2 : 0);\n    const bTier = b.tier === 'A' ? 5 : (b.tier === 'B' ? 2 : 0);\n    const aScore = (a.score?.total || 0) + aBoost + aTier;\n    const bScore = (b.score?.total || 0) + bBoost + bTier;\n    return bScore - aScore;\n  })\n  .slice(0, 3);\n\n// Format source tag\nconst getSourceTag = (source) => {\n  if (source?.includes('GitHub')) return '🔧';\n  if (source?.includes('Reddit')) return '💬';\n  if (source?.includes('Product Hunt')) return '🚀';\n  return '📰';\n};\n\nconst escapeHtml = (value) => String(value || '')\n  .replace(/&/g, '&amp;')\n  .replace(/</g, '&lt;')\n  .replace(/>/g, '&gt;');\n\nconst escapeAttr = (value) => escapeHtml(encodeURI(String(value || '')));\n\n// Build Telegram message (HTML format)\nconst highlightsText = highlights.map((h, i) => {\n  const tag = getSourceTag(h.source);\n  const title = escapeHtml((h.title || '').substring(0, 60));\n  const shortWhy = escapeHtml((h.score?.why || '').substring(0, 50));\n  const link = h.url ? `<a href=\"${escapeAttr(h.url)}\">查看链接</a>` : '查看链接';\n  return `${tag} <b>${i + 1}. ${title}</b>\\n   ${link}\\n   <i>${shortWhy}</i>`;\n}).join('\\n\\n');\n\n// Pick best tweet (discovery style)\nconst bestTweet = tweets.hot_take?.text || tweets.framework?.text || tweets.case?.text || '';\nconst bestTweetText = escapeHtml(`${bestTweet.substring(0, 250)}${bestTweet.length > 250 ? '...' : ''}`);\nconst highlightsBlock = highlightsText || '<i>今日无重大变动</i>';\n\nconst message = `📦 <b>AI Frontline Daily</b>\n<i>${escapeHtml(new Date().toLocaleDateString('zh-CN', { timeZone: 'Asia/Shanghai' }))}</i>\n\n━━━━━━━━━━━━━━━━━━\n\n🔥 <b>今日亮点</b>\n\n${highlightsBlock}\n\n━━━━━━━━━━━━━━━━━━\n\n✍️ <b>推荐推文</b>\n\n<pre>${bestTweetText}</pre>\n\n━━━━━━━━━━━━━━━━━━\n\n📊 候选: ${stats.total_candidates || 0} | 平均分: ${stats.avg_score || 0}/30\n🔗 完整审阅请查看 Slack`;\n\ntry {\n  const response = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `https://api.telegram.org/bot${telegramToken}/sendMessage`,\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: {\n      chat_id: chatId,\n      text: message,\n      parse_mode: 'HTML',\n      disable_web_page_preview: false\n    },\n    throwHttpErrors: false\n  });\n\n  if (!response?.ok) {\n    const code = response?.error_code || response?.statusCode || 'unknown';\n    const desc = response?.description || response?.message || 'Unknown error';\n    throw new Error(`Telegram API error ${code}: ${desc}`);\n  }\n\n  console.log('Telegram: Message sent successfully');\n  return [{\n    json: {\n      telegram: 'sent',\n      message_id: response.result?.message_id,\n      chat_id: chatId\n    }\n  }];\n} catch (error) {\n  const message = `Telegram send failed: ${error.message}`;\n  console.error(message);\n  if (strictMode) {\n    throw new Error(message);\n  }\n  return [{ json: { telegram: 'failed', error: error.message } }];\n}\n"
      },
      "name": "Send to Telegram",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2140,
        450
      ],
      "id": "telegram-output"
    },
    {
      "parameters": {
        "jsCode": "// Multi News API Fetch Node\n// 统一采集多个新闻API，合并去重后输出\n// 所有API key从环境变量读取，不写死到代码中\n\n// API配置 - 从环境变量读取key\nconst APIs = {\n  newsapi: {\n    name: 'News API',\n    enabled: !!$env.NEWS_API_KEY,\n    key: $env.NEWS_API_KEY,\n    dailyLimit: 100,\n    baseUrl: 'https://newsapi.org/v2/everything',\n    buildUrl: (key, query) =>\n      `https://newsapi.org/v2/everything?q=${encodeURIComponent(query)}&language=en&sortBy=publishedAt&pageSize=10&apiKey=${key}`,\n    headers: () => ({ 'User-Agent': 'XDailyPack/1.0' }),\n    parseResponse: (data) => (data.articles || []).map(a => ({\n      title: a.title,\n      url: a.url,\n      source: `NewsAPI: ${a.source?.name || 'Unknown'}`,\n      snippet: a.description || '',\n      publishedAt: a.publishedAt\n    }))\n  },\n\n  newsdata: {\n    name: 'NewsData.io',\n    enabled: !!$env.NEWSDATA_API_KEY,\n    key: $env.NEWSDATA_API_KEY,\n    dailyLimit: 200,\n    buildUrl: (key, query) =>\n      `https://newsdata.io/api/1/latest?apikey=${key}&q=${encodeURIComponent(query)}&language=en`,\n    headers: () => ({ 'User-Agent': 'XDailyPack/1.0' }),\n    parseResponse: (data) => (data.results || []).map(a => ({\n      title: a.title,\n      url: a.link,\n      source: `NewsData: ${a.source_id || 'Unknown'}`,\n      snippet: a.description || '',\n      publishedAt: a.pubDate\n    }))\n  },\n\n  gnews: {\n    name: 'GNews API',\n    enabled: !!$env.GNEWS_API_KEY,\n    key: $env.GNEWS_API_KEY,\n    dailyLimit: 100,\n    buildUrl: (key, query) =>\n      `https://gnews.io/api/v4/search?q=${encodeURIComponent(query)}&lang=en&max=10&apikey=${key}`,\n    headers: () => ({ 'User-Agent': 'XDailyPack/1.0' }),\n    parseResponse: (data) => (data.articles || []).map(a => ({\n      title: a.title,\n      url: a.url,\n      source: `GNews: ${a.source?.name || 'Unknown'}`,\n      snippet: a.description || '',\n      publishedAt: a.publishedAt\n    }))\n  },\n\n  thenewsapi: {\n    name: 'TheNewsAPI',\n    enabled: !!$env.THENEWSAPI_KEY,\n    key: $env.THENEWSAPI_KEY,\n    dailyLimit: 100,\n    buildUrl: (key, query) =>\n      `https://api.thenewsapi.com/v1/news/all?api_token=${key}&search=${encodeURIComponent(query)}&language=en&limit=10`,\n    headers: () => ({ 'User-Agent': 'XDailyPack/1.0' }),\n    parseResponse: (data) => (data.data || []).map(a => ({\n      title: a.title,\n      url: a.url,\n      source: `TheNewsAPI: ${a.source || 'Unknown'}`,\n      snippet: a.description || a.snippet || '',\n      publishedAt: a.published_at\n    }))\n  },\n\n  currents: {\n    name: 'Currents API',\n    enabled: !!$env.CURRENTS_API_KEY,\n    key: $env.CURRENTS_API_KEY,\n    dailyLimit: 1000,\n    buildUrl: (key, query) =>\n      `https://api.currentsapi.services/v1/search?apiKey=${key}&keywords=${encodeURIComponent(query)}&language=en`,\n    headers: () => ({ 'User-Agent': 'XDailyPack/1.0' }),\n    parseResponse: (data) => (data.news || []).map(a => ({\n      title: a.title,\n      url: a.url,\n      source: `Currents: ${a.author || 'Unknown'}`,\n      snippet: a.description || '',\n      publishedAt: a.published\n    }))\n  }\n};\n\n// 已知媒体源tier映射\nconst sourceTierMap = {\n  // Tier A - 官方源 (通常不会出现在News API中，但保留以防)\n  'openai': 'A',\n  'anthropic': 'A',\n  'google ai': 'A',\n  'deepmind': 'A',\n\n  // Tier B - 权威媒体\n  'techcrunch': 'B',\n  'venturebeat': 'B',\n  'wired': 'B',\n  'the verge': 'B',\n  'mit technology review': 'B',\n  'reuters': 'B',\n  'bloomberg': 'B',\n  'cnbc': 'B',\n  'bbc': 'B',\n  'cnn': 'B',\n  'ars technica': 'B',\n  'engadget': 'B',\n  'zdnet': 'B',\n  'the information': 'B',\n  'financial times': 'B',\n  'wall street journal': 'B',\n  'new york times': 'B',\n  'washington post': 'B',\n\n  // Tier C - 社区/二手源\n  'reddit': 'C',\n  'medium': 'C',\n  'dev.to': 'C',\n  'hacker news': 'C',\n  'slashdot': 'C',\n  'digg': 'C',\n\n  // Tier D - 聚合源\n  'google news': 'D',\n  'yahoo news': 'D',\n  'msn': 'D',\n  'flipboard': 'D'\n};\n\n// 根据source名称分配tier\nconst assignTier = (sourceName) => {\n  const lower = (sourceName || '').toLowerCase();\n  for (const [name, tier] of Object.entries(sourceTierMap)) {\n    if (lower.includes(name)) return tier;\n  }\n  return 'B'; // 默认Tier B（权威媒体级别）\n};\n\n// 搜索关键词 - AI行业核心话题 (2026年1月更新)\nconst queries = [\n  'OpenAI OR GPT-5 OR ChatGPT',\n  'Anthropic OR Claude OR \"Claude Opus\"',\n  'Google Gemini OR \"Gemini 3\" OR DeepMind',\n  'DeepSeek OR \"DeepSeek V3\" OR \"DeepSeek V4\"',\n  'Mistral AI OR \"Mistral Large\" OR Llama 4',\n  'xAI OR Grok OR Perplexity'\n];\n\n// 每个API只执行一个查询，轮流分配以节省额度\nconst getQueryForApi = (apiIndex) => queries[apiIndex % queries.length];\n\n// 重试配置\nconst maxRetries = Number.parseInt($env.NEWS_API_RETRY_MAX_ATTEMPTS || '3', 10);\nconst retryDelayMs = Number.parseInt($env.NEWS_API_RETRY_INITIAL_DELAY_MS || '500', 10);\n\n// Exponential backoff retry function\nconst retryWithBackoff = async (fn, maxAttempts = maxRetries, initialDelayMs = retryDelayMs) => {\n  let lastError;\n  for (let attempt = 1; attempt <= maxAttempts; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      lastError = error;\n      const isRetryable = /timeout|ETIMEDOUT|ECONNRESET|ECONNREFUSED|429|503|502|rate limit/i.test(error.message);\n      if (!isRetryable || attempt === maxAttempts) throw error;\n      const delayMs = initialDelayMs * Math.pow(2, attempt - 1);\n      console.log(`[NewsAPI Retry] Attempt ${attempt}/${maxAttempts} failed, retrying in ${delayMs}ms`);\n      await new Promise(r => setTimeout(r, delayMs));\n    }\n  }\n  throw lastError;\n};\n\nconst allArticles = [];\nconst errors = [];\nconst stats = { apis: {} };\n\n// 获取启用的API列表\nconst enabledApis = Object.entries(APIs).filter(([_, cfg]) => cfg.enabled);\n\nif (enabledApis.length === 0) {\n  console.log('No News APIs configured');\n  return [];\n}\n\n// 并行请求所有API\nconst fetchPromises = enabledApis.map(async ([id, config], index) => {\n  const query = getQueryForApi(index);\n  const url = config.buildUrl(config.key, query);\n\n  try {\n    const response = await retryWithBackoff(async () => {\n      return await this.helpers.httpRequest({\n        method: 'GET',\n        url: url,\n        headers: config.headers(),\n        timeout: 15000,\n        returnFullResponse: false\n      });\n    });\n\n    const data = typeof response === 'string' ? JSON.parse(response) : response;\n\n    if (data.error || data.status === 'error') {\n      throw new Error(data.error?.message || data.message || 'API error');\n    }\n\n    const articles = config.parseResponse(data);\n    stats.apis[id] = { success: true, count: articles.length, query };\n    return { id, articles, error: null };\n  } catch (error) {\n    stats.apis[id] = { success: false, error: error.message };\n    return { id, articles: [], error: error.message };\n  }\n});\n\nconst results = await Promise.all(fetchPromises);\n\n// 收集结果\nresults.forEach(result => {\n  if (result.error) {\n    errors.push({ api: result.id, error: result.error });\n  } else {\n    result.articles.forEach(article => {\n      article.sourceType = 'NewsAPI';\n      article.tier = assignTier(article.source);\n      article.apiSource = result.id;\n    });\n    allArticles.push(...result.articles);\n  }\n});\n\n// URL去重\nconst seenUrls = new Set();\nconst uniqueArticles = allArticles.filter(article => {\n  if (!article.url || seenUrls.has(article.url)) return false;\n  seenUrls.add(article.url);\n  return true;\n});\n\n// 日志统计\nstats.total_apis = enabledApis.length;\nstats.successful_apis = results.filter(r => !r.error).length;\nstats.total_articles = allArticles.length;\nstats.unique_articles = uniqueArticles.length;\nstats.errors = errors;\n\nconsole.log('Multi News API Stats:', JSON.stringify(stats));\n\nreturn uniqueArticles.map(article => ({ json: article }));\n"
      },
      "name": "Multi News API",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        440,
        100
      ],
      "id": "news-api-fetch"
    },
    {
      "parameters": {
        "mode": "append"
      },
      "name": "Merge RSS+News",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [
        700,
        150
      ],
      "id": "merge-rss-news"
    }
  ],
  "connections": {
    "Manual Trigger": {
      "main": [
        [
          {
            "node": "RSS Fetch All",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Keyword Search",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Account Search",
            "type": "main",
            "index": 0
          },
          {
            "node": "Multi News API",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Trigger UTC 0h 12h": {
      "main": [
        [
          {
            "node": "RSS Fetch All",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Keyword Search",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Account Search",
            "type": "main",
            "index": 0
          },
          {
            "node": "Multi News API",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RSS Fetch All": {
      "main": [
        [
          {
            "node": "Merge RSS+News",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "X Keyword Search": {
      "main": [
        [
          {
            "node": "Merge X",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "X Account Search": {
      "main": [
        [
          {
            "node": "Merge X",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge X": {
      "main": [
        [
          {
            "node": "Merge All",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge All": {
      "main": [
        [
          {
            "node": "Normalize",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Normalize": {
      "main": [
        [
          {
            "node": "Cross-Day Dedupe",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Cross-Day Dedupe": {
      "main": [
        [
          {
            "node": "Semantic Dedupe",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Rank": {
      "main": [
        [
          {
            "node": "Generate Tweets",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Tweets": {
      "main": [
        [
          {
            "node": "Send to Slack",
            "type": "main",
            "index": 0
          },
          {
            "node": "Send to Telegram",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Semantic Dedupe": {
      "main": [
        [
          {
            "node": "LLM Rank",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge RSS+News": {
      "main": [
        [
          {
            "node": "Merge All",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Multi News API": {
      "main": [
        [
          {
            "node": "Merge RSS+News",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook Trigger": {
      "main": [
        [
          {
            "node": "RSS Fetch All",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Keyword Search",
            "type": "main",
            "index": 0
          },
          {
            "node": "X Account Search",
            "type": "main",
            "index": 0
          },
          {
            "node": "Multi News API",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1",
    "availableInMCP": false,
    "callerPolicy": "workflowsFromSameOwner",
    "timezone": "Asia/Shanghai"
  }
}